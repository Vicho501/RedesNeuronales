{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4370f6f1-d1f7-44fe-8eca-3ea1c52cf27d",
   "metadata": {},
   "source": [
    "<h1>Implementamos las librerias y los datos de MNIST</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0fc1f2ed-576f-4d53-a457-4311ca75261d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "import random\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data() \n",
    "X, y = zip(*random.sample(list(zip(X_train, y_train)), 2000))\n",
    " \n",
    "X, y = np.array(X, dtype='float64'), np.array(y, dtype='float64')\n",
    "X = np.reshape(X, (X.shape[0], -1))\n",
    "\n",
    "X= MinMaxScaler().fit_transform(X)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=123)\n",
    "\n",
    "y_train_value = y_train\n",
    "from keras.utils import to_categorical\n",
    "y_train = to_categorical(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583325d7-72f7-4dd7-a202-6f95a3c3066b",
   "metadata": {},
   "source": [
    "<h1>Declaramos Clase Base</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4b503e87-5889-4151-ac82-11ab22946759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clase base para CapaY\n",
    "class Layer:\n",
    "    def __init__(self):\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "\n",
    "    # Computes the output Y of a layer for a given input X\n",
    "    def forward_propagation(self, input):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    # Computes dE/dX for a given dE/dY (and update parameters if any)\n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3009d0-b74e-4679-97cd-9faf04c57a56",
   "metadata": {},
   "source": [
    "<h1>Pooling</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba4981f-6266-423b-8f09-e91a4ffc8690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para construir un modelo CNN con pooling\n",
    "def build_cnn_model_with_pooling():\n",
    "    model = Network()\n",
    "\n",
    "    # Primera capa convolucional seguida de MaxPooling\n",
    "    model.add(Conv2D(32, (3, 3), activation=relu))  # Capa convolucional\n",
    "    model.add(MaxPooling2D((2, 2)))  # Max Pooling con ventana de 2x2\n",
    "\n",
    "    # Segunda capa convolucional seguida de AveragePooling\n",
    "    model.add(Conv2D(64, (3, 3), activation=relu))  # Otra capa convolucional\n",
    "    model.add(AveragePooling2D((2, 2)))  # Average Pooling con ventana de 2x2\n",
    "\n",
    "    # Tercera capa convolucional seguida de MaxPooling\n",
    "    model.add(Conv2D(128, (3, 3), activation=relu))  # Otra capa convolucional\n",
    "    model.add(MaxPooling2D((2, 2)))  # Max Pooling con ventana de 2x2\n",
    "\n",
    "    # Aplanar para conectarlo a la capa densa\n",
    "    model.add(Flatten())  # Aplanamos la salida de las capas convolucionales\n",
    "    model.add(FCLayer(128, 1))  # Capa densa final para clasificación binaria\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2d8ce3-6b86-4feb-858d-c967fe31eb8e",
   "metadata": {},
   "source": [
    "<h1>La clase Dropout</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5259a5d4-6a12-4cc0-8290-e8a27d65e6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropoutLayer(Layer):\n",
    "    def init(self, rate):\n",
    "        # 'rate' es la fracción de neuronas que se apagan, p.ej., 0.2 significa que el 20% se apagarán\n",
    "        self.rate = rate\n",
    "        self.mask = None\n",
    "\n",
    "    def forward_propagation(self, input_data, training=True):\n",
    "        if training:\n",
    "            # Crear una máscara binaria aleatoria de la misma forma que los datos de entrada\n",
    "            self.mask = np.random.binomial(1, 1 - self.rate, size=input_data.shape)\n",
    "            # Apagar las neuronas según la máscara y escalar los valores restantes\n",
    "            self.output = input_data * self.mask / (1 - self.rate)\n",
    "        else:\n",
    "            # En la fase de evaluación, no se usa Dropout, simplemente pasamos los datos\n",
    "            self.output = input_data\n",
    "        return self.output\n",
    "\n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        # La máscara también se aplica en la retropropagación\n",
    "        return output_error * self.mask / (1 - self.rate)\n",
    "\n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        # Implementar la retropropagación\n",
    "        # Este es un ejemplo simplificado, necesitarás implementarlo adecuadamente\n",
    "        input_error = np.zeros(self.input.shape)\n",
    "        for f in range(self.filters):\n",
    "            for i in range(self.output.shape[1]):\n",
    "                for j in range(self.output.shape[2]):\n",
    "                    input_error[:, i:i + self.kernel_size[0], j:j + self.kernel_size[1]] += (\n",
    "                        output_error[f, i, j] * self.weights[f]\n",
    "                    )\n",
    "                    self.weights[f] -= learning_rate * output_error[f, i, j] * self.input[:, i:i + self.kernel_size[0], j:j + self.kernel_size[1]]\n",
    "                    self.bias[f] -= learning_rate * output_error[f, i, j]\n",
    "        return input_error\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2078be-7722-49cc-b128-b87e8737472b",
   "metadata": {},
   "source": [
    "<h1>Código integrado con Dropout</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d796554d-beb1-4120-a061-13944d53934f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Clase para capas densas (fully connected)\n",
    "class FCLayer(Layer):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.weights = np.random.rand(input_size, output_size) - 0.5\n",
    "        self.bias = np.random.rand(1, output_size) - 0.5\n",
    "\n",
    "    def forward_propagation(self, input_data):\n",
    "        self.input = input_data\n",
    "        self.output = np.dot(self.input, self.weights) + self.bias\n",
    "        return self.output\n",
    "\n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        input_error = np.dot(output_error, self.weights.T)\n",
    "        weights_error = np.dot(self.input.T, output_error)\n",
    "\n",
    "        # Actualizar los parámetros sin regularización L2\n",
    "        self.weights -= learning_rate * weights_error\n",
    "        self.bias -= learning_rate * output_error\n",
    "        return input_error\n",
    "\n",
    "# Clase para Capa de Activación\n",
    "class ActivationLayer(Layer):\n",
    "    def __init__(self, activation, activation_prime):\n",
    "        self.activation = activation\n",
    "        self.activation_prime = activation_prime\n",
    "\n",
    "    def forward_propagation(self, input_data):\n",
    "        self.input = input_data\n",
    "        self.output = self.activation(self.input)\n",
    "        return self.output\n",
    "\n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        return self.activation_prime(self.input) * output_error\n",
    "\n",
    "# Clase para Dropout\n",
    "class DropoutLayer(Layer):\n",
    "    def __init__(self, rate):\n",
    "        self.rate = rate\n",
    "        self.mask = None\n",
    "\n",
    "    def forward_propagation(self, input_data, training=True):\n",
    "        if training:\n",
    "            self.mask = np.random.binomial(1, 1 - self.rate, size=input_data.shape)\n",
    "            self.output = input_data * self.mask / (1 - self.rate)\n",
    "        else:\n",
    "            self.output = input_data\n",
    "        return self.output\n",
    "\n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        return output_error * self.mask / (1 - self.rate)\n",
    "\n",
    "# Clase para Capa Convolucional\n",
    "class Conv2D(Layer):\n",
    "    def __init__(self, filters, kernel_size, activation):\n",
    "        self.filters = filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.activation = activation\n",
    "        self.weights = np.random.randn(filters, kernel_size[0], kernel_size[1]) * 0.01\n",
    "        self.bias = np.zeros((filters, 1))\n",
    "\n",
    "    def forward_propagation(self, input_data):\n",
    "        self.input = input_data\n",
    "        # Implementar la operación de convolución\n",
    "        h, w = input_data.shape[1], input_data.shape[2]\n",
    "        fh, fw = self.kernel_size\n",
    "        self.output = np.zeros((self.filters, h - fh + 1, w - fw + 1))\n",
    "\n",
    "        for f in range(self.filters):\n",
    "            for i in range(h - fh + 1):\n",
    "                for j in range(w - fw + 1):\n",
    "                    self.output[f, i, j] = np.sum(input_data[:, i:i + fh, j:j + fw] * self.weights[f]) + self.bias[f]\n",
    "\n",
    "        # Aplicar la función de activación\n",
    "        self.output = self.activation(self.output)\n",
    "        return self.output\n",
    "\n",
    " \n",
    "# Clase para MaxPooling\n",
    "class MaxPooling2D(Layer):\n",
    "    def __init__(self, pool_size):\n",
    "        self.pool_size = pool_size\n",
    "\n",
    "    def forward_propagation(self, input_data):\n",
    "        self.input = input_data\n",
    "        h, w = input_data.shape[1], input_data.shape[2]\n",
    "        ph, pw = self.pool_size\n",
    "        self.output = np.zeros((input_data.shape[0], h // ph, w // pw))\n",
    "\n",
    "        for i in range(0, h, ph):\n",
    "            for j in range(0, w, pw):\n",
    "                self.output[:, i // ph, j // pw] = np.max(input_data[:, i:i + ph, j:j + pw], axis=(1, 2))\n",
    "\n",
    "        return self.output\n",
    "\n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        input_error = np.zeros(self.input.shape)\n",
    "        h, w = self.input.shape[1], self.input.shape[2]\n",
    "        ph, pw = self.pool_size\n",
    "\n",
    "        for i in range(0, h, ph):\n",
    "            for j in range(0, w, pw):\n",
    "                input_error[:, i:i + ph, j:j + pw] += (\n",
    "                    (output_error[:, i // ph, j // pw][:, np.newaxis, np.newaxis] *\n",
    "                     (self.input[:, i:i + ph, j:j + pw] == np.max(self.input[:, i:i + ph, j:j + pw], axis=(1, 2), keepdims=True)))\n",
    "                )\n",
    "\n",
    "        return input_error\n",
    "\n",
    "# Clase para AveragePooling\n",
    "class AveragePooling2D(Layer):\n",
    "    def __init__(self, pool_size):\n",
    "        self.pool_size = pool_size\n",
    "\n",
    "    def forward_propagation(self, input_data):\n",
    "        self.input = input_data\n",
    "        h, w = input_data.shape[1], input_data.shape[2]\n",
    "        ph, pw = self.pool_size\n",
    "        self.output = np.zeros((input_data.shape[0], h // ph, w // pw))\n",
    "\n",
    "        for i in range(0, h, ph):\n",
    "            for j in range(0, w, pw):\n",
    "                self.output[:, i // ph, j // pw] = np.mean(input_data[:, i:i + ph, j:j + pw], axis=(1, 2))\n",
    "\n",
    "        return self.output\n",
    "\n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        input_error = np.zeros(self.input.shape)\n",
    "        h, w = self.input.shape[1], self.input.shape[2]\n",
    "        ph, pw = self.pool_size\n",
    "\n",
    "        for i in range(0, h, ph):\n",
    "            for j in range(0, w, pw):\n",
    "                input_error[:, i:i + ph, j:j + pw] += output_error[:, i // ph, j // pw][:, np.newaxis, np.newaxis] / (ph * pw)\n",
    "\n",
    "        return input_error\n",
    "\n",
    "# Clase Red\n",
    "class Network:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        self.loss = None\n",
    "        self.loss_prime = None\n",
    "\n",
    "    # add layer to network\n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    # set loss to use\n",
    "    def use(self, loss, loss_prime):\n",
    "        self.loss = loss\n",
    "        self.loss_prime = loss_prime\n",
    "\n",
    "    # predict output for given input\n",
    "    def predict(self, input_data):\n",
    "        if input_data.ndim == 1:\n",
    "            input_data = np.array([[x] for x in input_data])\n",
    "        samples = len(input_data)\n",
    "        result = []\n",
    "\n",
    "        for i in range(samples):\n",
    "            output = input_data[i]\n",
    "            for layer in self.layers:\n",
    "                output = layer.forward_propagation(output)\n",
    "            result.append(output)\n",
    "        return result\n",
    "\n",
    "    # train the network\n",
    "    def fit(self, x_train, y_train, epochs, learning_rate, x_val=None, y_val=None):\n",
    "        if x_train[0].ndim == 1:\n",
    "            x_train = np.array([[x] for x in x_train])\n",
    "        samples = len(x_train)\n",
    "\n",
    "        for i in range(epochs):\n",
    "            err = 0\n",
    "            for j in range(samples):\n",
    "                output = x_train[j]\n",
    "                for layer in self.layers:\n",
    "                    output = layer.forward_propagation(output)\n",
    "\n",
    "                err += self.loss(y_train[j], output)\n",
    "\n",
    "                error = self.loss_prime(y_train[j], output)\n",
    "                for layer in reversed(self.layers):\n",
    "                    error = layer.backward_propagation(error, learning_rate)\n",
    "\n",
    "            err /= samples\n",
    "            err = np.mean(err)\n",
    "            print('epoch %d/%d   error=%f' % (i + 1, epochs, err))\n",
    "\n",
    "# Funciones de Activación\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_prime(x):\n",
    "    return 1 - np.tanh(x) ** 2\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_prime(x):\n",
    "    sig = sigmoid(x)\n",
    "    return sig * (1 - sig)\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_prime(x):\n",
    "    return np.where(x > 0, 1, 0)\n",
    "\n",
    "\n",
    "# Funciones de pérdida\n",
    "def mse(y_true, y_hat):\n",
    "    return (y_true - y_hat) ** 2\n",
    "\n",
    "def mse_prime(y_true, y_hat):\n",
    "    return 2 * (y_hat - y_true)\n",
    "\n",
    "def bce(y_true, y_hat):\n",
    "    epsilon = 1e-15\n",
    "    y_hat = np.clip(y_hat, epsilon, 1 - epsilon)\n",
    "    return -(y_true * np.log(y_hat) + (1 - y_true) * np.log(1 - y_hat))\n",
    "\n",
    "def bce_prime(y_true, y_hat):\n",
    "    epsilon = 1e-15\n",
    "    y_hat = np.clip(y_hat, epsilon, 1 - epsilon)\n",
    "    return -(y_true / y_hat) + (1 - y_true) / (1 - y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1099696e-f588-4aa8-848b-de7dc8a53963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/30   error=0.275342\n",
      "epoch 2/30   error=0.175796\n",
      "epoch 3/30   error=0.143178\n",
      "epoch 4/30   error=0.133792\n",
      "epoch 5/30   error=0.103752\n",
      "epoch 6/30   error=0.092134\n",
      "epoch 7/30   error=0.087471\n",
      "epoch 8/30   error=0.081053\n",
      "epoch 9/30   error=0.070765\n",
      "epoch 10/30   error=0.062393\n",
      "epoch 11/30   error=0.060012\n",
      "epoch 12/30   error=0.057110\n",
      "epoch 13/30   error=0.045405\n",
      "epoch 14/30   error=0.041346\n",
      "epoch 15/30   error=0.045356\n",
      "epoch 16/30   error=0.052242\n",
      "epoch 17/30   error=0.060302\n",
      "epoch 18/30   error=0.050757\n",
      "epoch 19/30   error=0.040720\n",
      "epoch 20/30   error=0.028774\n",
      "epoch 21/30   error=0.032936\n",
      "epoch 22/30   error=0.037235\n",
      "epoch 23/30   error=0.026061\n",
      "epoch 24/30   error=0.044376\n",
      "epoch 25/30   error=0.039553\n",
      "epoch 26/30   error=0.022985\n",
      "epoch 27/30   error=0.031815\n",
      "epoch 28/30   error=0.016302\n",
      "epoch 29/30   error=0.013322\n",
      "epoch 30/30   error=0.030399\n",
      "MATRIZ DE CONFUSIÓN para modelo ANN\n",
      "[[ 94   0   0   2   0   1   1   0   1   0]\n",
      " [  0 111   0   1   1   0   0   2   0   0]\n",
      " [  4   1  76   7   1   0   8   3   3   1]\n",
      " [  0   0   9  91   3   1   0   0   2   2]\n",
      " [  0   1   1   0  91   0   2   0   1   5]\n",
      " [  3   1   0   5   2  69   3   2   4   2]\n",
      " [  1   0   0   0   2   2  76   0   1   0]\n",
      " [  1   1   0   1   3   0   0  81   0   4]\n",
      " [  3   3   5   6   2   3   4   4  77   6]\n",
      " [  2   0   1   2   6   1   0   4   1  79]] \n",
      "\n",
      "La exactitud de testeo del modelo ANN es: 0.845\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "X_train_full, X_val, y_train_full, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "model = Network()\n",
    "\n",
    "entrada_dim = len(X_train_full[0])\n",
    "\n",
    "# Agregamos capas al modelo\n",
    "model.add(FCLayer(entrada_dim, 128))\n",
    "model.add(ActivationLayer(relu, relu_prime))\n",
    "model.add(DropoutLayer(rate=0.2))  # Añadiendo Dropout con un 20% de tasa\n",
    "model.add(FCLayer(128, 64))\n",
    "model.add(ActivationLayer(sigmoid, sigmoid_prime))\n",
    "model.add(DropoutLayer(rate=0.2))  # Añadiendo Dropout\n",
    "model.add(FCLayer(64, 10))\n",
    "model.add(ActivationLayer(sigmoid, sigmoid_prime))\n",
    "\n",
    "model.use(bce, bce_prime)\n",
    "\n",
    "model.fit(X_train_full, y_train_full, epochs=30, learning_rate=0.1, x_val=X_val, y_val=y_val)\n",
    "\n",
    "y_hat = model.predict(X_test)\n",
    "\n",
    "for i in range(len(y_hat)):\n",
    "    y_hat[i] = np.argmax(y_hat[i])\n",
    "\n",
    "# Reportamos los resultados del modelo\n",
    "matriz_conf = confusion_matrix(y_test, y_hat)\n",
    "print('MATRIZ DE CONFUSIÓN para modelo ANN')\n",
    "print(matriz_conf, '\\n')\n",
    "print('La exactitud de testeo del modelo ANN es: {:.3f}'.format(accuracy_score(y_test, y_hat)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4732a0-f625-4e38-9ce0-2f97b746824c",
   "metadata": {},
   "source": [
    "<h1>Ahora vamos con Early Stopping</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "82825ca0-62c5-463d-8b83-faa5459979b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clase EarlyStopping proporcionada\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.best_loss = None\n",
    "        self.wait = 0\n",
    "\n",
    "    def should_stop(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "            return False\n",
    "        if self.best_loss - val_loss > self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.wait = 0\n",
    "        else:\n",
    "            self.wait += 1\n",
    "        return self.wait >= self.patience\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6a55e6-229c-4a7f-a683-d5f0de77733b",
   "metadata": {},
   "source": [
    "<h1>Código implementado junto al pooling y el Early Stopping</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1b572815-8262-4d1f-bf0a-93a1e6d429e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Clase base para Capa\n",
    "class Layer:\n",
    "    def __init__(self):\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "\n",
    "    # computes the output Y of a layer for a given input X\n",
    "    def forward_propagation(self, input):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    # computes dE/dX for a given dE/dY (and update parameters if any)\n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "# Clase para capas densas (fully connected)\n",
    "class FCLayer(Layer):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.weights = np.random.rand(input_size, output_size) - 0.5\n",
    "        self.bias = np.random.rand(1, output_size) - 0.5\n",
    "\n",
    "    def forward_propagation(self, input_data):\n",
    "        self.input = input_data\n",
    "        self.output = np.dot(self.input, self.weights) + self.bias\n",
    "        return self.output\n",
    "\n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        input_error = np.dot(output_error, self.weights.T)\n",
    "        weights_error = np.dot(self.input.T, output_error)\n",
    "        self.weights -= learning_rate * weights_error\n",
    "        self.bias -= learning_rate * output_error\n",
    "        return input_error\n",
    "\n",
    "\n",
    "# Clase para Capa de Activación. Junto con la capa densa forman perceptrones.\n",
    "class ActivationLayer(Layer):\n",
    "    def __init__(self, activation, activation_prime):\n",
    "        self.activation = activation\n",
    "        self.activation_prime = activation_prime\n",
    "\n",
    "    def forward_propagation(self, input_data):\n",
    "        self.input = input_data\n",
    "        self.output = self.activation(self.input)\n",
    "        return self.output\n",
    "\n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        return self.activation_prime(self.input) * output_error\n",
    "\n",
    "\n",
    "# Clases para capas convolucionales y de pooling\n",
    "class Conv2D(Layer):\n",
    "    def __init__(self, filters, kernel_size, activation=None):\n",
    "        self.filters = filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.activation = activation\n",
    "        self.weights = np.random.rand(kernel_size[0], kernel_size[1], 1, filters) - 0.5\n",
    "        self.bias = np.random.rand(filters) - 0.5\n",
    "\n",
    "    def forward_propagation(self, input_data):\n",
    "        self.input = input_data\n",
    "        # Aplicar la convolución aquí (simplificada)\n",
    "        # Se necesita una implementación completa para la convolución\n",
    "        self.output = np.zeros((input_data.shape[0] - self.kernel_size[0] + 1, input_data.shape[1] - self.kernel_size[1] + 1, self.filters))\n",
    "        for f in range(self.filters):\n",
    "            for i in range(self.output.shape[0]):\n",
    "                for j in range(self.output.shape[1]):\n",
    "                    self.output[i, j, f] = np.sum(input_data[i:i+self.kernel_size[0], j:j+self.kernel_size[1]] * self.weights[..., f]) + self.bias[f]\n",
    "        if self.activation:\n",
    "            return self.activation(self.output)\n",
    "        return self.output\n",
    "\n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        # Se necesita implementar la retropropagación para la convolución\n",
    "        pass\n",
    "\n",
    "\n",
    "class MaxPooling2D(Layer):\n",
    "    def __init__(self, pool_size):\n",
    "        self.pool_size = pool_size\n",
    "\n",
    "    def forward_propagation(self, input_data):\n",
    "        self.input = input_data\n",
    "        # Max pooling (simplificada)\n",
    "        output_shape = (input_data.shape[0] // self.pool_size[0], input_data.shape[1] // self.pool_size[1], input_data.shape[2])\n",
    "        self.output = np.zeros(output_shape)\n",
    "        for i in range(output_shape[0]):\n",
    "            for j in range(output_shape[1]):\n",
    "                self.output[i, j] = np.max(input_data[i*self.pool_size[0]:(i+1)*self.pool_size[0], j*self.pool_size[1]:(j+1)*self.pool_size[1], :], axis=(0, 1))\n",
    "        return self.output\n",
    "\n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        # Se necesita implementar la retropropagación para max pooling\n",
    "        pass\n",
    "\n",
    "class AveragePooling2D(MaxPooling2D):\n",
    "    def forward_propagation(self, input_data):\n",
    "        self.input = input_data\n",
    "        output_shape = (input_data.shape[0] // self.pool_size[0], input_data.shape[1] // self.pool_size[1], input_data.shape[2])\n",
    "        self.output = np.zeros(output_shape)\n",
    "        for i in range(output_shape[0]):\n",
    "            for j in range(output_shape[1]):\n",
    "                self.output[i, j] = np.mean(input_data[i*self.pool_size[0]:(i+1)*self.pool_size[0], j*self.pool_size[1]:(j+1)*self.pool_size[1], :], axis=(0, 1))\n",
    "        return self.output\n",
    "\n",
    "\n",
    "\n",
    "# Clase Red, conecta múltiples capas.\n",
    "class Network:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        self.loss = None\n",
    "        self.loss_prime = None\n",
    "\n",
    "    # add layer to network\n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    # set loss to use\n",
    "    def use(self, loss, loss_prime):\n",
    "        self.loss = loss\n",
    "        self.loss_prime = loss_prime\n",
    "\n",
    "    # predict output for given input\n",
    "    def predict(self, input_data):\n",
    "        if input_data.ndim == 1:  # Ajuste para arreglos unidimensionales\n",
    "            input_data = np.array([[x] for x in input_data])\n",
    "        samples = len(input_data)\n",
    "        result = []\n",
    "\n",
    "        for i in range(samples):\n",
    "            output = input_data[i]\n",
    "            for layer in self.layers:\n",
    "                output = layer.forward_propagation(output)\n",
    "            result.append(output)\n",
    "        return result\n",
    "\n",
    "    # train the network with Early Stopping\n",
    "    def fit(self, x_train, y_train, epochs, learning_rate, x_val=None, y_val=None, early_stopping=None):\n",
    "        if x_train[0].ndim == 1:\n",
    "            x_train = np.array([[x] for x in x_train])\n",
    "        samples = len(x_train)\n",
    "\n",
    "        for i in range(epochs):\n",
    "            err = 0\n",
    "            for j in range(samples):\n",
    "                output = x_train[j]\n",
    "                for layer in self.layers:\n",
    "                    output = layer.forward_propagation(output)\n",
    "\n",
    "                err += self.loss(y_train[j], output)\n",
    "\n",
    "                error = self.loss_prime(y_train[j], output)\n",
    "                for layer in reversed(self.layers):\n",
    "                    error = layer.backward_propagation(error, learning_rate)\n",
    "\n",
    "            err /= samples\n",
    "            err = np.mean(err)\n",
    "\n",
    "            print('epoch %d/%d   error=%f' % (i + 1, epochs, err))\n",
    "\n",
    "            # Validación y Early Stopping\n",
    "            if x_val is not None and y_val is not None and early_stopping is not None:\n",
    "                val_err = 0\n",
    "                for j in range(len(x_val)):\n",
    "                    val_output = x_val[j]\n",
    "                    for layer in self.layers:\n",
    "                        val_output = layer.forward_propagation(val_output)\n",
    "                    val_err += self.loss(y_val[j], val_output)\n",
    "                val_err /= len(x_val)\n",
    "                val_err = np.mean(val_err)\n",
    "\n",
    "                print('Validation error: %f' % val_err)\n",
    "\n",
    "                if early_stopping.should_stop(val_err):\n",
    "                    print('Early stopping at epoch %d' % (i + 1))\n",
    "                    break\n",
    "\n",
    "\n",
    "# Funciones de Activación\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_prime(x):\n",
    "    return 1 - np.tanh(x) ** 2\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_prime(x):\n",
    "    sig = sigmoid(x)\n",
    "    return sig * (1 - sig)\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_prime(x):\n",
    "    return np.where(x > 0, 1, 0)\n",
    "\n",
    "\n",
    "# Funciones de pérdida\n",
    "def mse(y_true, y_hat):\n",
    "    return (y_true - y_hat) ** 2\n",
    "\n",
    "def mse_prime(y_true, y_hat):\n",
    "    return 2 * (y_hat - y_true)\n",
    "\n",
    "def bce(y_true, y_hat):\n",
    "    epsilon = 1e-15\n",
    "    y_hat = np.clip(y_hat, epsilon, 1 - epsilon)\n",
    "    return -(y_true * np.log(y_hat) + (1 - y_true) * np.log(1 - y_hat))\n",
    "\n",
    "def bce_prime(y_true, y_hat):\n",
    "    epsilon = 1e-15\n",
    "    y_hat = np.clip(y_hat, epsilon, 1 - epsilon)\n",
    "    return -(y_true / y_hat) + (1 - y_true) / (1 - y_hat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4283c397-fb24-4485-ab6f-d433db0acacf",
   "metadata": {},
   "source": [
    "<h1>Entrenamos y imprimimos</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ecb5d62e-28e9-4496-acd7-83e4a7df6679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/30   error=0.228073\n",
      "Validation error: 0.122745\n",
      "epoch 2/30   error=0.121789\n",
      "Validation error: 0.110490\n",
      "epoch 3/30   error=0.081817\n",
      "Validation error: 0.095770\n",
      "epoch 4/30   error=0.054483\n",
      "Validation error: 0.108022\n",
      "epoch 5/30   error=0.040151\n",
      "Validation error: 0.112454\n",
      "epoch 6/30   error=0.026330\n",
      "Validation error: 0.094817\n",
      "epoch 7/30   error=0.013364\n",
      "Validation error: 0.102930\n",
      "epoch 8/30   error=0.006153\n",
      "Validation error: 0.087987\n",
      "epoch 9/30   error=0.002951\n",
      "Validation error: 0.084543\n",
      "epoch 10/30   error=0.001527\n",
      "Validation error: 0.085199\n",
      "epoch 11/30   error=0.001149\n",
      "Validation error: 0.085808\n",
      "epoch 12/30   error=0.000955\n",
      "Validation error: 0.086645\n",
      "epoch 13/30   error=0.000822\n",
      "Validation error: 0.087280\n",
      "epoch 14/30   error=0.000725\n",
      "Validation error: 0.087963\n",
      "Early stopping at epoch 14\n",
      "MATRIZ DE CONFUSIÓN para modelo ANN\n",
      "[[ 95   0   1   0   0   1   1   0   1   0]\n",
      " [  0 113   1   0   0   1   0   0   0   0]\n",
      " [  2   1  82   3   2   0   5   3   6   0]\n",
      " [  0   0   4  92   2   3   2   3   1   1]\n",
      " [  0   0   2   0  95   0   1   0   0   3]\n",
      " [  2   1   0   4   0  74   1   1   5   3]\n",
      " [  0   0   0   0   1   1  80   0   0   0]\n",
      " [  1   1   0   0   2   0   0  85   0   2]\n",
      " [  1   2   0   3   4   4   4   3  89   3]\n",
      " [  0   1   0   1   7   2   1   4   4  76]] \n",
      "\n",
      "La exactitud de testeo del modelo ANN es: 0.881\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "X_train_full, X_val, y_train_full, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "model = Network()\n",
    "\n",
    "# Necesitamos identificar cuantos nodos tiene nuestra entrada, y eso depende del tamaño de X.\n",
    "entrada_dim = len(X_train_full[0])\n",
    "\n",
    "model.add(FCLayer(entrada_dim, 128))\n",
    "model.add(ActivationLayer(relu, relu_prime))\n",
    "model.add(FCLayer(128, 64))\n",
    "model.add(ActivationLayer(sigmoid, sigmoid_prime))\n",
    "model.add(FCLayer(64, 10))\n",
    "model.add(ActivationLayer(sigmoid, sigmoid_prime))\n",
    "\n",
    "model.use(bce, bce_prime)\n",
    "\n",
    "# Inicializamos EarlyStopping\n",
    "early_stopping = EarlyStopping(patience=5, min_delta=0.001)\n",
    "\n",
    "model.fit(X_train_full, y_train_full, epochs=30, learning_rate=0.1, x_val=X_val, y_val=y_val, early_stopping=early_stopping)\n",
    "\n",
    "# Usamos el modelo para predecir sobre los datos de prueba (X_test)\n",
    "y_hat = model.predict(X_test)\n",
    "\n",
    "# Transformamos la salida en un vector one-hot encoded, es decir, obtenemos la clase predicha\n",
    "for i in range(len(y_hat)):\n",
    "    y_hat[i] = np.argmax(y_hat[i])\n",
    "\n",
    "# Reportamos los resultados del modelo\n",
    "matriz_conf = confusion_matrix(y_test, y_hat)\n",
    "print('MATRIZ DE CONFUSIÓN para modelo ANN')\n",
    "print(matriz_conf, '\\n')\n",
    "print('La exactitud de testeo del modelo ANN es: {:.3f}'.format(accuracy_score(y_test, y_hat)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d1b1cd-581c-4e7e-b209-1a6cc7ba5430",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
