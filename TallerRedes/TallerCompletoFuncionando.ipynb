{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2k_sAWlXgPs",
        "outputId": "1a18814b-0a0a-48b7-9e2c-f1647ee4772d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1/20   error=0.109713\n",
            "epoch 2/20   error=0.083046\n",
            "epoch 3/20   error=0.077856\n",
            "epoch 4/20   error=0.072762\n",
            "epoch 5/20   error=0.068089\n",
            "epoch 6/20   error=0.063889\n",
            "epoch 7/20   error=0.059695\n",
            "epoch 8/20   error=0.055402\n",
            "epoch 9/20   error=0.051271\n",
            "epoch 10/20   error=0.047379\n",
            "epoch 11/20   error=0.043711\n",
            "epoch 12/20   error=0.040377\n",
            "epoch 13/20   error=0.037426\n",
            "epoch 14/20   error=0.034859\n",
            "epoch 15/20   error=0.032612\n",
            "epoch 16/20   error=0.030639\n",
            "epoch 17/20   error=0.028899\n",
            "epoch 18/20   error=0.027322\n",
            "epoch 19/20   error=0.025901\n",
            "epoch 20/20   error=0.024651\n",
            "epoch 1/30   error=0.159089\n",
            "Validation loss: 0.11011066595590974\n",
            "epoch 2/30   error=0.098381\n",
            "Validation loss: 0.08950435934302037\n",
            "epoch 3/30   error=0.086513\n",
            "Validation loss: 0.08301718634284518\n",
            "epoch 4/30   error=0.081810\n",
            "Validation loss: 0.0793917440965683\n",
            "epoch 5/30   error=0.078485\n",
            "Validation loss: 0.07646178436578975\n",
            "epoch 6/30   error=0.075296\n",
            "Validation loss: 0.07373895606248258\n",
            "epoch 7/30   error=0.072049\n",
            "Validation loss: 0.07107395023674956\n",
            "epoch 8/30   error=0.068768\n",
            "Validation loss: 0.06845339806280162\n",
            "epoch 9/30   error=0.065524\n",
            "Validation loss: 0.06591808748358578\n",
            "epoch 10/30   error=0.062399\n",
            "Validation loss: 0.06349564767660777\n",
            "epoch 11/30   error=0.059444\n",
            "Validation loss: 0.06118705497392463\n",
            "epoch 12/30   error=0.056681\n",
            "Validation loss: 0.05900015927251717\n",
            "epoch 13/30   error=0.054107\n",
            "Validation loss: 0.05694954368433608\n",
            "epoch 14/30   error=0.051713\n",
            "Validation loss: 0.05504551842266638\n",
            "epoch 15/30   error=0.049489\n",
            "Validation loss: 0.05329068476738223\n",
            "epoch 16/30   error=0.047418\n",
            "Validation loss: 0.05167915482746849\n",
            "epoch 17/30   error=0.045487\n",
            "Validation loss: 0.050198705780742925\n",
            "epoch 18/30   error=0.043684\n",
            "Validation loss: 0.04883643368093397\n",
            "epoch 19/30   error=0.041999\n",
            "Validation loss: 0.047579310841733974\n",
            "epoch 20/30   error=0.040418\n",
            "Validation loss: 0.04641155649396425\n",
            "epoch 21/30   error=0.038929\n",
            "Validation loss: 0.045317805669830895\n",
            "epoch 22/30   error=0.037522\n",
            "Validation loss: 0.04428657868769287\n",
            "epoch 23/30   error=0.036189\n",
            "Validation loss: 0.04331151760275389\n",
            "epoch 24/30   error=0.034925\n",
            "Validation loss: 0.042391339292576616\n",
            "epoch 25/30   error=0.033729\n",
            "Validation loss: 0.041527457373705945\n",
            "epoch 26/30   error=0.032599\n",
            "Validation loss: 0.04072074918776897\n",
            "epoch 27/30   error=0.031536\n",
            "Validation loss: 0.03997111499530847\n",
            "epoch 28/30   error=0.030538\n",
            "Validation loss: 0.03927925478657242\n",
            "epoch 29/30   error=0.029606\n",
            "Validation loss: 0.03864528229551992\n",
            "epoch 30/30   error=0.028740\n",
            "Validation loss: 0.03806591370139579\n",
            "MATRIZ DE CONFUSIÃ“N para modelo ANN\n",
            "[[ 85   0   3   2   0   2   2   1   0   0]\n",
            " [  0 104   3   2   0   0   0   2   1   0]\n",
            " [  2   1  86   4   2   0   7   4   2   0]\n",
            " [  2   0   4  73   1   9   1   0   1   0]\n",
            " [  0   1   0   1  96   2   0   0   2   4]\n",
            " [  5   2   0  15   6  47   3   3   3   5]\n",
            " [  4   0   1   0   2   0  76   0   1   0]\n",
            " [  2   0   2   0   3   0   0  96   0   3]\n",
            " [  2   0   4   8   0   4   4   1  79   6]\n",
            " [  2   0   1   1   6   2   0   2   1  86]] \n",
            "\n",
            "La exactitud de testeo del modelo ANN es: 0.828\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from keras.datasets import mnist\n",
        "import random\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "\n",
        "\n",
        "class Layer:\n",
        "    \"\"\"Base class for a layer.\"\"\"\n",
        "    def __init__(self):\n",
        "        self.input = None\n",
        "        self.output = None\n",
        "\n",
        "    def forward_propagation(self, input_data):\n",
        "        \"\"\"Computes the output of a layer for a given input.\"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def backward_propagation(self, output_error, learning_rate):\n",
        "        \"\"\"Computes dE/dX for a given dE/dY (and updates parameters if any).\"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "\n",
        "class FCLayer(Layer):\n",
        "    \"\"\"Fully connected layer.\"\"\"\n",
        "    def __init__(self, input_size: int, output_size: int):\n",
        "        self.weights = np.random.rand(input_size, output_size) - 0.5\n",
        "        self.bias = np.random.rand(1, output_size) - 0.5\n",
        "\n",
        "    def forward_propagation(self, input_data):\n",
        "        self.input = input_data\n",
        "        self.output = np.dot(self.input, self.weights) + self.bias\n",
        "        return self.output\n",
        "\n",
        "    def backward_propagation(self, output_error, learning_rate):\n",
        "        input_error = np.dot(output_error, self.weights.T)\n",
        "        weights_error = np.dot(self.input.T, output_error)\n",
        "        self.weights -= learning_rate * weights_error\n",
        "        self.bias -= learning_rate * output_error\n",
        "        return input_error\n",
        "\n",
        "\n",
        "class ActivationLayer(Layer):\n",
        "    \"\"\"Activation layer.\"\"\"\n",
        "    def __init__(self, activation, activation_prime):\n",
        "        self.activation = activation\n",
        "        self.activation_prime = activation_prime\n",
        "\n",
        "    def forward_propagation(self, input_data):\n",
        "        self.input = input_data\n",
        "        self.output = self.activation(self.input)\n",
        "        return self.output\n",
        "\n",
        "    def backward_propagation(self, output_error, learning_rate):\n",
        "        return self.activation_prime(self.input) * output_error\n",
        "\n",
        "class ConvLayer(Layer):\n",
        "    \"\"\"Convolutional layer.\"\"\"\n",
        "    def __init__(self, num_filters, kernel_size, stride, padding, activation=None):\n",
        "        self.num_filters = num_filters\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "        self.weights = np.random.rand(num_filters, kernel_size, kernel_size) - 0.5\n",
        "        self.bias = np.random.rand(num_filters) - 0.5\n",
        "        self.activation = activation\n",
        "\n",
        "    def forward_propagation(self, input_data):\n",
        "        # Implement convolutional forward pass\n",
        "        self.input=input_data\n",
        "        if self.activation:\n",
        "            self.output = self.activation(self.input)\n",
        "        else:\n",
        "            self.output = self.input\n",
        "        return self.output\n",
        "\n",
        "    def backward_propagation(self, output_error, learning_rate):\n",
        "        # Implement convolutional backward pass\n",
        "        pass\n",
        "\n",
        "class PoolingLayer(Layer):\n",
        "    \"\"\"Base class for pooling layers.\"\"\"\n",
        "    def __init__(self, pool_size, stride):\n",
        "        self.pool_size = pool_size\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward_propagation(self, input_data):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def backward_propagation(self, output_error, learning_rate):\n",
        "        raise NotImplementedError\n",
        "\n",
        "\n",
        "class MaxPoolingLayer(PoolingLayer):\n",
        "    \"\"\"Max pooling layer.\"\"\"\n",
        "    def __init__(self, pool_size, stride):\n",
        "        super().__init__(pool_size, stride)\n",
        "\n",
        "    def forward_propagation(self, input_data):\n",
        "        self.input = input_data\n",
        "        output = np.zeros((input_data.shape[0], input_data.shape[1] // self.stride, input_data.shape[2] // self.stride))\n",
        "        for i in range(0, input_data.shape[1], self.stride):\n",
        "            for j in range(0, input_data.shape[2], self.stride):\n",
        "                output[:, i // self.stride, j // self.stride] = np.max(input_data[:, i:i + self.pool_size, j:j + self.pool_size], axis=(1, 2))\n",
        "        self.output = output\n",
        "        return self.output\n",
        "\n",
        "    def backward_propagation(self, output_error, learning_rate):\n",
        "        input_error = np.zeros(self.input.shape)\n",
        "        for i in range(0, self.input.shape[1], self.stride):\n",
        "            for j in range(0, self.input.shape[2], self.stride):\n",
        "                max_idx = np.argmax(self.input[:, i:i + self.pool_size, j:j + self.pool_size], axis=(1, 2))\n",
        "                input_error[:, i:i + self.pool_size, j:j + self.pool_size] = output_error[:, i // self.stride, j // self.stride][:, np.newaxis, np.newaxis] * (max_idx == np.arange(self.pool_size * self.pool_size)[:, np.newaxis, np.newaxis])\n",
        "        return input_error\n",
        "\n",
        "\n",
        "class AveragePoolingLayer(PoolingLayer):\n",
        "    \"\"\"Average pooling layer.\"\"\"\n",
        "    def __init__(self, pool_size, stride):\n",
        "        super().__init__(pool_size, stride)\n",
        "\n",
        "    def forward_propagation(self, input_data):\n",
        "        self.input = input_data\n",
        "        output = np.zeros((input_data.shape[0], input_data.shape[1] // self.stride, input_data.shape[2] // self.stride))\n",
        "        for i in range(0, input_data.shape[1], self.stride):\n",
        "            for j in range(0, input_data.shape[2], self.stride):\n",
        "                output[:, i // self.stride, j // self.stride] = np.mean(input_data[:, i:i + self.pool_size, j:j + self.pool_size], axis=(1, 2))\n",
        "        self.output = output\n",
        "        return self.output\n",
        "\n",
        "    def backward_propagation(self, output_error, learning_rate):\n",
        "        input_error = np.zeros(self.input.shape)\n",
        "        for i in range(0, self.input.shape[1], self.stride):\n",
        "            for j in range(0, self.input.shape[2], self.stride):\n",
        "                input_error[:, i:i + self.pool_size, j:j + self.pool_size] = output_error[:, i // self.stride, j // self.stride][:, np.newaxis, np.newaxis] / (self.pool_size ** 2)\n",
        "        return input_error\n",
        "\n",
        "class FlattenLayer(Layer):\n",
        "    \"\"\"Flatten layer.\"\"\"\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def forward_propagation(self, input_data):\n",
        "        self.input = input_data\n",
        "        self.output = input_data.reshape(input_data.shape[0], -1)\n",
        "        return self.output\n",
        "\n",
        "    def backward_propagation(self, output_error, learning_rate):\n",
        "        input_error = output_error.reshape(self.input.shape)\n",
        "        return input_error\n",
        "\n",
        "class DropoutLayer(Layer):\n",
        "    def __init__(self, rate):\n",
        "        # 'rate' es la fracciÃ³n de neuronas que se apagan, p.ej., 0.2 significa que el 20% se apagarÃ¡n\n",
        "        super().__init__()\n",
        "        self.rate = rate\n",
        "        self.mask = None\n",
        "\n",
        "    def forward_propagation(self, input_data, training=True):\n",
        "        if training:\n",
        "            # Crear una mÃ¡scara binaria aleatoria de la misma forma que los datos de entrada\n",
        "            self.mask = np.random.binomial(1, 1 - self.rate, size=input_data.shape)\n",
        "            # Apagar las neuronas segÃºn la mÃ¡scara y escalar los valores restantes\n",
        "            self.output = input_data * self.mask / (1 - self.rate)\n",
        "        else:\n",
        "            # En la fase de evaluaciÃ³n, no se usa Dropout, simplemente pasamos los datos\n",
        "            self.output = input_data\n",
        "        return self.output\n",
        "\n",
        "    def backward_propagation(self, output_error, learning_rate):\n",
        "        # La mÃ¡scara tambiÃ©n se aplica en la retropropagaciÃ³n\n",
        "        return output_error * self.mask / (1 - self.rate)\n",
        "\n",
        "    def backward_propagation(self, output_error, learning_rate):\n",
        "        # Implementar la retropropagaciÃ³n\n",
        "        # Este es un ejemplo simplificado, necesitarÃ¡s implementarlo adecuadamente\n",
        "        input_error = np.zeros(self.input.shape)\n",
        "        for f in range(self.filters):\n",
        "            for i in range(self.output.shape[1]):\n",
        "                for j in range(self.output.shape[2]):\n",
        "                    input_error[:, i:i + self.kernel_size[0], j:j + self.kernel_size[1]] += (\n",
        "                        output_error[f, i, j] * self.weights[f]\n",
        "                    )\n",
        "                    self.weights[f] -= learning_rate * output_error[f, i, j] * self.input[:, i:i + self.kernel_size[0], j:j + self.kernel_size[1]]\n",
        "                    self.bias[f] -= learning_rate * output_error[f, i, j]\n",
        "        return input_error\n",
        "\n",
        "# Clase EarlyStopping\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=5, min_delta=0):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.best_loss = None\n",
        "        self.wait = 0\n",
        "\n",
        "    def should_stop(self, val_loss):\n",
        "        if self.best_loss is None:\n",
        "            self.best_loss = val_loss\n",
        "            return False\n",
        "        if self.best_loss - val_loss > self.min_delta:\n",
        "            self.best_loss = val_loss\n",
        "            self.wait = 0\n",
        "        else:\n",
        "            self.wait += 1\n",
        "        return self.wait >= self.patience\n",
        "\n",
        "\n",
        "\n",
        "class Network:\n",
        "    \"\"\"Neural network.\"\"\"\n",
        "    def __init__(self):\n",
        "        self.layers = []\n",
        "        self.loss = None\n",
        "        self.loss_prime = None\n",
        "\n",
        "    def add(self, layer: Layer):\n",
        "        \"\"\"Adds a layer to the network.\"\"\"\n",
        "        self.layers.append(layer)\n",
        "\n",
        "    def use(self, loss, loss_prime):\n",
        "        \"\"\"Sets the loss function and its derivative.\"\"\"\n",
        "        self.loss = loss\n",
        "        self.loss_prime = loss_prime\n",
        "\n",
        "    def predict(self, input_data):\n",
        "        \"\"\"Predicts the output for a given input.\"\"\"\n",
        "        input_data = np.array([[x] for x in input_data])\n",
        "        samples = len(input_data)\n",
        "        result = []\n",
        "\n",
        "        for i in range(samples):\n",
        "            output = input_data[i]\n",
        "            for layer in self.layers:\n",
        "                output = layer.forward_propagation(output)\n",
        "            result.append(output)\n",
        "        return result\n",
        "    def fit(self, x_train, y_train, epochs, learning_rate, x_val=None, y_val=None, early_stopping=None):\n",
        "        x_train = np.array([[x] for x in x_train])\n",
        "        samples = len(x_train)\n",
        "        # Loop de Entrenamiento\n",
        "        for i in range(epochs):\n",
        "            err = 0\n",
        "            for j in range(samples):\n",
        "                output = x_train[j]\n",
        "                for layer in self.layers:\n",
        "                    output = layer.forward_propagation(output)\n",
        "                err += self.loss(y_train[j], output)\n",
        "                error = self.loss_prime(y_train[j], output)\n",
        "                for layer in reversed(self.layers):\n",
        "                    error = layer.backward_propagation(error, learning_rate)\n",
        "\n",
        "            # calculate average error on all samples\n",
        "            err /= samples\n",
        "            # calculamos el error promedio entre nodos de salida.\n",
        "            err = np.mean(err)\n",
        "            # Imprimomos el error promedio de cada Ã©poca, mÃ¡s que nada\n",
        "            print('epoch %d/%d   error=%f' % (i+1, epochs, err))\n",
        "\n",
        "              # Early stopping check if validation data is provided\n",
        "            if x_val is not None and y_val is not None and early_stopping is not None:\n",
        "                val_loss = self.validate(x_val, y_val)  # Assuming you have a validate method\n",
        "                print(f\"Validation loss: {val_loss}\")\n",
        "                if early_stopping.should_stop(val_loss):\n",
        "                    print(\"Early stopping triggered!\")\n",
        "                    break\n",
        "\n",
        "    def validate(self, x_val, y_val):\n",
        "        \"\"\"Calculates the loss on the validation set.\"\"\"\n",
        "        x_val = np.array([[x] for x in x_val])\n",
        "        val_loss = 0\n",
        "        for j in range(len(x_val)):\n",
        "            output = x_val[j]\n",
        "            for layer in self.layers:\n",
        "                output = layer.forward_propagation(output)\n",
        "            val_loss += self.loss(y_val[j], output)\n",
        "        return np.mean(val_loss) / len(x_val)\n",
        "\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_prime(x):\n",
        "    return sigmoid(x) * (1 - sigmoid(x))\n",
        "\n",
        "def tanh(x):\n",
        "    return np.tanh(x)\n",
        "\n",
        "def tanh_prime(x):\n",
        "    return 1 - np.tanh(x)**2\n",
        "\n",
        "def mse(y_true, y_pred):\n",
        "    \"\"\"Calculates the Mean Squared Error.\"\"\"\n",
        "    return np.mean(np.power(y_true - y_pred, 2))\n",
        "\n",
        "def mse_prime(y_true, y_pred):\n",
        "    \"\"\"Calculates the derivative of the Mean Squared Error.\"\"\"\n",
        "    return 2 * (y_pred - y_true) / y_true.size\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# No necesitamos tantos datos.\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "#random.seed(123) # Vamos a controlar la aleatoriedad en adelante.\n",
        "X, y = zip(*random.sample(list(zip(X_train, y_train)), 2000))\n",
        "\n",
        "# SÃ­ necesitamos que la forma de X sea la de un vector, en lugar de una matriz.\n",
        "X, y = np.array(X, dtype='float64'), np.array(y, dtype='float64')\n",
        "X = np.reshape(X, (X.shape[0], -1))\n",
        "\n",
        "# Normalizamos Min-Max\n",
        "X= MinMaxScaler().fit_transform(X)\n",
        "\n",
        "# Dividimos la muestra en dos, una para entrenar y otra para testing, como tenemos\n",
        "# muestra de sobra nos damos el lujo de testear con la misma cantidad que entrenamos.\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=123)\n",
        "\n",
        "# Necesitamos que y_train sea un valor categÃ³rico, en lugar de un dÃ­gito entero.\n",
        "y_train_value = y_train # Guardaremos y_train como valor para un observaciÃ³n mÃ¡s abajo.\n",
        "from keras.utils import to_categorical\n",
        "y_train = to_categorical(y_train)\n",
        "\n",
        "# Necesitamos identificar cuantos nodos tiene nuestra entrada, y eso depende del tamaÃ±o de X.\n",
        "entrada_dim = len(X_train[0])\n",
        "\n",
        "# Crear instancia de Network\n",
        "model = Network()\n",
        "\n",
        "# Agregamos capas al modelo\n",
        "model.add(FCLayer(entrada_dim, 16))\n",
        "model.add(ActivationLayer(tanh, tanh_prime))\n",
        "model.add(FCLayer(16, 10))\n",
        "model.add(ActivationLayer(sigmoid, sigmoid_prime))\n",
        "\n",
        "# Usar el modelo creado\n",
        "model.use(mse, mse_prime)\n",
        "model.fit(X_train, y_train, epochs=20, learning_rate=0.1)\n",
        "\n",
        "# Usamos el modelo para predecir sobre el conjunto de prueba\n",
        "y_hat = model.predict(X_test)\n",
        "\n",
        "# Transformamos la salida en un vector one-hot encoded, es decir 0s y un 1.\n",
        "for i in range(len(y_hat)):\n",
        "    y_hat[i] = np.argmax(y_hat[i][0])\n",
        "\n",
        "# Reportamos los resultados del modelo\n",
        "matriz_conf = confusion_matrix(y_test, y_hat)\n",
        "\n",
        "# The following lines are causing the issue:\n",
        "# model.add(ConvLayer(num_filters=32, kernel_size=3, stride=1, padding=1))\n",
        "# model.add(MaxPoolingLayer(pool_size=2, stride=2))\n",
        "# model.add(FlattenLayer())\n",
        "# model.add(FCLayer(input_size=128, output_size=10))\n",
        "# model.add(DropoutLayer(rate=0.2))\n",
        "\n",
        "# Instead, if you want to add more fully connected layers:\n",
        "model.add(FCLayer(10, 10)) # Example of another fully connected layer\n",
        "model.add(ActivationLayer(sigmoid, sigmoid_prime))\n",
        "\n",
        "X_train_full, X_val, y_train_full, y_val = train_test_split(\n",
        "    X_train, y_train, test_size=0.2, random_state=123)  # Adjust test_size as needed\n",
        "# Inicializamos EarlyStopping\n",
        "early_stopping = EarlyStopping(patience=5, min_delta=0.001)\n",
        "\n",
        "model.fit(\n",
        "    X_train_full,\n",
        "    y_train_full,\n",
        "    epochs=30,\n",
        "    learning_rate=0.1,\n",
        "    x_val=X_val,\n",
        "    y_val=y_val,\n",
        "    early_stopping=early_stopping,\n",
        ")\n",
        "\n",
        "print('MATRIZ DE CONFUSIÃ“N para modelo ANN')\n",
        "print(matriz_conf,'\\n')\n",
        "print('La exactitud de testeo del modelo ANN es: {:.3f}'.format(accuracy_score(y_test,y_hat)))\n",
        "\n",
        "\n"
      ]
    }
  ]
}