{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26092db2",
   "metadata": {},
   "source": [
    "<head><b><center>MACHINE LEARNING</center>\n",
    "\n",
    "<center>Redes Neuronales Artificiales</center>\n",
    "    \n",
    "\n",
    "<center>Profesor: Gabriel Jara </center></head><br>\n",
    "El presente Jupyter Notebook busca:\n",
    "<ul>\n",
    "    <li>Retomar conceptos sobre Redes Neuronales Artificiales (ANN).</li>\n",
    "    <li>Profundizar en el mecanismo de aprendizaje de las ANN</li>\n",
    "    <li>Analizar la implementación de una red neuronal sin apoyo de librerías especializadas.</li>\n",
    "</ul>\n",
    "\n",
    "Fuentes: \n",
    "<ul>\n",
    "    <li><a href=https://towardsdatascience.com/math-neural-network-from-scratch-in-python-d6da9f29ce65>Neural Network from scratch in Python</a></li>\n",
    "    <li>Imágenes han sido robadas de internet y son enlaces a su correspondiente fuente. </li>\n",
    "</ul> \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d672aa9",
   "metadata": {},
   "source": [
    "<b>En el capítulo anterior </b>\n",
    "\n",
    "Presentamos las Redes Neuronales Artificiales como modelo de aprendizaje. Identificamos el <b><i>Perceptron</i></b> como unidad básica de este modelo, cuya función emula el rol de una neurona: percibe un estímulo según el cual se activa (o no) y con ello transmite una señal. \n",
    "<br> <img src=https://images.deepai.org/glossary-terms/perceptron-6168423.jpg\n",
    "          width=\"400\"/>\n",
    "\n",
    "La naturaleza limitada de un perceptrón no permitiría que resolviera problemas muy complejos. En el contexto de Clasificación, sólo podría clasificar observaciones que fueran linealmente separables. Pero como nodos en una red organizada en capas densamente conectadas, alcanzan capacidades mucho mayores. Esta es la definición básica de una red neuronal, denominada <b><i>Multi-Layer Perceptron</i></b>. \n",
    "<br> <img src=https://miro.medium.com/v2/resize:fit:563/1*4_BDTvgB6WoYVXyxO8lDGA.png\n",
    "          width=\"350\"/>\n",
    "          \n",
    "\n",
    "\n",
    "\n",
    "          "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836482d2",
   "metadata": {},
   "source": [
    "<b>RED NEURONAL PASO A PASO</b>\n",
    "\n",
    "Si bien regularmente vamos a aprovechar implementaciones ya disponibles, puede ocurrir que en ocasiones requiramos alguna modificación no prevista, o necesitamos controlar muy detalladamente el algoritmo que se ejecuta con la Red Neuronal. Por eso vamos a adaptar y analizar una implementación que fue publicada por Omar Aflak en https://towardsdatascience.com/math-neural-network-from-scratch-in-python-d6da9f29ce65, la cual hemos modificado para fines de esta experiencia.          \n",
    "\n",
    "Iremos comentando por partes la implementación. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e2d6a9",
   "metadata": {},
   "source": [
    "<b>Clase Layer</b>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a33064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base class\n",
    "class Layer:\n",
    "    def __init__(self):\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "\n",
    "    # computes the output Y of a layer for a given input X\n",
    "    def forward_propagation(self, input):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    # computes dE/dX for a given dE/dY (and update parameters if any)\n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e14ef99",
   "metadata": {},
   "source": [
    "Esta es la clase base para las capas en la red neuronal. Tiene dos funciones principales que deben ser implementadas por las clases hijas: forward_propagation y backward_propagation. En las redes neuronales, durante la fase de entrenamiento, los datos pasan hacia adelante a través de la red (forward propagation) y luego los errores se propagan hacia atrás (backward propagation) para ajustar los pesos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98b30d9",
   "metadata": {},
   "source": [
    "<b>Clase FCLayer</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5fdbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# inherit from base class Layer\n",
    "class FCLayer(Layer):\n",
    "    # input_size = number of input neurons\n",
    "    # output_size = number of output neurons\n",
    "    def __init__(self, input_size, output_size):\n",
    "        # Sólo con fines pedagógicos, incluímos un seed para controlar la aleatoriedad\n",
    "        # de los pesos iniciales en la red. \n",
    "        np.random.seed(1234)\n",
    "        self.weights = np.random.rand(input_size, output_size, ) - 0.5\n",
    "        self.bias = np.random.rand(1, output_size) - 0.5\n",
    "\n",
    "    # returns output for a given input\n",
    "    def forward_propagation(self, input_data):\n",
    "        self.input = input_data\n",
    "        self.output = np.dot(self.input, self.weights) + self.bias\n",
    "        return self.output\n",
    "\n",
    "    # computes dE/dW, dE/dB for a given output_error=dE/dY. Returns input_error=dE/dX.\n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        input_error = np.dot(output_error, self.weights.T)\n",
    "        weights_error = np.dot(self.input.T, output_error)\n",
    "\n",
    "        # update parameters\n",
    "        self.weights -= learning_rate * weights_error\n",
    "        self.bias -= learning_rate * output_error\n",
    "        return input_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982a8ea7",
   "metadata": {},
   "source": [
    "Esta es una capa completamente conectada (Fully-Connected Layer) de la red neuronal. Cada neurona en una capa está conectada a todas las neuronas en la capa anterior y todas las neuronas en la capa siguiente.\n",
    "\n",
    "<ul>\n",
    "    <li>En el método <b>__init__</b>, se inicializan los pesos y los sesgos con valores aleatorios.</li>\n",
    "    <li>El método <b>forward_propagation</b> toma la entrada y la multiplica con los pesos (y suma los sesgos) para obtener la salida. Esto se puede expresar con la ecuación: $y = W \\cdot x + b$, donde $W$ son los pesos, $x$ es la entrada, $b$ es el sesgo y $y$ es la salida.</li>\n",
    "    <li>El método <b>backward_propagation</b> se encarga de calcular los gradientes y actualizar los pesos y sesgos. La ecuación relevante para calcular el error de entrada es $E_{in} = E_{out} \\cdot W^T$, donde $E_{out}$ es el error de salida y $W^T$ es la transposición de la matriz de pesos. Luego se actualizan los pesos y sesgos con las ecuaciones: $W = W - \\alpha \\cdot E_{out} \\cdot X^T$, $b = b - \\alpha \\cdot E_{out}$, donde $\\alpha$ es la <i>learning rate</i> y $X$ es la entrada.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76405aaa",
   "metadata": {},
   "source": [
    "¿Dé dónde salen las transformaciones realizadas en el paso <i>backward propagation</i>? Demostraremos a continuación la más importante de estas, la que permite actualizar los pesos de la red. Entender esto no sólo justifica la implementación de red neuronal que tenemos a la vista, nos lleva además a entender el cómo aprende una red neuronal artificial.  \n",
    "<br> <img src=https://i.imgflip.com/1rks94.jpg?a469896\n",
    "          width=\"100\"/></br>\n",
    "\n",
    "Supongamos una red neuronal de dos capas: una entrada de tamaño $i$ y una salida de tamaño $j$. Queremos minimizar el error de salida de la red, para lo cual pretendemos usar el método del gradiente. Esto quiere decir que queremos actualizar la salida de la red $\\hat{y}$ en la dirección de la derivada del error: $$\\frac{\\partial E}{\\partial \\hat{y}}$$\n",
    "\n",
    "Conocemos la función de pérdida $E$, así que conocemos su derivada respecto a $\\hat{y}$. Pero no podemos manipular $\\hat{y}$ directamente, sólo podemos hacer que cambie modificando los parámetros de la red, es decir los pesos $w$ que conectan cada neurona de la primera capa con cada neurona de la segunda capa, que conforman la matriz $W_{i \\times j}$. \n",
    "\n",
    "$$ W = \\begin{bmatrix}\n",
    "  w_{11} & w_{12} & \\dots & w_{1j} \\\\\n",
    "  w_{21} & w_{22} & \\dots & w_{2j} \\\\\n",
    "  \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "  w_{i1} & w_{i2} & \\dots & w_{ij}\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "El método del gradiente indica que la actualización de la matriz de pesos que requerimos sería:\n",
    "$$W \\leftarrow W - \\alpha \\frac{\\partial E}{\\partial W}$$\n",
    "\n",
    "<br> <img src=https://easyai.tech/wp-content/uploads/2019/01/tiduxiajiang-1.png\n",
    "          width=\"400\"/></br>\n",
    "\n",
    "Donde la matriz gradiente $\\frac{\\partial E}{\\partial W}$ es:\n",
    "\n",
    "$$ \\frac{\\partial E}{\\partial W} = \\begin{bmatrix}\n",
    "  \\frac{\\partial E}{\\partial w_{11}} & \\frac{\\partial E}{\\partial w_{12}} & \\dots & \\frac{\\partial E}{\\partial w_{1j}} \\\\\n",
    "  \\frac{\\partial E}{\\partial w_{21}} & \\frac{\\partial E}{\\partial w_{22}} & \\dots & \\frac{\\partial E}{\\partial w_{2j}} \\\\\n",
    "  \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "  \\frac{\\partial E}{\\partial w_{i1}} & \\frac{\\partial E}{\\partial w_{i2}} & \\dots & \\frac{\\partial E}{\\partial w_{ij}}\n",
    "\\end{bmatrix}\n",
    "$$ \n",
    "\n",
    "Por lo que la actualización a nivel de cada peso sería:\n",
    "$$w_{ij} \\leftarrow w_{ij} - \\alpha \\frac{\\partial E}{\\partial w_{ij}}$$\n",
    "\n",
    "Pero, ¿cuánto vale $\\frac{\\partial E}{\\partial w_{ij}}$? Usando la regla de la cadena (función multivariable) lo podemos expresar como:\n",
    "\n",
    "$$ \\frac{\\partial E}{\\partial w_{ij}} = \\frac{\\partial E}{\\partial {\\hat{y}_1}}\\frac{\\partial {\\hat{y}_1}}{\\partial w_{ij}}+\\frac{\\partial E}{\\partial {\\hat{y}_2}}\\frac{\\partial {\\hat{y}_2}}{\\partial w_{ij}}+\\dots+\\frac{\\partial E}{\\partial {\\hat{y}_j}}\\frac{\\partial {\\hat{y}_j}}{\\partial w_{ij}} $$\n",
    "\n",
    "Pero, ¿cuánto vale $\\frac{\\partial {\\hat{y}_1}}{\\partial w_{ij}}$? \n",
    "\n",
    "Observemos que $\\hat{y}_{1}$ no depende de $w_{ij}$, así que esa componente de la derivada es cero. Lo mismo ocurre para todas las salidas $\\hat{y}$, salvo $\\hat{y}_j$ que sí depende de $w_{ij}$. \n",
    "\n",
    "Pero, ¿cuánto vale $\\frac{\\partial {\\hat{y}_j}}{\\partial w_{ij}}$?\n",
    "\n",
    "Recordemos que:\n",
    "$${\\hat{y}}_{j} = x_{1}*w_{1j} + x_{2}*w_{2j} + \\dots + x_{i}*w_{ij} + b $$\n",
    "\n",
    "De lo que obtenemos que:\n",
    "$$\\frac{\\partial {\\hat{y}_j}}{\\partial w_{ij}} = x_{i}$$\n",
    "\n",
    "De lo que sale que:\n",
    "$$ \\frac{\\partial E}{\\partial w_{ij}} = \\frac{\\partial E}{\\partial {\\hat{y}_j}}x_{i} $$\n",
    "\n",
    "Este resultado es válido para cualquier $i$ y cualquier $j$ en $w_{ij}$. Así que podemos volver a expresar la matriz gradiente de los pesos como: \n",
    "\n",
    "$$ \\frac{\\partial E}{\\partial W} = \\begin{bmatrix}\n",
    "  \\frac{\\partial E}{\\partial \\hat{y}_{1}}x_{1} & \\frac{\\partial E}{\\partial \\hat{y}_{2}}x_{1} & \\dots & \\frac{\\partial E}{\\partial \\hat{y}_{j}}x_{1} \\\\\n",
    "  \\frac{\\partial E}{\\partial \\hat{y}_{1}}x_{2} & \\frac{\\partial E}{\\partial \\hat{y}_{2}}x_{2} & \\dots & \\frac{\\partial E}{\\partial \\hat{y}_{j}}x_{2} \\\\\n",
    "  \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "  \\frac{\\partial E}{\\partial \\hat{y}_{1}}x_{i} & \\frac{\\partial E}{\\partial \\hat{y}_{2}}x_{i} & \\dots & \\frac{\\partial E}{\\partial \\hat{y}_{j}}x_{i}\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "  x_{1} & x_{2} & \\dots & x_{i}\n",
    "\\end{bmatrix}\n",
    "\\cdot\n",
    "\\begin{bmatrix}\n",
    "  \\frac{\\partial E}{\\partial \\hat{y}_{1}} \\\\\n",
    "  \\frac{\\partial E}{\\partial \\hat{y}_{2}} \\\\\n",
    "  \\vdots \\\\\n",
    "  \\frac{\\partial E}{\\partial \\hat{y}_{j}}\n",
    "\\end{bmatrix}\n",
    "$$ \n",
    "\n",
    "De lo que resulta que:\n",
    "$$ \\frac{\\partial E}{\\partial W} = X^T \\cdot \\frac{\\partial E}{\\partial \\hat{y}} $$\n",
    "\n",
    "Acá conectamos con el punto inicial de este análisis, conocemos la función de pérdida $E$, así que conocemos su derivada respecto a $\\hat{y}$. Por lo tanto, cuando ocurre backpropagation tenemos que actualizar los pesos de acuerdo a la siguiente fórmula:\n",
    "\n",
    "$$W \\leftarrow W - \\alpha * X^T \\cdot \\frac{\\partial E}{\\partial \\hat{y}}$$\n",
    "\n",
    "Recordemos que $\\alpha$ corresponde al <b><i>learning date</i></b>, que es la tasa con que castigamos el aprendizaje para fines facilitar convergencia más rápida pero también mitigando el riesgo de caer en óptimos locales o enfrentar no convergencia\n",
    "<br> <img src=https://www.jeremyjordan.me/content/images/2018/02/Screen-Shot-2018-02-24-at-11.47.09-AM.png\n",
    "          width=\"700\"/></br>\n",
    "\n",
    "\n",
    "El análisis anterior se ha realizado sobre el supuesto una red con una capa de entrada y una de salida. En redes más profundas se repite la misma mecánica durante el aprendizaje, capa por capa. De allí en nombre de esta fase: <b><i>backward propagation</i></b>. \n",
    "<br> <img src=https://miro.medium.com/v2/resize:fit:828/format:webp/1*0QPRST83oBicKPE_R4biJA.png\n",
    "          width=\"500\"/></br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e34f48a",
   "metadata": {},
   "source": [
    "<b>Clase ActivationLayer</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2761d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inherit from base class Layer\n",
    "class ActivationLayer(Layer):\n",
    "    def __init__(self, activation, activation_prime):\n",
    "        self.activation = activation\n",
    "        self.activation_prime = activation_prime\n",
    "\n",
    "    # returns the activated input\n",
    "    def forward_propagation(self, input_data):\n",
    "        self.input = input_data\n",
    "        self.output = self.activation(self.input)\n",
    "        return self.output\n",
    "\n",
    "    # Returns input_error=dE/dX for a given output_error=dE/dY.\n",
    "    # learning_rate is not used because there is no \"learnable\" parameters.\n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        return self.activation_prime(self.input) * output_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17f7636",
   "metadata": {},
   "source": [
    "Esta capa aplica una función de activación a las salidas de la capa anterior. Necesita recibir como parámetros de iniciación la respectiva función de activación y la derivada de la función de activación. Los pesos (incluida la constante se sesgo) se inician en valores aleatorios. En el método forward_propagation, simplemente aplica la función de activación a la entrada. En el método backward_propagation, se calcula el error de entrada como el error de salida multiplicado por la derivada de la función de activación."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6cd619",
   "metadata": {},
   "source": [
    "<b>Clase Network</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0d8092",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        self.loss = None\n",
    "        self.loss_prime = None\n",
    "\n",
    "    # add layer to network\n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    # set loss to use\n",
    "    def use(self, loss, loss_prime):\n",
    "        self.loss = loss\n",
    "        self.loss_prime = loss_prime\n",
    "\n",
    "    # predict output for given input\n",
    "    def predict(self, input_data):\n",
    "        # sample dimension first\n",
    "        input_data = np.array([[x] for x in input_data])\n",
    "        samples = len(input_data)\n",
    "        result = []\n",
    "\n",
    "        # run network over all samples\n",
    "        for i in range(samples):\n",
    "            # forward propagation\n",
    "            output = input_data[i]\n",
    "            for layer in self.layers:\n",
    "                output = layer.forward_propagation(output)\n",
    "            result.append(output)\n",
    "        return result\n",
    "\n",
    "    # train the network\n",
    "    def fit(self, x_train, y_train, epochs, learning_rate):\n",
    "        # sample dimension first\n",
    "        x_train = np.array([[x] for x in x_train])\n",
    "        samples = len(x_train)\n",
    "        # training loop\n",
    "        for i in range(epochs):\n",
    "            err = 0\n",
    "            for j in range(samples):\n",
    "                # recurrimos al método Predict que hace fordward propagation\n",
    "                output = self.predict(x_train[j])[0] # La respuesta es un array, desempacamos.\n",
    "\n",
    "                # compute loss (for display purpose only)\n",
    "                err += self.loss(y_train[j], output)\n",
    "\n",
    "                # backward propagation\n",
    "                error = self.loss_prime(y_train[j], output)\n",
    "                for layer in reversed(self.layers):\n",
    "                    error = layer.backward_propagation(error, learning_rate)\n",
    "\n",
    "            # calculate average error on all samples\n",
    "            err /= samples\n",
    "            \n",
    "            # Para usar en clasificación (con más de dos clases)\n",
    "            # calculamos el error promedio entre nodos de salida.\n",
    "            err = np.mean(err)\n",
    "            \n",
    "            # Imprimomos el error promedio de cada época, más que nada\n",
    "            # para seguimiento del aprendizaje. \n",
    "            print('epoch %d/%d   error=%f' % (i+1, epochs, err))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729715e9",
   "metadata": {},
   "source": [
    "Esta es la red neuronal en sí. Contiene una lista de capas y la función de pérdida a utilizar, así como la derivada en $y$ de la misma.\n",
    "    \n",
    "<ul>\n",
    "    <li>El método <b>add</b> se utiliza para añadir capas a la red.</li>\n",
    "    <li>El método <b>use</b> se utiliza para establecer la función de pérdida.</li>\n",
    "    <li>El método <b>predict</b> se utiliza para calcular las salidas de la red para un conjunto dado de entradas.</li>\n",
    "    <li>El método <b>fit</b> se utiliza para entrenar la red con un conjunto de datos de entrenamiento. Durante cada época, para cada muestra, primero se realiza la propagación hacia adelante para calcular las salidas, y luego se realiza la propagación hacia atrás para actualizar los pesos.</li>\n",
    "</ul>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fc3b3e",
   "metadata": {},
   "source": [
    "Si ponemos atención al método fit veremos que hace uso sucesivamente de forward propagation y backward propagation, en la primera usa la función de pérdida para computar el error, mientras que en la segunda usa la derivada de la función de pérdida para transmitir la señal que tiene que actualizar hacia atrás los pesos de la red. Este es el punto donde estamos alimentando la pieza que hace posible computar la derivada del error respecto a los pesos de cada capa, permitiendo así que se actualicen, desde la capa de salida hacia la capa de entrada de la red.\n",
    "\n",
    "Pero, por ahora, aún no hemos visto de donde sale la función de pérdida y la derivada de la función de pérdida, sólo está asumiendo que están disponibles como parámetros de entrada a la red. \n",
    "\n",
    "Además, podemos observar que se va a imprimir el error (pérdida) computado en cada epoch. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4a49e6",
   "metadata": {},
   "source": [
    "<b>Funciones de Activación y Pérdida </b>\n",
    "\n",
    "Las clases previas permiten construir la Red Neuronal, pero para que esta pueda operar necesita definir previamente funciones que operan como parámetros, que corresponden a dos categorías: funciones de activación y función de pérdida. Cada una de estas va a necesitas sus correspondientes derivadas durante el proceso de back propagation. \n",
    "\n",
    "La capa de activación depende de una <b>función de activación y su derivada en $x$</b>, por lo que necesitamos establecer dicha función. En las capas intermedias podemos usar Tangente Hiperbólica como función de activación, la cual tiende a retornar valores -1 ó 1. \n",
    "\n",
    "$$\\tanh(x) = \\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}$$\n",
    "\n",
    "<br> <img src=https://upload.wikimedia.org/wikipedia/commons/thumb/8/87/Hyperbolic_Tangent.svg/1200px-Hyperbolic_Tangent.svg.png\n",
    "          width=\"350\"/>\n",
    "\n",
    "Es necesario también proveer de la derivada de la función tangente hiperbólica:\n",
    "          \n",
    "$$\\frac{{d}}{{dx}}\\tanh(x) = 1 - \\tanh^2(x)$$\n",
    "\n",
    "En capa de salida podríamos requerir otra función de activación, por ejemplo en el caso en que tenemos clasificación entre valores 0 y 1. La función sigmoidal sería la opción por efecto. \n",
    "\n",
    "$$\\sigma(x) = \\frac{1}{1 + e^{-x}}$$\n",
    "\n",
    "<br> <img src=https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/1200px-Logistic-curve.svg.png\n",
    "          width=\"350\"/>\n",
    "          \n",
    "La derivada de la función sigmoidal es:\n",
    "\n",
    "$$ \\frac{{d}}{{dx}}\\sigma(x) = \\sigma(x) \\cdot (1 - \\sigma(x)) $$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9690236d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# activation function and its derivative\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_prime(x):\n",
    "    return 1-np.tanh(x)**2\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_prime(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86c2cb0",
   "metadata": {},
   "source": [
    "Al igual que con la función de activación, también vamos a necesitar especificar la <b>función de pérdida (error)</b>: \n",
    "$$\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$$\n",
    "\n",
    "También hace falta la <b>derivada de la función de pérdida en $y$</b>:\n",
    "$$\\frac{{\\partial \\text{MSE}}}{{\\partial \\hat{y}_i}} = \\frac{2}{n} \\sum_{i=1}^{n} (\\hat{y}_i - y_i)$$\n",
    "\n",
    "Sin embargo, a menos que implementemos <i>minibath</i>, el error se calculará para cada observación, así que no necesitamos considerar $y$ e $\\hat{y}$ como vectores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc5c418",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# loss function and its derivative\n",
    "def mse(y_real, y_hat):\n",
    "    return (y_real-y_hat)**2\n",
    "\n",
    "def mse_prime(y_real, y_hat):\n",
    "    return 2*(y_hat-y_real)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebccc077",
   "metadata": {},
   "source": [
    "A continuación, creamos un modelo de clasificación usando la clase Network, le agregamos capas intermedias declarando en cada una su tamaño de entrada y salida. Cada capa full conected debe ir acompañada de una capa de activación. La capa de entrada necesita corresponder a la forma de $X$, mientras que la salida corresponde a la forma de $y$. Hemos definido un parámetros de epochs para controlar la cantidad de iteraciones permitidas a la red. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8031c6",
   "metadata": {},
   "source": [
    "<b>Datos para probar</b>\n",
    "\n",
    "Cuando parecía que ya se había terminado de hundir, vamos a reflotar al Titanic, una vez más. Este set de datos ya lo conocemos, puesto que lo usamos muchas veces como ejemplo en la asignatura Ciencia de Datos. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34137aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Una vez más cargamos y procesamos el Titanic\n",
    "file = open(\"Titanic.csv\", \"r\")\n",
    "titanic = file.read()\n",
    "file.close()\n",
    "titanic = titanic.split(\"\\n\")\n",
    "for j in range(len(titanic)):\n",
    "    titanic[j] = titanic[j].split(\",\")\n",
    "\n",
    "# El nombre tiene ',' así que necesitamos volver a unirlo.\n",
    "for elemento in titanic[1:]:\n",
    "    elemento[3] = elemento[3]+elemento[4]\n",
    "    del elemento[4]\n",
    "titanic = np.array(titanic)\n",
    "titanic = np.delete(titanic, [3,8,10,11], axis=1)\n",
    "\n",
    "# Codificamos género como 0-1\n",
    "for linea in titanic[1:]:\n",
    "    linea[3] = '0' if linea[3] == 'male' else '1'\n",
    "\n",
    "# Quitamos las observaciones con datos perdidos\n",
    "borrar = []\n",
    "for l in range(len(titanic)):\n",
    "    if '' in titanic[l]:\n",
    "        borrar.append(l)\n",
    "titanic = np.delete(titanic, borrar, axis=0)\n",
    "\n",
    "# Arreglamos vector y matriz como array\n",
    "y = titanic[:,1][1:]\n",
    "X = titanic[:,2:][1:]\n",
    "X, y = np.array(X, dtype='float64'), np.array(y, dtype='float64')\n",
    "\n",
    "# Aplicamos Normalización Min-Max\n",
    "X= MinMaxScaler().fit_transform(X)\n",
    "\n",
    "# Separamos la muestra al azar, 80% para entrenar, 20% para testeo final. \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f42a26",
   "metadata": {},
   "source": [
    "Usemos la implementación que ya tenemos, creando una instancia de red neuronal invocando la clase network, y a continuación agregamos dos capas intermedias. Cada una de estas capas intermedias tiene, a su vez, dos capas: una full conected y otra de activación. En cada capa full conected debemos indicar el tamaño de entrada (número de nodos de la capa anterior) y salida (número de nodos de la capa). La capa full conected inicial requiere como tamaño de entrada la cifra de dimensiones con que se presenta el problema. \n",
    "\n",
    "En cada capa de activación debemos entregar la función de activación y la derivada de la función de activación. Cerramos la red con una capa de salida, cuyo tamaño de salida debe corresponder al tamaño de la solución esperada. \n",
    "En este caso, probemos con una red con dos capas ocultas de 10 nodos con activación tangente hiperbólica, y con función sigmoidal en la capa de salida.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3d07bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear instancia de Network\n",
    "model = Network()\n",
    "\n",
    "# y agregamos capas al modelo. Debemos usarcapas en pares: (FC, Activartion)\n",
    "# las capas FC es donde declaramos la cantidad de nodos. Debemos hacer calzar entradas con salidas.\n",
    "# las capas de activación tienen las funciones de activación (y derivada) que decidamos usar.\n",
    "model.add(FCLayer(6, 10))               \n",
    "model.add(ActivationLayer(tanh, tanh_prime))\n",
    "model.add(FCLayer(10, 10))                                   \n",
    "model.add(ActivationLayer(tanh, tanh_prime))\n",
    "model.add(FCLayer(10, 1))                   \n",
    "model.add(ActivationLayer(sigmoid, sigmoid_prime))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1462ace",
   "metadata": {},
   "source": [
    "Para entrenar la red falta establecer la función de pérdida que define el error, y la derivada de la misma función que se requiere durante back propagation. \n",
    "\n",
    "El entrenamiento requiere proveer los datos de entrenamiento, dividido en la matriz $X$ con variables predictoras y el vector $y$ de resultados. Además, se necesita establecer dos parámetros: la tasa de aprendizaje (learning rate) y número de iteraciones (epochs). Probemos nuestra red con $10$ epochs y $0.1$ de learning rate. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e365d5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usar el modelo creado\n",
    "model.use(mse, mse_prime)\n",
    "model.fit(X_train, y_train, epochs=10, learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8724fb",
   "metadata": {},
   "source": [
    "La evaluación de los resultados obtenidos por el modelo la debemos hacer sobre el set de datos de testeo que hemos reservado para estos efectos. Podemos observar la calidad de los resultados computando el accuracy y matriz de confusión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d947aa73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "# Usamos el modelo para predecir sobre el conjunto de prueba\n",
    "y_hat = model.predict(X_test)\n",
    "\n",
    "# Nuestra red devuelve arrays en la respuesta, debemos desempacarlo.\n",
    "# Además, la respuesta es una probabilidad (entre 0 y 1), lo necesitamos como 0 o 1. \n",
    "y_hat = [round(y[0][0],0) for y in y_hat]\n",
    "\n",
    "# Reportamos los resultados del modelo\n",
    "matriz_conf = confusion_matrix(y_test, y_hat)\n",
    "\n",
    "print('MATRIZ DE CONFUSIÓN para modelo ANN')\n",
    "print(matriz_conf,'\\n')\n",
    "print('La exactitud de testeo del modelo ANN es: {:.3f}'.format(accuracy_score(y_test,y_hat)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6755fdde",
   "metadata": {},
   "source": [
    "<b>Reconocimiento de Imágenes con MNIST</b>\n",
    "\n",
    "Ya nos hemos aburrido del Titanic, al final siempre se hunde. \n",
    "\n",
    "Vamos a usar un dataset disponible en Keras, de forma que no tendremos que tener nuestros propios datos. Se trata de <b><i>Mnist</i></b>, una colección de imágenes de dígitos (números) escritos a mano por distintas personas, con la que ya trabajamos durante Ciencia de Datos. \n",
    "<br> <img src=https://www.ttested.com/gallery/thumbnails/ditch-mnist.jpg\n",
    "          width=\"400\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7891d104",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  \n",
    "from keras.datasets import mnist\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import tabulate\n",
    "\n",
    "random.seed(1234)\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X, y = zip(*random.sample(list(zip(X_train, y_train)), 2))\n",
    "\n",
    "print(np.array(X).shape)\n",
    "\n",
    "print(tabulate.tabulate(X[0]))\n",
    "fig = plt.figure\n",
    "plt.imshow(X[0], cmap='gray_r')\n",
    "plt.show()\n",
    "print(y[0])\n",
    "print()\n",
    "print()\n",
    "\n",
    "print(tabulate.tabulate(X[1]))\n",
    "fig = plt.figure\n",
    "plt.imshow(X[1], cmap='gray_r')\n",
    "plt.show()\n",
    "print(y[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f98a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No necesitamos tantos datos.\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "random.seed(123) # Vamos a controlar la aleatoriedad en adelante. \n",
    "X, y = zip(*random.sample(list(zip(X_train, y_train)), 2000))\n",
    "\n",
    "# Sí necesitamos que la forma de X sea la de un vector, en lugar de una matriz. \n",
    "X, y = np.array(X, dtype='float64'), np.array(y, dtype='float64')\n",
    "X = np.reshape(X, (X.shape[0], -1))\n",
    "\n",
    "# Normalizamos Min-Max\n",
    "X= MinMaxScaler().fit_transform(X)\n",
    "\n",
    "# Dividomos la muestra en dos, una para entrenar y otra para testing, como tenemos \n",
    "# muestra de sobra nos damos el lujo de testear con la misma cantidad que entrenamos.\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=123)\n",
    "\n",
    "# Necesitamos que y_train sea un valor categórico, en lugar de un dígito entero.\n",
    "from keras.utils import to_categorical\n",
    "print('y_train antes de categorizar: ',y_train[0])\n",
    "y_train = to_categorical(y_train)\n",
    "print('y_train después de categorizar: ',y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8cf120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necesitamos identificar cuantos nodos tiene nuestra entrada, y eso depende de la cantidad de Xs.\n",
    "entrada_dim = len(X_train[0])\n",
    "\n",
    "# Crear instancia de Network\n",
    "model = Network()\n",
    "\n",
    "# Agregamos capas al modelo\n",
    "model.add(FCLayer(entrada_dim, 16))               \n",
    "model.add(ActivationLayer(tanh, tanh_prime))\n",
    "model.add(FCLayer(16, 10))                                                   \n",
    "model.add(ActivationLayer(sigmoid, sigmoid_prime))\n",
    "\n",
    "# Usar el modelo creado\n",
    "model.use(mse, mse_prime)\n",
    "model.fit(X_train, y_train, epochs=10, learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f526ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usamos el modelo para predecir sobre el conjunto de prueba\n",
    "y_hat = model.predict(X_test)\n",
    "\n",
    "# Transformamos la salida en un vector one-hot encoded, es decir 0s y un 1. \n",
    "for i in range(len(y_hat)):\n",
    "    y_hat[i] = np.argmax(y_hat[i][0])\n",
    "\n",
    "# Reportamos los resultados del modelo\n",
    "matriz_conf = confusion_matrix(y_test, y_hat)\n",
    "\n",
    "print('MATRIZ DE CONFUSIÓN para modelo ANN')\n",
    "print(matriz_conf,'\\n')\n",
    "print('La exactitud de testeo del modelo ANN es: {:.3f}'.format(accuracy_score(y_test,y_hat)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea1dcdd",
   "metadata": {},
   "source": [
    "Ya obtenemos resultados bastante buenos. Incrementemos la capacidad de representación del modelo, por la vía de aumentar la cantidad de capas, es decir haremos la red más profunda. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cd6b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necesitamos identificar cuantos nodos tiene nuestra entrada, y eso depende de la cantidad de Xs.\n",
    "entrada_dim = len(X_train[0])\n",
    "\n",
    "# Crear instancia de Network\n",
    "model = Network()\n",
    "\n",
    "# Agregamos capas al modelo\n",
    "model.add(FCLayer(entrada_dim, 16))               \n",
    "model.add(ActivationLayer(tanh, tanh_prime))\n",
    "model.add(FCLayer(16, 16))                           \n",
    "model.add(ActivationLayer(tanh, tanh_prime))\n",
    "model.add(FCLayer(16, 10))                                                   \n",
    "model.add(ActivationLayer(sigmoid, sigmoid_prime))\n",
    "\n",
    "# Usar el modelo creado\n",
    "model.use(mse, mse_prime)\n",
    "model.fit(X_train, y_train, epochs=10, learning_rate=0.1)\n",
    "\n",
    "# Usamos el modelo para predecir sobre el conjunto de prueba\n",
    "y_hat = model.predict(X_test)\n",
    "\n",
    "# Transformamos la salida en un vector one-hot encoded, es decir 0s y un 1. \n",
    "for i in range(len(y_hat)):\n",
    "    y_hat[i] = np.argmax(y_hat[i][0])\n",
    "\n",
    "# Reportamos los resultados del modelo\n",
    "matriz_conf = confusion_matrix(y_test, y_hat)\n",
    "\n",
    "print('MATRIZ DE CONFUSIÓN para modelo ANN')\n",
    "print(matriz_conf,'\\n')\n",
    "print('La exactitud de testeo del modelo ANN es: {:.3f}'.format(accuracy_score(y_test,y_hat)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c3407b",
   "metadata": {},
   "source": [
    "Profundizar la red no necesariamente mejora el aprendizaje, especialmente al momento de generalizar sobre el set de datos de prueba. \n",
    "\n",
    "Probemos incrementar la cantidad de neuronas de la red con una capa oculta.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62859e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necesitamos identificar cuantos nodos tiene nuestra entrada, y eso depende de la cantidad de Xs.\n",
    "entrada_dim = len(X_train[0])\n",
    "\n",
    "# Crear instancia de Network\n",
    "model = Network()\n",
    "\n",
    "# Agregamos capas al modelo\n",
    "model.add(FCLayer(entrada_dim, 32))               \n",
    "model.add(ActivationLayer(tanh, tanh_prime))\n",
    "model.add(FCLayer(32, 10))                                                   \n",
    "model.add(ActivationLayer(sigmoid, sigmoid_prime))\n",
    "\n",
    "# Usar el modelo creado\n",
    "model.use(mse, mse_prime)\n",
    "model.fit(X_train, y_train, epochs=10, learning_rate=0.1)\n",
    "\n",
    "# Usamos el modelo para predecir sobre el conjunto de prueba\n",
    "y_hat = model.predict(X_test)\n",
    "\n",
    "# Transformamos la salida en un vector one-hot encoded, es decir 0s y un 1. \n",
    "for i in range(len(y_hat)):\n",
    "    y_hat[i] = np.argmax(y_hat[i][0])\n",
    "\n",
    "# Reportamos los resultados del modelo\n",
    "matriz_conf = confusion_matrix(y_test, y_hat)\n",
    "\n",
    "print('MATRIZ DE CONFUSIÓN para modelo ANN')\n",
    "print(matriz_conf,'\\n')\n",
    "print('La exactitud de testeo del modelo ANN es: {:.3f}'.format(accuracy_score(y_test,y_hat)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd938caf",
   "metadata": {},
   "source": [
    "Aparentemente más neuronas en una capa funciona mejor. Hagamos la prueba de incrementar más fuertemente el número de neuronas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217924e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necesitamos identificar cuantos nodos tiene nuestra entrada, y eso depende de la cantidad de Xs.\n",
    "entrada_dim = len(X_train[0])\n",
    "\n",
    "# Crear instancia de Network\n",
    "model = Network()\n",
    "\n",
    "# Agregamos capas al modelo\n",
    "model.add(FCLayer(entrada_dim, 200))               \n",
    "model.add(ActivationLayer(tanh, tanh_prime))\n",
    "model.add(FCLayer(200, 10))                                                   \n",
    "model.add(ActivationLayer(sigmoid, sigmoid_prime))\n",
    "\n",
    "# Usar el modelo creado\n",
    "model.use(mse, mse_prime)\n",
    "model.fit(X_train, y_train, epochs=10, learning_rate=0.1)\n",
    "\n",
    "# Usamos el modelo para predecir sobre el conjunto de prueba\n",
    "y_hat = model.predict(X_test)\n",
    "\n",
    "# Transformamos la salida en un vector one-hot encoded, es decir 0s y un 1. \n",
    "for i in range(len(y_hat)):\n",
    "    y_hat[i] = np.argmax(y_hat[i][0])\n",
    "\n",
    "# Reportamos los resultados del modelo\n",
    "matriz_conf = confusion_matrix(y_test, y_hat)\n",
    "\n",
    "print('MATRIZ DE CONFUSIÓN para modelo ANN')\n",
    "print(matriz_conf,'\\n')\n",
    "print('La exactitud de testeo del modelo ANN es: {:.3f}'.format(accuracy_score(y_test,y_hat)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78e2645",
   "metadata": {},
   "source": [
    "Incrementar el número de neuronas también termina deteriorando la capacidad de aprender del modelo. \n",
    "\n",
    "Dejemos la red como estaba al principio, pero entrenémosla más. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341c9941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necesitamos identificar cuantos nodos tiene nuestra entrada, y eso depende de la cantidad de Xs.\n",
    "entrada_dim = len(X_train[0])\n",
    "\n",
    "# Crear instancia de Network\n",
    "model = Network()\n",
    "\n",
    "# Agregamos capas al modelo\n",
    "model.add(FCLayer(entrada_dim, 16))               \n",
    "model.add(ActivationLayer(tanh, tanh_prime))\n",
    "model.add(FCLayer(16, 10))                                                   \n",
    "model.add(ActivationLayer(sigmoid, sigmoid_prime))\n",
    "\n",
    "# Usar el modelo creado\n",
    "model.use(mse, mse_prime)\n",
    "model.fit(X_train, y_train, epochs=50, learning_rate=0.1)\n",
    "\n",
    "# Usamos el modelo para predecir sobre el conjunto de prueba\n",
    "y_hat = model.predict(X_test)\n",
    "\n",
    "# Transformamos la salida en un vector one-hot encoded, es decir 0s y un 1. \n",
    "for i in range(len(y_hat)):\n",
    "    y_hat[i] = np.argmax(y_hat[i][0])\n",
    "\n",
    "# Reportamos los resultados del modelo\n",
    "matriz_conf = confusion_matrix(y_test, y_hat)\n",
    "\n",
    "print('MATRIZ DE CONFUSIÓN para modelo ANN')\n",
    "print(matriz_conf,'\\n')\n",
    "print('La exactitud de testeo del modelo ANN es: {:.3f}'.format(accuracy_score(y_test,y_hat)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d00166",
   "metadata": {},
   "source": [
    "Es probable que al aumentar la capacidad de representación de la red, ya sea por disponer de más capas, más neuronas o más tiempo de entrenamiento, estamos generando las condiciones que facilitan el <b><i>overfitting</i></b>. Hasta ahora hemos tratado de controlarlo por la vía de seleccionar los parámetros de configuración del modelo buscando que no tenga demasiada capacidad. Sin embargo, con esto estamos limitando nuestra posibilidad de aprovechar modelos más robustos. El aprendizaje profundo, que ha permitido generar los modelos AI que están cambiando el mundo, requiere que podamos enfrentar este problema por otras vías.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa1223a",
   "metadata": {},
   "source": [
    "<b> CONCLUSIÓN </b>\n",
    "\n",
    "Este apunte profundiza en la mecánica del aprendizaje de una red neuronal, que podemos resumir en una serie de operaciones de algebra matricial. Usando solo recursos genéricos de Python hemos demostrado que se puede construir redes neuronales con la capacidad de aprender sobre distintos problemas. Si bien hemos usado una implementación previamente publicada, hemos procurado explicar la matemática que justifica dicha implementación, demostrando su validez teórica. Luego hemos realizado experimentos donde se aprecia la capacidad de aprendizaje y generalización de los modelos construidos, demostrando empíricamente la validez de estos. \n",
    "\n",
    "Buena parte de la asignatura consistirá en extender esta implementación, incorporando distintos métodos para mejorar el aprendizaje de nuestra red neuronal, y adaptándola a distintos contexto de problema. Esto lo hacemos con fines pedagógicos, puesto que en general lograríamos los mismos y mejores resultados usando librerías que ya están disponibles. También habrá capítulos de la asignatura donde nos apoyaremos en esas implementaciones que ya existen, cuando abordemos problemas crecientemente complejos. Pero siempre será satisfactorio saber que podemos implementar nuestra propia red neuronal. \n",
    "\n",
    "<img src=https://img2-levelup.buscafs.com/415775_945x532.jpg width=\"200\"/>\n",
    "<center>CON JUEGOS DE AZAR Y PERSONUELAS</center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
