{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26092db2",
   "metadata": {},
   "source": [
    "<head><b><center>MACHINE LEARNING</center>\n",
    "\n",
    "<center>Redes Neuronales Artificiales</center>\n",
    "    \n",
    "\n",
    "<center>Profesor: Gabriel Jara </center></b></head><br>\n",
    "El presente Jupyter Notebook busca:\n",
    "<ul>\n",
    "    <li>Retomar conceptos sobre Redes Neuronales Artificiales (ANN).</li>\n",
    "    <li>Profundizar en el mecanismo de aprendizaje de las ANN</li>\n",
    "    <li>Analizar la implementación de un Perceptron, unidad básica de las redes neuronales.</li>\n",
    "</ul>\n",
    "\n",
    "\n",
    "Algunas imágenes han sido robadas de internet y son enlaces a su correspondiente fuente... por ejemplo esta: \n",
    "<br> <img src=https://pbs.twimg.com/media/F-Q_3VXXkAAsRNX.jpg:large\n",
    "          width=\"300\"/></br>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d672aa9",
   "metadata": {},
   "source": [
    "<b>PERCEPTRON</b>\n",
    "\n",
    "Cuando en Ciencia de Datos presentamos las Redes Neuronales Artificiales como modelo de aprendizaje, identificamos el perceptrón como unidad básica de este modelo, cuya función emula el rol de una neurona: percibe un estímulo según el cual se activa (o no) y con ello transmite una señal. \n",
    "<br> <img src=https://images.deepai.org/glossary-terms/perceptron-6168423.jpg\n",
    "          width=\"400\"/></br>\n",
    "          \n",
    "En Ciencia de Datos construimos modelos usando implementaciones de redes neuronales ya disponibles, como <i>Keras - TensorFlow</i>, y en general es lo que haríamos en un contexto productivo. Pero en esta asignatura queremos profundizar en la topología misma de la red, desentrañar el cómo y porqué aprende, y eso no lo lograríamos sólo importando librerías. Así que hoy comenzaremos a construir nuestra propia implementación de red neuronal, y partiremos con una simple neurona. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1e00c3",
   "metadata": {},
   "source": [
    "Definamos una clase <i>Perceptron</i>, que se inicialice con parámetros que definen tasa de aprendizaje y cantidad de épocas de entrenamiento por defecto (más adelante volveremos sobre esto), así como pesos que se asocian a cada entrada y un peso adicional de sesgo. Estos pesos se multiplican por cada valor de entrada y se suman con el sesgo, para luego ser aplicada la función de activación, en este caso una función de paso, que determina si el perceptrón se activa o no a partir de los valores de entrada.    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b8580f2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Definimos la clase Perceptron\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mPerceptron\u001b[39;00m:\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Definimos la clase Perceptron\n",
    "class Perceptron:\n",
    "    def __init__(self, learning_rate=0.01, n_iters=1000):\n",
    "        # Valores por defecto en caso que no se especifique Learnng rate o n_iters\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iters = n_iters\n",
    "        # Pesos y sesgo\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "            \n",
    "    def _unit_step_function(self, x):\n",
    "        # Esta es la función de activación\n",
    "        return np.where(x >= 0, 1, 0)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Método que se empleará para el entrenamiento (ajuste) del Perceptrón\n",
    "        # Se identifica cuantos datos hay en la muestra y con cuantos atributos\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        # Se inicializan tantos pesos como atributos...\n",
    "        self.weights = np.zeros(n_features)\n",
    "        # ...más un sesgo\n",
    "        self.bias = 0\n",
    "        # Por ahora hemos iniciado todos los pesos y sesgo en valor cero. \n",
    "        # Más adelante debieramos revisar esta decisión. \n",
    "        \n",
    "        for _ in range(self.n_iters):\n",
    "            # Iteraremos haciendo ajuste de los pesos\n",
    "            for idx, x_i in enumerate(X):\n",
    "                # Primero calculamos la salida de la transformación lineal: X*W + B\n",
    "                linear_output = np.dot(x_i, self.weights) + self.bias\n",
    "                # La salida de esa transformación lineal la pasamos por la función de activación.\n",
    "                y_predicted = self._unit_step_function(linear_output)\n",
    "                # Calculamos la magnitud de actualización, que es proporcional al learning rate. \n",
    "                # Note que Si y == y_hat; => update == 0. \n",
    "                delta = self.learning_rate * (y[idx] - y_predicted)\n",
    "                # Actualizamos pesos y sesgo. \n",
    "                self.weights += delta * x_i\n",
    "                self.bias += delta\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Método que usaremos para obtener predicciones con el Perceptrón\n",
    "        # Primero calculamos la salida de la transformación lineal: X*W + B\n",
    "        linear_output = np.dot(X, self.weights) + self.bias\n",
    "        # La salida de esa transformación lineal la pasamos por la función de activación.\n",
    "        y_predicted = self._unit_step_function(linear_output)\n",
    "        # y esa es nuestra predicción, así que la retornamos. \n",
    "        return y_predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1d9957",
   "metadata": {},
   "source": [
    "El perceptrón debe tener una función de activación, que corresponde a la transformación no lineal que se aplica sobre la transformación lineal determinada por pesos y sesgo, y que típicamente es una función de paso binaria, es decir que devuelve cero o uno según si su entrada supera o no un umbral. \n",
    "\n",
    "El perceptrón tiene dos métodos adicionales. Uno de ellos es para el entrenamiento en que ajusta sus pesos para resolver un problema a partir de la muestra $X$ e $y$ que reciba. El otro método es para realizar predicciones basada en una muestra $X$.\n",
    "\n",
    "<b>¿En qué consiste predecir con el perceptrón?</b>\n",
    "\n",
    "Tenemos datos en una matriz $X$ donde cada columna son atributos, cada fila son observaciones. El perceptrón ejecuta una transformación lineal de cada observación en $X$ multiplicando cada atributo por un coeficiente de peso y sumando un sesgo. El resultado de la transformación lineal alimenta a una función de paso que lo transforma en cero o uno, y esta salida es la predicción del valor que tendrían las observaciones en un hipotético vector $y$. \n",
    "\n",
    "<b>¿Cómo se ajusta los pesos del perceptrón?</b>\n",
    "\n",
    "Tenemos datos en una matriz $X$ donde cada columna son atributos, cada fila son observaciones y cada una de estas observaciones se corresponde con un valor en el vector $y$ (semántica posicional). El perceptrón ajusta los pesos buscando que dado $X$ como entrada a la salida se obtenga $y$. Para esto usa la <b>Regla Delta</b>.\n",
    "\n",
    "La regla delta es una forma ingeniosa de decretar la dirección y magnitud del ajuste requerido por los pesos. Suponga que dado cierta entrada $X$ observamos que a la salida del perceptrón se obtiene un valor 0, pero el valor $y$ esperado era 1, entonces diríamos que se cometió un error de 1. Lo que quisíeramos es que los parámetros de peso se ajusten al alza, para aumentar la probabilidad de que ante la misma entrada $X$ la respuesta del perceptrón sea 1. Si por otro lado el perceptrón responde 1 cuando debería haber sido 0, entonces diríamos que se cometió un error de magnitud -1 y quisíeramos que los pesos se ajusten a la baja para aumentar la probabilidad de que responda 0 para esa entrada. \n",
    "\n",
    "El error lo estamos definiendo como la diferencia entre el valor real $y$ y la predicción que genera el perceptrón. \n",
    "\n",
    "$$ error = y - \\hat{y}$$\n",
    "\n",
    "Entonces lo que hacemos es ajustar los pesos de la red en la dirección del error. La magnitud del ajuste queda controlada por un parámetro $\\alpha$ que conocemos como <i>Learning Rate</i>. Entonces, cada peso $i$ se actualiza de acuerdo con la fórmula:\n",
    "\n",
    "$$w_{i} \\leftarrow w_{i} - \\alpha (y - \\hat{y}) $$\n",
    "\n",
    "Cuando el perceptrón acierta en el pronóstico significa que $y = \\hat{y}$, y por lo mismo en ese caso la actualización de pesos se vuelve nula. Así, los pesos del perceptrón se ajustan cada vez que éste se equivoca, en la dirección adecuada para reducir el error, y no se ajustan cuando el perceptrón acierta. \n",
    "\n",
    "<br> <img src=https://media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs00521-022-07233-1/MediaObjects/521_2022_7233_Fig2_HTML.png\n",
    "          width=\"300\"/></br>\n",
    "\n",
    "En nuestra próxima clase veremos como la idea de la regla delta se extiende usando el concepto de función de error y gradiente, habilitando así el aprendizaje a nivel de redes de perceptrones. Pero por ahora, sigamos analizando el caso de un perceptrón. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84534bab",
   "metadata": {},
   "source": [
    "<b>¿Para qué podemos usar este simple perceptrón? </b>\n",
    "\n",
    "Por ejemplo, para resolver algunas operaciones lógicas como la operación <i>AND</i>. A continuación tenemos operación lógica <i>AND</i> muy simple, con sólo cuatro posibles entradas y dos posibles salidas, que usamos para entrenar al perceptrón y luego observar que replica el resultado perfectamente. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591395d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datos de entrenamiento (X: atributos, y: etiquetas)\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([0, 0, 0, 1])  # AND lógico\n",
    "\n",
    "# Creamos una instancia del perceptrón y lo entrenamos\n",
    "perceptron = Perceptron(learning_rate=0.1, n_iters=10)\n",
    "\n",
    "\n",
    "perceptron.fit(X, y)\n",
    "\n",
    "# Realizamos predicciones\n",
    "predicciones = perceptron.predict(X)\n",
    "print(\"Predicciones de AND:\", predicciones)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ed4461",
   "metadata": {},
   "source": [
    "En lugar de <i>AND</i> podríamos aprender <i>OR</i>. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf62f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datos de entrenamiento (X: atributos, y: etiquetas)\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([0, 1, 1, 1])  # OR lógico\n",
    "\n",
    "# Creamos una instancia del perceptrón y lo entrenamos\n",
    "perceptron = Perceptron(learning_rate=0.1, n_iters=10)\n",
    "perceptron.fit(X, y)\n",
    "\n",
    "# Realizamos predicciones\n",
    "predicciones = perceptron.predict(X)\n",
    "print(\"Predicciones de OR:\", predicciones)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d5a1e5",
   "metadata": {},
   "source": [
    "Se aprecia que nuestro perceptrón no tiene dificultades en aprender estas dos operaciones lógicas. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06dacd7",
   "metadata": {},
   "source": [
    "¿Cómo generalizamos un poco este experimento? Definamos una operación de tipo <i>AND</i> algo más compleja, que dé verdadero si $x$ e $y$ son positivo, en cualquier otro caso sea falso. Sería una versión continua de la misma función <i>AND</i>. Podemos visualizar datos de ejemplo de dicha función <i>AND</i> continua en la gráfica a continuación. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e77226",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generamos 100 puntos aleatorios en el plano XY\n",
    "X = np.random.uniform(-1, 1, (100, 2))\n",
    "\n",
    "# Calculamos los valores de la función AND\n",
    "y = np.logical_and(X[:, 0] > 0, X[:, 1] > 0)\n",
    "\n",
    "# Configuramos el gráfico\n",
    "plt.figure(figsize=(8, 8))\n",
    "\n",
    "# Recorremos los puntos y los dibujamos con su respectivo símbolo y color\n",
    "for i in range(X.shape[0]):\n",
    "    if y[i]:\n",
    "        plt.text(X[i, 0], X[i, 1], 'O', color='blue', fontsize=12, ha='center', va='center')\n",
    "    else:\n",
    "        plt.text(X[i, 0], X[i, 1], 'X', color='red', fontsize=12, ha='center', va='center')\n",
    "\n",
    "# Configuramos los límites y las líneas del eje\n",
    "plt.xlim(-1, 1)\n",
    "plt.ylim(-1, 1)\n",
    "plt.axhline(0, color='black', linewidth=0.5)\n",
    "plt.axvline(0, color='black', linewidth=0.5)\n",
    "plt.grid(color='gray', linestyle='--', linewidth=0.5)\n",
    "\n",
    "# Añadimos la leyenda\n",
    "plt.text(1, 1.2, 'O: Verdadero', color='blue', fontsize=12, ha='left')\n",
    "plt.text(1, 1.1, 'X: Falso', color='red', fontsize=12, ha='left')\n",
    "\n",
    "# Añadimos título y etiquetas\n",
    "plt.title('Representación de la función AND en el plano XY')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "\n",
    "# Mostramos el gráfico\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32686ca3",
   "metadata": {},
   "source": [
    "¿Puede nuestro perceptrón aprender esta nueva función <i>AND</i>? Nada nos impide intentarlo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11664aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Entrenamos un Perceptrón que aprenda AND\n",
    "perceptron = Perceptron(learning_rate=0.1, n_iters=10)\n",
    "perceptron.fit(X, y)\n",
    "\n",
    "# Utilizamos el Perceptron ya entrenado\n",
    "y_hat = perceptron.predict(X)\n",
    "\n",
    "# Configuramos el gráfico\n",
    "plt.figure(figsize=(8, 8))\n",
    "\n",
    "# Recorremos los puntos y los dibujamos con su respectivo símbolo y color\n",
    "for i in range(X.shape[0]):\n",
    "    if y_hat[i]:\n",
    "        plt.text(X[i, 0], X[i, 1], 'O', color='blue', fontsize=12, ha='center', va='center')\n",
    "    else:\n",
    "        plt.text(X[i, 0], X[i, 1], 'X', color='red', fontsize=12, ha='center', va='center')\n",
    "\n",
    "# Configuramos los límites y las líneas del eje\n",
    "plt.xlim(-1, 1)\n",
    "plt.ylim(-1, 1)\n",
    "plt.axhline(0, color='black', linewidth=0.5)\n",
    "plt.axvline(0, color='black', linewidth=0.5)\n",
    "plt.grid(color='gray', linestyle='--', linewidth=0.5)\n",
    "\n",
    "# Añadimos la leyenda\n",
    "plt.text(1, 1.2, 'O: Verdadero', color='blue', fontsize=12, ha='left')\n",
    "plt.text(1, 1.1, 'X: Falso', color='red', fontsize=12, ha='left')\n",
    "\n",
    "# Añadimos título y etiquetas\n",
    "plt.title('Función AND en el plano XY según nuestro Perceptron')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "\n",
    "# Mostramos el gráfico\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0073477",
   "metadata": {},
   "source": [
    "Se nota que sí está tratando de aprender la función <i>AND</i> pero dejó mal clasificados varios puntos. Podríamos especular que es porque no ha entrenado lo suficiente, dado que le redujimos la cantidad de épocas a sólo 10, ¿qué pasa si las aumentamos?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31c62bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Entrenamos un Perceptrón que aprenda AND\n",
    "perceptron = Perceptron(learning_rate=0.1, n_iters=1000)\n",
    "perceptron.fit(X, y)\n",
    "\n",
    "# Utilizamos el Perceptron entrenado\n",
    "y_hat = perceptron.predict(X)\n",
    "\n",
    "# Configuramos el gráfico\n",
    "plt.figure(figsize=(8, 8))\n",
    "\n",
    "# Recorremos los puntos y los dibujamos con su respectivo símbolo y color\n",
    "for i in range(X.shape[0]):\n",
    "    if y_hat[i]:\n",
    "        plt.text(X[i, 0], X[i, 1], 'O', color='blue', fontsize=12, ha='center', va='center')\n",
    "    else:\n",
    "        plt.text(X[i, 0], X[i, 1], 'X', color='red', fontsize=12, ha='center', va='center')\n",
    "\n",
    "# Configuramos los límites y las líneas del eje\n",
    "plt.xlim(-1, 1)\n",
    "plt.ylim(-1, 1)\n",
    "plt.axhline(0, color='black', linewidth=0.5)\n",
    "plt.axvline(0, color='black', linewidth=0.5)\n",
    "plt.grid(color='gray', linestyle='--', linewidth=0.5)\n",
    "\n",
    "# Añadimos la leyenda\n",
    "plt.text(1, 1.2, 'O: Verdadero', color='blue', fontsize=12, ha='left')\n",
    "plt.text(1, 1.1, 'X: Falso', color='red', fontsize=12, ha='left')\n",
    "\n",
    "# Añadimos título y etiquetas\n",
    "plt.title('Función AND en el plano XY según nuestro Perceptron')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "\n",
    "# Mostramos el gráfico\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d571d4f",
   "metadata": {},
   "source": [
    "Salvo que se demoró más, el resultado es esencialmente el mismo, con mínimas diferencias (si es que las hubiere). No parece ser un problema de cuanto entrenemos, y lo podemos apreciar si consideramos cómo trabaja el perceptrón, que es en esencia lineal. \n",
    "\n",
    "Observemos que los valores asociados a los pesos (incluido el sesgo), relacionan los valores $X$ con la salida $y$ de la red usando una línea recta. Podemos apreciar dicha recta directamente. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d66d6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuramos el gráfico\n",
    "plt.figure(figsize=(8, 8))\n",
    "\n",
    "# Recorremos los puntos y los dibujamos con su respectivo símbolo y color\n",
    "for i in range(X.shape[0]):\n",
    "    if y_hat[i]:\n",
    "        plt.text(X[i, 0], X[i, 1], 'O', color='blue', fontsize=12, ha='center', va='center')\n",
    "    else:\n",
    "        plt.text(X[i, 0], X[i, 1], 'X', color='red', fontsize=12, ha='center', va='center')\n",
    "        \n",
    "# Configuramos los límites y las líneas del eje\n",
    "plt.xlim(-1, 1)\n",
    "plt.ylim(-1, 1)\n",
    "plt.axhline(0, color='black', linewidth=0.5)\n",
    "plt.axvline(0, color='black', linewidth=0.5)\n",
    "plt.grid(color='gray', linestyle='--', linewidth=0.5)\n",
    "\n",
    "# Añadimos la leyenda\n",
    "plt.text(1, 1.3, 'O: Verdadero', color='blue', fontsize=12, ha='left')\n",
    "plt.text(1, 1.2, 'X: Falso', color='red', fontsize=12, ha='left')\n",
    "plt.text(1, 1.1, '--'': Frontera de decisión', color='black', fontsize=12, ha='left')\n",
    "\n",
    "# Dibujamos la frontera de decisión\n",
    "# No confunda XY del plano con X e y del problema. Para evitar confuciones \n",
    "# llamaremos al plano x0 y x1. \n",
    "x0 = np.array([-1, 1])\n",
    "# para los valores x0 = -1 y 1, calcularemos x1 usando \n",
    "# la ecuación de la recta y = w0*x0 + w1*x1 + b, con y = 0. \n",
    "# Note que es la misma recta que usa el perceptrón cuando hace la predicción. \n",
    "x1 = -(perceptron.weights[0] * x0 + perceptron.bias) / perceptron.weights[1]\n",
    "# Añadimos esta recta al gráfico, para visualizar así la frontera de decisión del perceptrón.\n",
    "plt.plot(x0, x1, color='black', linestyle='--')\n",
    "\n",
    "# Añadimos título y etiquetas\n",
    "plt.title('Función AND en el plano XY según nuestro Perceptron')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "\n",
    "# Mostramos el gráfico\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f775ddbd",
   "metadata": {},
   "source": [
    "Ahora podemos ver con más claridad porque el perceptrón no llega a resolver por completo nuestra versión continua de <i>AND</i>, lo que pasa es que no hay forma de trazar una línea recta que separe perfectamente los verdadero de los falso. No importa cuanto se entrene, el perceptrón no va a poder aprender a la perfección esta función, por no ser ella linealmente separable. \n",
    "\n",
    "Por otro lado, veamos el vaso medio lleno, el perceptrón sí ha aprendido, puesto que identifica la frontera que minimiza el error, es decir logra clasificar correctamente la mayoría de los puntos posible con una línea recta. Puede parecer poca cosa, pero este es el principio de la inteligencia, al menos la artificial (probablemente también la biológica). \n",
    "\n",
    "Así que diremos que el perceptrón sí resuelve <i>AND</i>, ya sabemos que también resuelve <i>OR</i>. ¿Qué operación lógica no resuelve?\n",
    "\n",
    "Se conoce como <i>XOR</i> al \"O estricto\", es decir la relación \"o\" y sólo \"o\", excluyendo el caso en que \"y\" da verdadero. Esta operación lógica no es linealmente separable en su versión discreta, y por lo mismo no ofrece al perceptrón ninguna posibilidad de aprendizaje.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c82e8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datos de entrenamiento (X: atributos, y: etiquetas)\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([0, 1, 1, 0])  # XOR lógico\n",
    "\n",
    "# Creamos una instancia del perceptrón y lo entrenamos\n",
    "perceptron = Perceptron(learning_rate=0.1, n_iters=10)\n",
    "perceptron.fit(X, y)\n",
    "\n",
    "# Realizamos predicciones\n",
    "predicciones = perceptron.predict(X)\n",
    "print(\"Predicciones de XOR:\", predicciones)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5efbdf",
   "metadata": {},
   "source": [
    "Así como no funciona en la versión discreta de <i>XOR</i>, tampoco lo hará para nuestra adaptación continua. El resultado que obtengamos será simplemente aleatorio, puesto que el perceptrón no tiene forma de aprender esta función, dado que No existe la línea recta que minimiza el error de predicción para este caso. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2314654d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generamos 100 puntos aleatorios en el plano XY\n",
    "X = np.random.uniform(-1, 1, (100, 2))\n",
    "\n",
    "# Calculamos los valores de la función AND\n",
    "y = np.logical_xor(X[:, 0] > 0, X[:, 1] > 0)\n",
    "\n",
    "# Configuramos el gráfico\n",
    "plt.figure(figsize=(8, 8))\n",
    "\n",
    "# Recorremos los puntos y los dibujamos con su respectivo símbolo y color\n",
    "for i in range(X.shape[0]):\n",
    "    if y[i]:\n",
    "        plt.text(X[i, 0], X[i, 1], 'O', color='blue', fontsize=12, ha='center', va='center')\n",
    "    else:\n",
    "        plt.text(X[i, 0], X[i, 1], 'X', color='red', fontsize=12, ha='center', va='center')\n",
    "\n",
    "# Configuramos los límites y las líneas del eje\n",
    "plt.xlim(-1, 1)\n",
    "plt.ylim(-1, 1)\n",
    "plt.axhline(0, color='black', linewidth=0.5)\n",
    "plt.axvline(0, color='black', linewidth=0.5)\n",
    "plt.grid(color='gray', linestyle='--', linewidth=0.5)\n",
    "\n",
    "# Añadimos la leyenda\n",
    "plt.text(1, 1.2, 'O: Verdadero', color='blue', fontsize=12, ha='left')\n",
    "plt.text(1, 1.1, 'X: Falso', color='red', fontsize=12, ha='left')\n",
    "\n",
    "# Añadimos título y etiquetas\n",
    "plt.title('Representación de la función XOR en el plano XY')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "\n",
    "# Mostramos el gráfico\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2e04fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Entrenamos un Perceptrón que aprenda XOR\n",
    "perceptron = Perceptron(learning_rate=0.1, n_iters=10)\n",
    "perceptron.fit(X, y)\n",
    "\n",
    "# Utilizamos el Perceptron que entrenado\n",
    "y_hat = perceptron.predict(X)\n",
    "\n",
    "# Configuramos el gráfico\n",
    "plt.figure(figsize=(8, 8))\n",
    "\n",
    "# Recorremos los puntos y los dibujamos con su respectivo símbolo y color\n",
    "for i in range(X.shape[0]):\n",
    "    if y_hat[i]:\n",
    "        plt.text(X[i, 0], X[i, 1], 'O', color='blue', fontsize=12, ha='center', va='center')\n",
    "    else:\n",
    "        plt.text(X[i, 0], X[i, 1], 'X', color='red', fontsize=12, ha='center', va='center')\n",
    "\n",
    "# Configuramos los límites y las líneas del eje\n",
    "plt.xlim(-1, 1)\n",
    "plt.ylim(-1, 1)\n",
    "plt.axhline(0, color='black', linewidth=0.5)\n",
    "plt.axvline(0, color='black', linewidth=0.5)\n",
    "plt.grid(color='gray', linestyle='--', linewidth=0.5)\n",
    "\n",
    "# Añadimos la leyenda\n",
    "plt.text(1, 1.1, 'O: Verdadero', color='blue', fontsize=12, ha='left')\n",
    "plt.text(1, 1.2, 'X: Falso', color='red', fontsize=12, ha='left')\n",
    "\n",
    "\n",
    "# Añadimos título y etiquetas\n",
    "plt.title('Función XOR en el plano XY según nuestro Perceptron')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "\n",
    "# Mostramos el gráfico\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34ac1cb",
   "metadata": {},
   "source": [
    "Así que podemos concluir que el perceptrón es bastante limitado, sólo puede trazar líneas rectas y sólo logra aprender a resolver problemas que sean linealmente separables. \n",
    "\n",
    "<b>PERCEPTRÓN PARA CLASIFICACIÓN</b>\n",
    "\n",
    "El modelo que estamos estudiando es adecuado en problemas de clasificación. <i>¿Recuerda algún modelo de clasificación, visto en Ciencia de Datos, que se pareciera al Perceptrón?</i>\n",
    "\n",
    "Veamos como le va a nuestra implementación de perceptron en comparación. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c572b27b",
   "metadata": {},
   "source": [
    "<b>Datos para probar</b>\n",
    "\n",
    "Cuando parecía que ya se había terminado de hundir, vamos a reflotar al Titanic, una vez más. Este set de datos ya lo conocemos, puesto que lo usamos muchas veces como ejemplo en la asignatura Ciencia de Datos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e0ca1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Una vez más cargamos y procesamos el Titanic\n",
    "file = open(\"Titanic.csv\", \"r\")\n",
    "titanic = file.read()\n",
    "file.close()\n",
    "titanic = titanic.split(\"\\n\")\n",
    "for j in range(len(titanic)):\n",
    "    titanic[j] = titanic[j].split(\",\")\n",
    "\n",
    "# El nombre tiene ',' así que necesitamos volver a unirlo.\n",
    "for elemento in titanic[1:]:\n",
    "    elemento[3] = elemento[3]+elemento[4]\n",
    "    del elemento[4]\n",
    "titanic = np.array(titanic)\n",
    "titanic = np.delete(titanic, [3,8,10,11], axis=1)\n",
    "\n",
    "# Codificamos género como 0-1\n",
    "for linea in titanic[1:]:\n",
    "    linea[3] = '0' if linea[3] == 'male' else '1'\n",
    "\n",
    "# Quitamos las observaciones con datos perdidos\n",
    "borrar = []\n",
    "for l in range(len(titanic)):\n",
    "    if '' in titanic[l]:\n",
    "        borrar.append(l)\n",
    "titanic = np.delete(titanic, borrar, axis=0)\n",
    "\n",
    "# Arreglamos vector y matriz como array\n",
    "y = titanic[:,1][1:]\n",
    "X = titanic[:,2:][1:]\n",
    "X, y = np.array(X, dtype='float64'), np.array(y, dtype='float64')\n",
    "\n",
    "# Aplicamos Normalización Min-Max\n",
    "X= MinMaxScaler().fit_transform(X)\n",
    "\n",
    "# Separamos la muestra al azar, 60% para entrenar, 40% para testeo final.\n",
    "# La proporción para entrenamiento es alta dado que el Perceptrón es sensible\n",
    "# a datos \"adversos\", así que estamos tratando de reducir ese riesgo.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c54093",
   "metadata": {},
   "source": [
    "<b>Modelo Base para comparar</b>\n",
    "\n",
    "Cualquier resultado que obtengamos con el Perceptrón, ¿cómo podríamos calificarlo de bueno o malo? En general no se puede, pero sí podemos decir observar si se comporta mejor, peor o similar a otros modelos. Por eso es bueno tener un modelo que sirvan de línea base de comparación. En problemas de clasificación se suele usar Regresión Logística para este fin. \n",
    "\n",
    "El Perceptrón, como hemos visto, es un modelo de clasificación líneal, es decir se basa en una frontera de decisión caracterizada como una recta. Tiene por tanto doble sentido el comparar el perceptrón con la regresión logística, dado que ambos son modelos lineales.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab231f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#Se define y entrena la Regresión Logística\n",
    "lg_model = LogisticRegression().fit(X_train, y_train)\n",
    "\n",
    "# Utilizamos el modelo para predecir\n",
    "y_hat = lg_model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55f90c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "# Reportamos los resultados del modelo\n",
    "matriz_conf = confusion_matrix(y_test, y_hat)\n",
    "\n",
    "print('MATRIZ DE CONFUSIÓN para modelo ANN')\n",
    "print(matriz_conf,'\\n')\n",
    "print('La exactitud de testeo con Regresión Logística es: {:.3f}'.format(accuracy_score(y_test,y_hat)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df9c09b",
   "metadata": {},
   "source": [
    "Con regresión logística logramos clasificar con una exactitud mayor de lo que se podría justificar con simple azar. La teoría es que el perceptrón debiera lograr resultados similares. Nótese que la exactitud reportada corresponde a la que se obtiene con datos de testeo, distintos a los que se usó para ajustar la regresión.\n",
    "\n",
    "Debemos tener en cuenta, eso sí, que el perceptrón y especialmente nuestra rudimentaria implemetación, podría enfrentar problemas de optimización que le impidan converger a una solución adecuada. El experimento que se ha organizado con una distribución de 60-40% entre entrenamiento y testeo, en parte para tratar de evitar distribuciones adversas en los datos. Si ocurriera que nuestro perceptrón no aprende (clasifica mal), repitiendo el experimento desde la organización de los set de datos podríamos obtener otra versión que sí aprenda. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2273dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Entrenamos un Perceptrón con los datos de Titanic\n",
    "perceptron = Perceptron(learning_rate=0.01, n_iters=1000)\n",
    "perceptron.fit(X_train, y_train)\n",
    "\n",
    "# Utilizamos el Perceptron para predecir\n",
    "y_hat = perceptron.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4c3092",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "# Reportamos los resultados del modelo\n",
    "matriz_conf = confusion_matrix(y_test, y_hat)\n",
    "\n",
    "print('MATRIZ DE CONFUSIÓN para modelo ANN')\n",
    "print(matriz_conf,'\\n')\n",
    "print('La exactitud de testeo con Perceptrón es: {:.3f}'.format(accuracy_score(y_test,y_hat)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d279ef9e",
   "metadata": {},
   "source": [
    "Si hemos repetido el experimento con el perceptrón un par de veces, es altamente probable que observemos resultados equivalentes a los que produce regresión logística. Con esto estamos validando que el perceptrón tiene la capacidad de resolver problemas linealmente separables, al igual que lo hace regresión logística. \n",
    "\n",
    "Por ahora no tenemos una implementación que asegure la convergencia del aprendizaje, y es que nuestro enfoque para la optimización de pesos del perceptrón es todavía muy básico, tiene el potencial de funcionar pero no garantía de que ocurra. Esto se refleja en que cuando repetimos el experimento nuestro perceptrón no siempre logra el aprendizaje. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf859c6",
   "metadata": {},
   "source": [
    "<b>PERCEPTRON PARA REGRESIÓN</b>\n",
    "\n",
    "Ya hemos visto que el perceptrón puede resolver problemas de clasificación, <i>¿puede el perceptrón resolver problemas de regresión?, ¿qué modificación habría que realizar para lograrlo?</i>\n",
    "\n",
    "Vamos a utilizar los datos de producción de energía en una planta de ciclo combinado, que ya habíamos usado para regresión en Ciencia de Datos.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "567d74b0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MinMaxScaler\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Importar datos desde csv\n",
    "file = open(\"Ciclo_Combinado.csv\", \"r\")\n",
    "obs = file.read()\n",
    "file.close()\n",
    "obs = obs.split(\"\\n\")\n",
    "# hay un salto de línea demás en el archivo\n",
    "obs.remove(obs[-1])\n",
    "# dividir los datos por coma. \n",
    "for j in range(len(obs)):\n",
    "    obs[j] = obs[j].split(\",\")\n",
    "# transformar a numpy array.\n",
    "obs = np.array(obs)\n",
    "# Valores Y (variable dependiente)\n",
    "y = obs[:,-1][1:]\n",
    "# Matriz X (variables independientes)\n",
    "X = obs[:,:-1][1:]\n",
    "\n",
    "# Los valores de X e y tienen que ser numéricos\n",
    "X, y = np.array(X, dtype='float64'), np.array(y, dtype='float64')\n",
    "\n",
    "# Usamos Min-Max para estandarizar el rango de datos \n",
    "X= MinMaxScaler().fit_transform(X)\n",
    "\n",
    "# Usaremos 80% para entrenar y 20% para testear\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757d7f93",
   "metadata": {},
   "source": [
    "Sabemos que este tipo de problema se puede resolver usando Regresión Lineal, modelo que usaremos como base para comparar los resultados que ustedes obtendrán adaptando el Perceptrón. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcccc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "#Se define el modelo\n",
    "model = LinearRegression().fit(X_train, y_train)\n",
    "r_sq = model.score(X_test, y_test)\n",
    "print('Coeficiente de determinación (R2):', r_sq)\n",
    "print('Intercepto (Beta0):', model.intercept_)\n",
    "print('Pendiente (Betas 1 en adelante):', model.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271edd06",
   "metadata": {},
   "source": [
    "<b>Actividad de aprendizaje práctico</b>\n",
    "\n",
    "Modifique la implementación de nuestro perceptrón para que se pueda utilizar como modelo de regresión. Compare los resultados con los que se obtiene regresión lineal.  \n",
    "\n",
    "Para mayor provecho de esta tarea evitaremos buscar la solución ya implementada o descansar en servicios AI. Discutamos qué necesita ser modificado y demostremos que podemos hacer ese cambio por nuestra cuenta. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7024b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Haga acá su experimento con perceptrón para regresión,\n",
    "# agregue celdas que necesite,\n",
    "# incluya comparación con regresión lineal. \n",
    "\n",
    "# Modifique la clase Perceptrón para usar en regresión\n",
    "class Perceptron:\n",
    "    def __init__(self, learning_rate=0.01, n_iters=1000):\n",
    "        # Valores por defecto en caso que no se especifique Learnng rate o n_iters\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iters = n_iters\n",
    "        # Pesos y sesgo\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "            \n",
    "    def _unit_step_function(self, x):\n",
    "        # Esta es la función de activación\n",
    "        return np.where(x >= 0, 1, 0)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Método que se empleará para el entrenamiento (ajuste) del Perceptrón\n",
    "        # Se identifica cuantos datos hay en la muestra y con cuantos atributos\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        # Se inicializan tantos pesos como atributos...\n",
    "        self.weights = np.zeros(n_features)\n",
    "        # ...más un sesgo\n",
    "        self.bias = 0\n",
    "        # Por ahora hemos iniciado todos los pesos y sesgo en valor cero. \n",
    "        # Más adelante debieramos revisar esta decisión. \n",
    "        \n",
    "        for _ in range(self.n_iters):\n",
    "            # Iteraremos haciendo ajuste de los pesos\n",
    "            for idx, x_i in enumerate(X):\n",
    "                # Primero calculamos la salida de la transformación lineal: X*W + B\n",
    "                linear_output = np.dot(x_i, self.weights) + self.bias\n",
    "                # La salida de esa transformación lineal la pasamos por la función de activación.\n",
    "                y_predicted = self._unit_step_function(linear_output)\n",
    "                # Calculamos la magnitud de actualización, que es proporcional al learning rate. \n",
    "                # Note que Si y == y_hat; => update == 0. \n",
    "                delta = self.learning_rate * (y[idx] - y_predicted)\n",
    "                # Actualizamos pesos y sesgo. \n",
    "                self.weights += delta * x_i\n",
    "                self.bias += delta\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Método que usaremos para obtener predicciones con el Perceptrón\n",
    "        # Primero calculamos la salida de la transformación lineal: X*W + B\n",
    "        linear_output = np.dot(X, self.weights) + self.bias\n",
    "        # La salida de esa transformación lineal la pasamos por la función de activación.\n",
    "        y_predicted = self._unit_step_function(linear_output)\n",
    "        # y esa es nuestra predicción, así que la retornamos. \n",
    "        return y_predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7028f3e",
   "metadata": {},
   "source": [
    "<b>CONCLUSIÓN</b>\n",
    "\n",
    "Hemos presentado el Perceptrón, modelo matemático que es la unidad básica de las redes neuronales artificales. Hemos visto que el perceptrón puede realizar tareas de clasificación y regresión, pero también observamos que su limitación natural sólo permite que realice estas tareas en forma lineal. Es decir, sólo puede separar aquello que es linealmente separable.\n",
    "\n",
    "Hemos visto que el perceptrón aprende a partir de observar datos previamente clasificados (aprendizaje supervisado), y lo hace ajustando sus parámetros internos (pesos y sesgo) iterativamente, usando el error de salida (diferencia entre predicción y realidad) para indicar la dirección adecuada en ese ajuste. Tenemos a la vista el código (Python) de toda esta operación, por lo que no hay misterio respecto al cómo aprende el perceptrón. \n",
    "\n",
    "Podemos modificar nuestra implementación de perceptrón para que satisfaga diversos propósitos, usarlo para regresión en lugar de clasificación es sólo un ejemplo de lo que podemos hacer. \n",
    "\n",
    "En nuestra próxima clase extenderemos el concepto de perceptrón a capas, permitendo así conformar redes neuronales con múltiples capas. Por ahora seguiremos trabajando en nuestra propia implementación, para así asegurarnos de entender cómo ocurre el aprendizaje en nuestro modelo de red neuronal. Cuando más adelante aprovechemos modelos más avanzados y que ya están implementados, siempre sobre la base de conocer suficientemente su funcionamiento para asegurar que no nos parezca magia. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
