{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26092db2",
   "metadata": {},
   "source": [
    "<head><b><center>MACHINE LEARNING</center>\n",
    "\n",
    "<center>Redes Neuronales Artificiales</center>\n",
    "    \n",
    "\n",
    "<center>Profesor: Gabriel Jara </center></head><br>\n",
    "El presente Jupyter Notebook busca:\n",
    "<ul>\n",
    "    <li>Retomar conceptos sobre Redes Neuronales Artificiales (ANN).</li>\n",
    "    <li>Profundizar en el mecanismo de aprendizaje de las ANN</li>\n",
    "    <li>Analizar la implementación de una red neuronal sin apoyo de librerías especializadas.</li>\n",
    "</ul>\n",
    "\n",
    "Fuentes: \n",
    "<ul>\n",
    "    <li>Imágenes han sido robadas de internet y son enlaces a su correspondiente fuente. </li>\n",
    "</ul> \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d672aa9",
   "metadata": {},
   "source": [
    "<b>En el capítulo anterior </b>\n",
    "\n",
    "Extendimos a idea del perceptrón básico...\n",
    "<br> <img src=https://images.deepai.org/glossary-terms/perceptron-6168423.jpg\n",
    "          width=\"400\"/>\n",
    "\n",
    "...para construir una red de perceptrones, <b><i>Multi-Layer Perceptron</i></b>, que es nuestra primera Red Neuronal Artificial. \n",
    "<br> <img src=https://miro.medium.com/v2/resize:fit:563/1*4_BDTvgB6WoYVXyxO8lDGA.png\n",
    "          width=\"350\"/>\n",
    "          \n",
    "Esta topología será la base sobre la que exploraremos algunas expansiones, ajustándonos a distintos problemas y buscando como mejorar los resultados.           "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98b30d9",
   "metadata": {},
   "source": [
    "<b>Implementación Base</b>\n",
    "\n",
    "La siguiente es la misma implementación que dejamos en la clase previa, con la única diferencia de que hemos comentado el seed para controlar aleatoriedad en la inicialización de pesos. Las Redes Neuronales Artificiales son modelos probabilísticos, no se supone que ante una misma muestra den siempre los mismos resultados, es parte de su valor el que no lo hagan. Mantendremos los seed (pero comentados) por si en algún momento necesitamos resultados reproducibles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5fdbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Clase base para Capa\n",
    "class Layer:\n",
    "    def __init__(self):\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "    # computes the output Y of a layer for a given input X\n",
    "    def forward_propagation(self, input):\n",
    "        raise NotImplementedError\n",
    "    # computes dE/dX for a given dE/dY (and update parameters if any)\n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "# Clase para capas densas (fully connected)\n",
    "class FCLayer(Layer):\n",
    "    # input_size = number of input neurons\n",
    "    # output_size = number of output neurons\n",
    "    def __init__(self, input_size, output_size):\n",
    "        # YA NO CONTROLAREMOS ALEATORIEDAD\n",
    "        #np.random.seed(1234)\n",
    "        self.weights = np.random.rand(input_size, output_size, ) - 0.5\n",
    "        self.bias = np.random.rand(1, output_size) - 0.5\n",
    "\n",
    "    # returns output for a given input\n",
    "    def forward_propagation(self, input_data):\n",
    "        self.input = input_data\n",
    "        self.output = np.dot(self.input, self.weights) + self.bias\n",
    "        return self.output\n",
    "\n",
    "    # computes dE/dW, dE/dB for a given output_error=dE/dY. Returns input_error=dE/dX.\n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        input_error = np.dot(output_error, self.weights.T)\n",
    "        weights_error = np.dot(self.input.T, output_error)\n",
    "\n",
    "        # update parameters\n",
    "        self.weights -= learning_rate * weights_error\n",
    "        self.bias -= learning_rate * output_error\n",
    "        return input_error\n",
    "\n",
    "# Clase para Capa de Activación. Junto con la capa densa forman perceptrones. \n",
    "class ActivationLayer(Layer):\n",
    "    def __init__(self, activation, activation_prime):\n",
    "        self.activation = activation\n",
    "        self.activation_prime = activation_prime\n",
    "\n",
    "    # returns the activated input\n",
    "    def forward_propagation(self, input_data):\n",
    "        self.input = input_data\n",
    "        self.output = self.activation(self.input)\n",
    "        return self.output\n",
    "\n",
    "    # Returns input_error=dE/dX for a given output_error=dE/dY.\n",
    "    # learning_rate is not used because there is no \"learnable\" parameters.\n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        return self.activation_prime(self.input) * output_error\n",
    "    \n",
    "# Clase Red, conecta múltiples capas.\n",
    "class Network:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        self.loss = None\n",
    "        self.loss_prime = None\n",
    "\n",
    "    # add layer to network\n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    # set loss to use\n",
    "    def use(self, loss, loss_prime):\n",
    "        self.loss = loss\n",
    "        self.loss_prime = loss_prime\n",
    "\n",
    "    # predict output for given input\n",
    "    def predict(self, input_data):\n",
    "        # sample dimension first\n",
    "        input_data = np.array([[x] for x in input_data])\n",
    "        samples = len(input_data)\n",
    "        result = []\n",
    "\n",
    "        # run network over all samples\n",
    "        for i in range(samples):\n",
    "            # forward propagation\n",
    "            output = input_data[i]\n",
    "            for layer in self.layers:\n",
    "                output = layer.forward_propagation(output)\n",
    "            result.append(output)\n",
    "        return result\n",
    "\n",
    "    # train the network\n",
    "    def fit(self, x_train, y_train, epochs, learning_rate):\n",
    "        # sample dimension first\n",
    "        x_train = np.array([[x] for x in x_train])\n",
    "        samples = len(x_train)\n",
    "        # training loop\n",
    "        for i in range(epochs):\n",
    "            err = 0\n",
    "            for j in range(samples):\n",
    "                # forward propagation\n",
    "                output = x_train[j]\n",
    "                for layer in self.layers:\n",
    "                    output = layer.forward_propagation(output)\n",
    "\n",
    "                # compute loss (for display purpose only)\n",
    "                err += self.loss(y_train[j], output)\n",
    "\n",
    "                # backward propagation\n",
    "                error = self.loss_prime(y_train[j], output)\n",
    "                for layer in reversed(self.layers):\n",
    "                    error = layer.backward_propagation(error, learning_rate)\n",
    "\n",
    "            # calculate average error on all samples\n",
    "            err /= samples\n",
    "            \n",
    "            # Para usar en clasificación (con más de dos clases)\n",
    "            # calculamos el error promedio entre nodos de salida.\n",
    "            err = np.mean(err)\n",
    "            \n",
    "            # Imprimomos el error promedio de cada época, más que nada\n",
    "            # para seguimiento del aprendizaje. \n",
    "            print('epoch %d/%d   error=%f' % (i+1, epochs, err))\n",
    "            \n",
    "# Funciones de Activación, y su correspondiente función derivada. \n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_prime(x):\n",
    "    return 1-np.tanh(x)**2\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_prime(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "# Función de pérdida y su derivada. \n",
    "def mse(y_real, y_hat):\n",
    "    return (y_real-y_hat)**2\n",
    "\n",
    "def mse_prime(y_real, y_hat):\n",
    "    return 2*(y_hat-y_real)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6755fdde",
   "metadata": {},
   "source": [
    "<b>Reconocimiento de Imágenes con MNIST</b>\n",
    "\n",
    "Ya hemos probado nuestra red con <b><i>Mnist</i></b>, colección de imágenes de dígitos (números) escritos a mano por distintas personas. \n",
    "<br> <img src=https://www.ttested.com/gallery/thumbnails/ditch-mnist.jpg\n",
    "          width=\"400\"/>\n",
    "\n",
    "La siguiente celda carga muestras para entrenar y para testear resultados.     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f98a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "import random\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# No necesitamos tantos datos.\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "#random.seed(123) # Vamos a controlar la aleatoriedad en adelante. \n",
    "X, y = zip(*random.sample(list(zip(X_train, y_train)), 2000))\n",
    "\n",
    "# Sí necesitamos que la forma de X sea la de un vector, en lugar de una matriz. \n",
    "X, y = np.array(X, dtype='float64'), np.array(y, dtype='float64')\n",
    "X = np.reshape(X, (X.shape[0], -1))\n",
    "\n",
    "# Normalizamos Min-Max\n",
    "X= MinMaxScaler().fit_transform(X)\n",
    "\n",
    "# Dividimos la muestra en dos, una para entrenar y otra para testing, como tenemos \n",
    "# muestra de sobra nos damos el lujo de testear con la misma cantidad que entrenamos.\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=123)\n",
    "\n",
    "# Necesitamos que y_train sea un valor categórico, en lugar de un dígito entero.\n",
    "y_train_value = y_train # Guardaremos y_train como valor para un observación más abajo.\n",
    "from keras.utils import to_categorical\n",
    "y_train = to_categorical(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fcf042",
   "metadata": {},
   "source": [
    "A continuación usamos nuestra red neuronal para aprender <i>Mnist</i> y evaluar el resultado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8cf120",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "# Necesitamos identificar cuantos nodos tiene nuestra entrada, y eso depende del tamaño de X.\n",
    "entrada_dim = len(X_train[0])\n",
    "\n",
    "# Crear instancia de Network\n",
    "model = Network()\n",
    "\n",
    "# Agregamos capas al modelo\n",
    "model.add(FCLayer(entrada_dim, 16))               \n",
    "model.add(ActivationLayer(tanh, tanh_prime))\n",
    "model.add(FCLayer(16, 10))                                                   \n",
    "model.add(ActivationLayer(sigmoid, sigmoid_prime))\n",
    "\n",
    "# Usar el modelo creado\n",
    "model.use(mse, mse_prime)\n",
    "model.fit(X_train, y_train, epochs=10, learning_rate=0.1)\n",
    "\n",
    "# Usamos el modelo para predecir sobre el conjunto de prueba\n",
    "y_hat = model.predict(X_test)\n",
    "\n",
    "# Transformamos la salida en un vector one-hot encoded, es decir 0s y un 1. \n",
    "for i in range(len(y_hat)):\n",
    "    y_hat[i] = np.argmax(y_hat[i][0])\n",
    "\n",
    "# Reportamos los resultados del modelo\n",
    "matriz_conf = confusion_matrix(y_test, y_hat)\n",
    "\n",
    "print('MATRIZ DE CONFUSIÓN para modelo ANN')\n",
    "print(matriz_conf,'\\n')\n",
    "print('La exactitud de testeo del modelo ANN es: {:.3f}'.format(accuracy_score(y_test,y_hat)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fea3345",
   "metadata": {},
   "source": [
    "Tal como observamos la clase anterior, el resultado es muy bueno en comparación con lo que se habría logrado por simple azar. Considerando, además, que la muestra de testeo son observaciones no disponibles durante el entrenamiento, podemos decir con alta certeza que el modelo a \"aprendido\" a reconocer dígitos escritos a mano, incluso si todavía el resultado no es altamente fiable. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0af262",
   "metadata": {},
   "source": [
    "<b>Comentario sobre la inicialización de los pesos $W$</b>\n",
    "\n",
    "En nuestra implementación cuando se crea una capa densa (<i>fully connected</i>) inicializamos cada peso $w$ con un valor aleatorio. Sin embargo, cuando en la clase anterior implementamos un perceptrón pusimos que los pesos $w$ tuvieran valor cero al inicializarse. Observamos que una vez entrenado el perceptrón los pesos $w$ ajustan su valor, por lo que rapidamene dejan de ser cero. En el caso de una red neuronal, también se produce el ajuste de los pesos $w$ al realizar el entrenamiento. \n",
    "\n",
    "<b>¿Por qué entonces no adoptar valor cero para los $w$ al inicializar la red?, ¿qué diferencia hace el que hagamos un trabajo adicional buscando pesos aleatorios cuando podrían simplemente ser cero?</b>\n",
    "\n",
    "Podemos hacer el experimento de inicializar todos los pesos $w$ en cero, en la red neuronal, y apreciar si ello perjudica la capacidad de aprendizaje del modelo. \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5dbe91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sólo cambiaremos la clase FCLayer\n",
    "# Los pesos w quedarán inicializados en cero. \n",
    "class FCLayer(Layer):\n",
    "    # input_size = number of input neurons\n",
    "    # output_size = number of output neurons\n",
    "    def __init__(self, input_size, output_size):\n",
    "        # Los pesos y el sesgo (b) quedarán todos inicializados en cero.\n",
    "        # Recuerde que estamos haciendo un experimento, no hemos dicho \n",
    "        # que sea buena idea hacer esto... \n",
    "        self.weights = np.zeros((input_size, output_size))\n",
    "        self.bias = np.zeros((1, output_size))\n",
    "\n",
    "    # returns output for a given input\n",
    "    def forward_propagation(self, input_data):\n",
    "        self.input = input_data\n",
    "        self.output = np.dot(self.input, self.weights) + self.bias\n",
    "        return self.output\n",
    "\n",
    "    # computes dE/dW, dE/dB for a given output_error=dE/dY. Returns input_error=dE/dX.\n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        input_error = np.dot(output_error, self.weights.T)\n",
    "        weights_error = np.dot(self.input.T, output_error)\n",
    "\n",
    "        # update parameters\n",
    "        self.weights -= learning_rate * weights_error\n",
    "        self.bias -= learning_rate * output_error\n",
    "        return input_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212bf315",
   "metadata": {},
   "source": [
    "A continuación, volveremos a realizar el experimento de entrenar nuestra red para aprender <i>Mnist</i>, veamos si mantiene la capacidad de aprendizaje. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0a8178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necesitamos identificar cuantos nodos tiene nuestra entrada, y eso depende del tamaño de X.\n",
    "entrada_dim = len(X_train[0])\n",
    "\n",
    "# Crear instancia de Network\n",
    "model = Network()\n",
    "\n",
    "# Agregamos capas al modelo\n",
    "model.add(FCLayer(entrada_dim, 16))               \n",
    "model.add(ActivationLayer(tanh, tanh_prime))\n",
    "model.add(FCLayer(16, 10))                                                   \n",
    "model.add(ActivationLayer(sigmoid, sigmoid_prime))\n",
    "\n",
    "# Usar el modelo creado\n",
    "model.use(mse, mse_prime)\n",
    "model.fit(X_train, y_train, epochs=10, learning_rate=0.1)\n",
    "\n",
    "# Usamos el modelo para predecir sobre el conjunto de prueba\n",
    "y_hat = model.predict(X_test)\n",
    "\n",
    "# Transformamos la salida en un vector one-hot encoded, es decir 0s y un 1. \n",
    "for i in range(len(y_hat)):\n",
    "    y_hat[i] = np.argmax(y_hat[i][0])\n",
    "\n",
    "# Reportamos los resultados del modelo\n",
    "matriz_conf = confusion_matrix(y_test, y_hat)\n",
    "\n",
    "print('\\nMATRIZ DE CONFUSIÓN para modelo ANN')\n",
    "print(matriz_conf,'\\n')\n",
    "print('La exactitud de testeo del modelo ANN es: {:.3f}'.format(accuracy_score(y_test,y_hat)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10bfaafa",
   "metadata": {},
   "source": [
    "Se observa claramente que ahora la red no logra aprender nada, obtiene resultado no mejor que el azar, clasificando todas las observaciones como de la misma clase. ¿Qué clase elige? el resultado es aleatorio, pero debiera haber una marcada tendencia a elegir la clase dominante (o al menos entre las de alta frecuencia) en la muestra de entrenamiento. Claro que la misma clase no tendría por qué tener alta frecuencia en la muestra de testeo.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3d01e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Crear el histograma\n",
    "plt.hist(y_train_value, bins=30, edgecolor='black')\n",
    "\n",
    "# Añadir títulos y etiquetas\n",
    "plt.title('Histograma de y_train_value')\n",
    "plt.xlabel('Valor')\n",
    "plt.ylabel('Frecuencia')\n",
    "\n",
    "# Mostrar el histograma\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6987fc6a",
   "metadata": {},
   "source": [
    "El problema que se produce es que, al inicializar todos los pesos en el mismo valor (en este caso cero) y aplicarles el mismo gradiente durante el entrenamiento, el ajuste al que es sometido cada peso es igual en magnitud y dirección ante cualquier muestra, durante todos los epochs. Es decir, los pesos siguen teniendo el mismo valor entre sí por más que se les ajuste, dado que por <b><i>simetría</i></b> en el proceso de ajuste siempre se mantienen sincronizados. El nombre de este problema es, justamente, simetría. \n",
    "\n",
    "Cuando dejamos que los perceptrones de nuestra red se entrenen simétricamente le quitamos la capacidad de aprovechar la multiplicidad de capas y perceptrones. En la práctica es como tener una red neuronal donde todas las neuronas son exactamente iguales y siempre se activan al unísono, lo que es equivalente a tener una única neurona. Con algo de suerte sólo puede aspirar a alinearse con la clase dominante, pero no mucho más. \n",
    "\n",
    "Así que devolvamos a nuestra red neuronal su capacidad, inicializando pesos aleatorios. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12470e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCLayer(Layer):\n",
    "    # input_size = number of input neurons\n",
    "    # output_size = number of output neurons\n",
    "    def __init__(self, input_size, output_size):\n",
    "        # YA NO CONTROLAREMOS ALEATORIEDAD\n",
    "        #np.random.seed(1234)\n",
    "        self.weights = np.random.rand(input_size, output_size, ) - 0.5\n",
    "        self.bias = np.random.rand(1, output_size) - 0.5\n",
    "\n",
    "    # returns output for a given input\n",
    "    def forward_propagation(self, input_data):\n",
    "        self.input = input_data\n",
    "        self.output = np.dot(self.input, self.weights) + self.bias\n",
    "        return self.output\n",
    "\n",
    "    # computes dE/dW, dE/dB for a given output_error=dE/dY. Returns input_error=dE/dX.\n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        input_error = np.dot(output_error, self.weights.T)\n",
    "        weights_error = np.dot(self.input.T, output_error)\n",
    "\n",
    "        # update parameters\n",
    "        self.weights -= learning_rate * weights_error\n",
    "        self.bias -= learning_rate * output_error\n",
    "        return input_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4280c84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necesitamos identificar cuantos nodos tiene nuestra entrada, y eso depende del tamaño de X.\n",
    "entrada_dim = len(X_train[0])\n",
    "\n",
    "# Crear instancia de Network\n",
    "model = Network()\n",
    "\n",
    "# Agregamos capas al modelo\n",
    "model.add(FCLayer(entrada_dim, 16))               \n",
    "model.add(ActivationLayer(tanh, tanh_prime))\n",
    "model.add(FCLayer(16, 10))                                                   \n",
    "model.add(ActivationLayer(sigmoid, sigmoid_prime))\n",
    "\n",
    "# Usar el modelo creado\n",
    "model.use(mse, mse_prime)\n",
    "model.fit(X_train, y_train, epochs=10, learning_rate=0.1)\n",
    "\n",
    "# Usamos el modelo para predecir sobre el conjunto de prueba\n",
    "y_hat = model.predict(X_test)\n",
    "\n",
    "# Transformamos la salida en un vector one-hot encoded, es decir 0s y un 1. \n",
    "for i in range(len(y_hat)):\n",
    "    y_hat[i] = np.argmax(y_hat[i][0])\n",
    "\n",
    "# Reportamos los resultados del modelo\n",
    "matriz_conf = confusion_matrix(y_test, y_hat)\n",
    "\n",
    "print('\\nMATRIZ DE CONFUSIÓN para modelo ANN')\n",
    "print(matriz_conf,'\\n')\n",
    "print('La exactitud de testeo del modelo ANN es: {:.3f}'.format(accuracy_score(y_test,y_hat)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0698c649",
   "metadata": {},
   "source": [
    "<b>Probando otra función de pérdida: <i>Binary Cross Entropy</i></b>\n",
    "\n",
    "<b>BCE</b> o Entropía Cruzada Binaria, es una función de pérdida más adecuada a la clasificación de lo que es <i>Mean Squared Error</i> (<b>MSE</b>) que venimos usando. \n",
    "\n",
    "MSE es una función de pérdida genérica, útil sobre todo en problemas de regresión, pese a que como hemos demostrado también funciona en clasificación. Puede llevar a resultados subóptimos, ya que no maneja bien la naturaleza probabilística de la predicción de clases, en contraste a predecir un valor escalar.\n",
    "\n",
    "Por otro lado, BCE está diseñada específicamente para tareas de clasificación binaria. Se usa típicamente en redes neuronales cuando la capa de salida utiliza una activación sigmoide (o similar) para generar probabilidades entre 0 y 1, que luego se interpretan como clase. Incluso si el problema de clasificación no es binario, como es el caso de <i>Mnist</i>, sigue siendo adecuado usar BCE dado que la topología de nuestra red tiene tantos nodos de salida como clases posibles, transformando así el resultado de la predicción en una colección de clasificaciones binarias.\n",
    "\n",
    "La entropía cruzada binaria se define a nivel de muestra de acuerdo con la siguiente fórmula:\n",
    "\n",
    "$$\\text{BCE} = - \\frac{1}{N} \\sum_{i=1}^{N} \\left[ y_i \\cdot \\log(\\hat{y}_i) + (1 - y_i) \\cdot \\log(1 - \\hat{y}_i) \\right]$$\n",
    "\n",
    "En el caso de evaluar una única observación se reduce a:\n",
    "\n",
    "$$\\text{BCE} = -  y \\cdot \\log(\\hat{y}) + (1 - y) \\cdot \\log(1 - \\hat{y}) $$\n",
    "\n",
    "La derivada de BCE corresponde a: \n",
    "\n",
    "$$\\frac{\\partial \\text{BCE}}{\\partial \\hat{y}} = -\\frac{y}{\\hat{y}} + \\frac{1 - y}{1 - \\hat{y}}$$\n",
    "\n",
    "<b>¿Cómo implementamos BCE en nuestra red Neuronal?</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e21671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Haga acá su implementación de BCE\n",
    "# Dado que trabajará con la función logaritmo, quizá convenga preveer el casos donde\n",
    "# log(0) = -infinito, para evitar errores por indefinición matemática.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d036417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pruebe si funciona. \n",
    "# No asuma que obtendrá de inmediato un resultado mejor que con MSE, ya que logre\n",
    "# clasificar es un buen resultado. Que sea o no mejor que los resultados previos con \n",
    "# MSE dependen de cuanto entrene y qué capacida tenga la red. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e671ca2",
   "metadata": {},
   "source": [
    "<b>Desvanecimiento del Gradiente (<i>Vanishing Gradient</i>)</b>\n",
    "\n",
    "El desvanecimiento del gradiente (vanishing gradient) es un problema que ocurre durante el entrenamiento de redes neuronales profundas, especialmente cuando se utilizan ciertas funciones de activación como sigmoidal o tangente hiperbólica. Este fenómeno dificulta el proceso de aprendizaje de la red, ya que afecta la capacidad de las capas más profundas de la red para ajustar sus pesos correctamente.\n",
    "\n",
    "El ajuste al que se somete cada peso $w$ durante el aprendizaje es proporcional a la derivada del error (función de pérdida) respecto a $\\hat{y}$. Ocurre que estamos usando funciones de pérdida como sigmoidal o tangente hiperbólica que tienen tramos extensos donde la derivada del error es cercana a cero, lo cual tiende a reducir la magnitud del ajuste. En las primeras capas no es notorio, aunque sea un ajuste pequeño puede estar compensado por la <i>learning rate</i>, y con suficientes iteraciones se logra igual la optimización. Pero si hay muchas capas en la red, durante el <i>backward propagation</i> el ajuste de pesos se va traspasando de capa en capa hacia atrás, siendo sucesivamente multiplicado por un factor cercado a cero (derivada de función de pérdida), haciendo que el ajuste sea cada vez más pequeño. \n",
    "\n",
    "Contribuye a lo anterior la multiplicación de los gradientes, que también van progresivamente haciendose cercanos a cero mientras nos acercamos a un óptimo. Esto significa que capa a capa, hacia atras, se va a comulando la multiplicacion de gradientes pequeños haciendo que la señal de error se desvanesca. Esta componente del problema ya no depende de la forma de la función de activación, y es estructural a la topología de las redes neuronales. \n",
    "\n",
    "<br> <img src=https://miro.medium.com/v2/resize:fit:828/format:webp/1*0QPRST83oBicKPE_R4biJA.png\n",
    "          width=\"500\"/></br>\n",
    "\n",
    "Resultado de lo anterior, vemos que la magnitud de los ajustes se va haciendo virtualmente cero en las capas iniciales de una red muy profunda (recuerde que el ajuste va hacia atras), y con ello pierde toda capacidad de aprendizaje. \n",
    "\n",
    "<br> <img src=https://blog.kakaocdn.net/dn/okTU1/btrCH4Fx7Ro/m5f9UkRkl7CP3dUEgXTtk1/img.png\n",
    "          width=\"600\"/></br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1608b00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCLayer(Layer):\n",
    "    # input_size = number of input neurons\n",
    "    # output_size = number of output neurons\n",
    "    def __init__(self, input_size, output_size):\n",
    "        # YA NO CONTROLAREMOS ALEATORIEDAD\n",
    "        #np.random.seed(1234)\n",
    "        self.weights = np.random.rand(input_size, output_size, ) - 0.5\n",
    "        self.bias = np.random.rand(1, output_size) - 0.5\n",
    "\n",
    "    # returns output for a given input\n",
    "    def forward_propagation(self, input_data):\n",
    "        self.input = input_data\n",
    "        self.output = np.dot(self.input, self.weights) + self.bias\n",
    "        return self.output\n",
    "\n",
    "    # computes dE/dW, dE/dB for a given output_error=dE/dY. Returns input_error=dE/dX.\n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        input_error = np.dot(output_error, self.weights.T)\n",
    "        weights_error = np.dot(self.input.T, output_error)\n",
    "        \n",
    "        # GUARDAMOS LA MAGNITUD DE GRADIENTES PARA ANALIZAR\n",
    "        self.gradients_magnitude = np.mean(np.abs(weights_error))\n",
    "\n",
    "        # update parameters\n",
    "        self.weights -= learning_rate * weights_error\n",
    "        self.bias -= learning_rate * output_error\n",
    "        return input_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64eb03bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necesitamos identificar cuantos nodos tiene nuestra entrada, y eso depende del tamaño de X.\n",
    "entrada_dim = len(X_train[0])\n",
    "\n",
    "# Crear instancia de Network\n",
    "model = Network()\n",
    "\n",
    "# CREAMOS UNA RED MÁS PROFUNDA, PARA PROPICIAR DESVANECIMIENTO DE GRADIENTE\n",
    "model.add(FCLayer(entrada_dim, 16))               \n",
    "model.add(ActivationLayer(sigmoid, sigmoid_prime))\n",
    "model.add(FCLayer(16, 16))\n",
    "model.add(ActivationLayer(sigmoid, sigmoid_prime))\n",
    "model.add(FCLayer(16, 16))\n",
    "model.add(ActivationLayer(sigmoid, sigmoid_prime))\n",
    "model.add(FCLayer(16, 16))\n",
    "model.add(ActivationLayer(sigmoid, sigmoid_prime))\n",
    "model.add(FCLayer(16, 16)) \n",
    "model.add(ActivationLayer(sigmoid, sigmoid_prime))\n",
    "'''model.add(FCLayer(16, 16)) \n",
    "model.add(ActivationLayer(sigmoid, sigmoid_prime))'''\n",
    "model.add(FCLayer(16, 10))                                                   \n",
    "model.add(ActivationLayer(sigmoid, sigmoid_prime))\n",
    "\n",
    "# Usar el modelo creado\n",
    "model.use(mse, mse_prime)\n",
    "model.fit(X_train, y_train, epochs=10, learning_rate=0.1)\n",
    "\n",
    "# Usamos el modelo para predecir sobre el conjunto de prueba\n",
    "y_hat = model.predict(X_test)\n",
    "\n",
    "# Transformamos la salida en un vector one-hot encoded, es decir 0s y un 1. \n",
    "for i in range(len(y_hat)):\n",
    "    y_hat[i] = np.argmax(y_hat[i][0])\n",
    "\n",
    "# Reportamos los resultados del modelo\n",
    "matriz_conf = confusion_matrix(y_test, y_hat)\n",
    "\n",
    "print('\\nMATRIZ DE CONFUSIÓN para modelo ANN')\n",
    "print(matriz_conf,'\\n')\n",
    "print('La exactitud de testeo del modelo ANN es: {:.3f}'.format(accuracy_score(y_test,y_hat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59daa3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Después del entrenamiento\n",
    "gradients = [layer.gradients_magnitude for layer in model.layers if isinstance(layer, FCLayer)]\n",
    "\n",
    "plt.plot(gradients)\n",
    "plt.title(\"Magnitud de los Gradientes a través de las Capas\")\n",
    "plt.xlabel(\"Capa\")\n",
    "plt.ylabel(\"Magnitud Media del Gradiente\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9834fd",
   "metadata": {},
   "source": [
    "Acá podemos apreciar como una red con capacidad \"de sobra\", al ser demasiado profunda, no logra aprender nada a raíz del desvanecimiento del gradiente. Una solución a este problema es usar funciones de pérdida que mitiguen este problema. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96824c77",
   "metadata": {},
   "source": [
    "<b>Agregando más funciones de activación: <i>ReLu</i></b>\n",
    "\n",
    "Nuestra implementación ya dispone de funciones de activación: Sigmoidal y Tangente Hiperbólica. Quizá recuerde que en ciencia de datos solíamos usar también funciones de activación lineal y lineal rectificada (<i>ReLu</i>), que justamente es popular como mitigador del problema del desvanecimiento del gradiente. \n",
    "\n",
    "<b>Implemente usted a continuación <i>ReLu</i> en nuestra red neuronal, y pruebe si mejora la situación previamente descrita. Incluya la gráfica de magnitud de gradiente por capa en su analisis</b>\n",
    "\n",
    "Para esta tarea tendrá que incluir la derivada de ReLu, con el problema de que justo en $x = 0$ es una función no derivable. Se le sugiere que para ese punto en particular considere que la derivada sea cero, es decir debiera usar una función aproximada de la derivada de ReLu. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac56da61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Haga acá la implementación de ReLu y su derivada. \n",
    "# Considere como abordar la derivada cuando x = 0 y se indefine\n",
    "# Se sugiere que la ReLu prima de x=0 sea 0. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d10435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pruebe acá una red profunda usando activación ReLu\n",
    "# Incluya la gráfica de gradientes por capa para analizar\n",
    "# efecto sobre el desvanecimiento del gradiente. \n",
    "# No espere ver que el problema desaparece del todo. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296766f3",
   "metadata": {},
   "source": [
    "<b>ReLu y el desvanecimiento del gradiente</b>\n",
    "\n",
    "Si implementó <i>ReLu</i> y la probó como función de activación de la Red Neuronal (en las capas intermedias), probablemente observe que efectivamente se mitiga el desvanecimiento del gradiente, pero no desaparece y sigue perjudicando el aprendizaje. Sólo una parte del problema se origina en la función de activación. Además, ReLu también tiene un tramo donde su derivada se hace cero. \n",
    "\n",
    "ReLU tiene la ventaja de que en la región positiva $(x > 0)$, donde la función es lineal y su derivada es constante e igual a 1. Esto significa que, a diferencia de funciones como sigmoid o tanh, los gradientes no se reducen exponencialmente en cada capa (propiciado por la función de activación), lo que ayuda a que las señales (gradientes) se mantengan lo suficientemente grandes para actualizar los pesos de manera efectiva.\n",
    "\n",
    "A pesar de que ReLU ayuda a mantener gradientes más grandes que sigmoid o tanh, en redes profundas los gradientes aún pueden disminuir a medida que se propagan hacia atrás a través de muchas capas. Aunque el efecto es menos severo que con otras funciones, sigue siendo un problema a medida que la profundidad de la red aumenta.\n",
    "\n",
    "<b>ReLu tiende a generar representaciones más dispersas</b>\n",
    "\n",
    "Otro tema importante con ReLU es que para valores negativos de entrada $(x < 0)$, la salida de ReLU es 0, y la derivada también es 0. Si una neurona recibe una entrada negativa persistente durante el entrenamiento, su salida será 0 y su gradiente será 0, lo que implica que no se actualizará (esta neurona se \"muere\"). Aunque no causa que el gradiente se desvanezca en el sentido clásico, sí lleva a que algunas neuronas no contribuyan al aprendizaje.\n",
    "\n",
    "¿Es esto un problema? Veremos que no necesariamente. Esta <b><i>\"esparsidad\"</i></b> actúa como una forma de regularización porque reduce la capacidad de la red para aprender relaciones espurias o para ajustarse demasiado a los datos de entrenamiento (sobreajuste). Al \"apagar\" algunas neuronas, ReLU reduce la complejidad efectiva del modelo, lo que puede llevar a una mejor generalización en nuevos datos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4b4f3c",
   "metadata": {},
   "source": [
    "<b>¿Y si incrementamos el número de Neuronas en lugar del número de Capas?</b>\n",
    "\n",
    "Agregar capas incrementa la capacidad de la red, pero a riesgo de desvanecimiento del gradiente. Podemos evitar ese riesgo incrementando el número de neuronas en lugar de incrementar las capas. Mantengamos algunas capas y muchas más neuronas, veamos como nos va.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b536a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necesitamos identificar cuantos nodos tiene nuestra entrada, y eso depende del tamaño de X.\n",
    "entrada_dim = len(X_train[0])\n",
    "\n",
    "# Crear instancia de Network\n",
    "model = Network()\n",
    "\n",
    "# USEMOS MÁS NEUROANAS.\n",
    "# Mantenerlos en potencias de 2 lo hace ver más científico!\n",
    "model.add(FCLayer(entrada_dim, 256))               \n",
    "model.add(ActivationLayer(sigmoid, sigmoid_prime))\n",
    "model.add(FCLayer(256, 256))\n",
    "model.add(ActivationLayer(sigmoid, sigmoid_prime))\n",
    "model.add(FCLayer(256, 10))                                                   \n",
    "model.add(ActivationLayer(sigmoid, sigmoid_prime))\n",
    "\n",
    "# Usar el modelo creado\n",
    "model.use(mse, mse_prime)\n",
    "model.fit(X_train, y_train, epochs=20, learning_rate=0.1) # DUPLIQUEMOS EL ENTRENAMIENTO\n",
    "\n",
    "# Usamos el modelo para predecir sobre el conjunto de prueba\n",
    "y_hat = model.predict(X_test)\n",
    "\n",
    "# Transformamos la salida en un vector one-hot encoded, es decir 0s y un 1. \n",
    "for i in range(len(y_hat)):\n",
    "    y_hat[i] = np.argmax(y_hat[i][0])\n",
    "\n",
    "# Reportamos los resultados del modelo\n",
    "matriz_conf = confusion_matrix(y_test, y_hat)\n",
    "\n",
    "print('\\nMATRIZ DE CONFUSIÓN para modelo ANN')\n",
    "print(matriz_conf,'\\n')\n",
    "print('La exactitud de testeo del modelo ANN es: {:.3f}'.format(accuracy_score(y_test,y_hat)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ecfe9ca",
   "metadata": {},
   "source": [
    "El éxito de la clasificación sobre el set de testeo no está tan bueno como esperaríamos si observamos cómo se reduce el error durante el entrenamiento, todavía falla del orden del 10% de los casos. \n",
    "\n",
    "¿Cómo le va a nuestro modelo en el set de datos de entrenamiento?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2890fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usamos el modelo para predecir sobre el conjunto de ENTRENAMIENTO\n",
    "y_hat = model.predict(X_train)\n",
    "\n",
    "# Transformamos la salida en un vector one-hot encoded, es decir 0s y un 1. \n",
    "for i in range(len(y_hat)):\n",
    "    y_hat[i] = np.argmax(y_hat[i][0])\n",
    "\n",
    "# Reportamos los resultados del modelo\n",
    "matriz_conf = confusion_matrix(y_train_value, y_hat)\n",
    "\n",
    "print('\\nMATRIZ DE CONFUSIÓN para modelo ANN')\n",
    "print(matriz_conf,'\\n')\n",
    "print('La exactitud de ENTRENAMIENTO del modelo ANN es: {:.3f}'.format(accuracy_score(y_train_value,y_hat)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2cba31d",
   "metadata": {},
   "source": [
    "Claramente en el entrenamiento se logra mucho mejor resultado con los datos del entrenamiento que de testeo. Esto ya sabemos de qué se trata. \n",
    "\n",
    "<br> <img src=https://pbs.twimg.com/media/FSAM8F1WQAMIIwB.jpg \n",
    "          width=\"250\">\n",
    "                    \n",
    "<b>OVERFITTING, viejo enemigo</b>\n",
    "\n",
    "Aumentar la capacidad de representación de la red, ya sea por disponer de más capas, más neuronas o más tiempo de entrenamiento, estamos generando las condiciones que facilitan el <b><i>overfitting</i></b>. Podemos tratar de controlarlo por la vía de seleccionar hiperparámetros de configuración del modelo buscando que no tenga demasiada capacidad. Sin embargo, con esto estamos limitando nuestra posibilidad de aprovechar modelos más robustos. El aprendizaje profundo, que ha permitido generar los modelos AI que sorprenden al mundo, requiere que podamos enfrentar este problema por otra vía."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90048d6a",
   "metadata": {},
   "source": [
    "<b>REGULARIZACIÓN</b>\n",
    "\n",
    "La regularización es un conjunto de técnicas utilizadas en machine learning y en particularmente en redes neuronales, para prevenir el overfitting, es decir que el modelo se ajuste demasiado a los datos de entrenamiento y pierde capacidad de generalizar sobre nuevas observaciones. Por lo general consiste en adoptar estrategias que reduzcan la capacidad del modelo, evitando así que se sobreajuste. \n",
    "\n",
    "Existe distintas estrategias de regularización, a continuación revisamos algunas de las más frecuentes:\n",
    "\n",
    "<b>Regularización L1 (Lasso)</b>: Añade a la función de costo un término penalizador proporcional al valor absoluto de los pesos. Así, redes que entrenan pesos muy grandes marcarán peor resultado que otras más simples. Esto puede llevar a que algunos pesos se vuelvan exactamente cero, lo que resulta en un modelo más <i>esparso</i> (disperso, muchos de los pesos son cero).\n",
    "\n",
    "<b>Regularización L2 (Ridge)</b>: Añade un término penalizador proporcional al cuadrado de la magnitud de los pesos. Es la forma más común de regularización y tiende a reducir todos los pesos hacia valores pequeños, pero no necesariamente cero.\n",
    "\n",
    "<br> <img src=https://sebastianraschka.com/images/faq/reqularization-linear/l2.png\n",
    "          width=\"350\"/>\n",
    "\n",
    "<b>Dropout</b>: Durante el entrenamiento, dropout apaga aleatoriamente una proporción de neuronas en las capas específicas, lo que evita que cualquier neurona dependa demasiado de las salidas de otras neuronas. Esencialmente, al hacer esto, estás entrenando una versión \"diluida\" de tu red en cada paso, y el modelo final es una especie de promedio de todas estas redes más pequeñas. Esto ayuda a mejorar la robustez y generalización del modelo.\n",
    "\n",
    "<br> <img src=https://miro.medium.com/v2/resize:fit:1044/1*iWQzxhVlvadk6VAJjsgXgg.png\n",
    "          width=\"400\"/>\n",
    "\n",
    "<b>Early stopping</b>: En lugar de entrenar la red hasta que el error de entrenamiento sea mínimo, se detiene el entrenamiento una vez que el error en un conjunto de validación separado comienza a aumentar. Esto significa que la red se ha ajustado lo suficiente a los datos de entrenamiento, pero antes de que comience a sobreajustarse.\n",
    "\n",
    "<br> <img src=https://www.researchgate.net/profile/Ramazan-Gencay/publication/3302948/figure/fig1/AS:671509056073733@1537111643349/Early-stopping-based-on-cross-validation.pbm\n",
    "          width=\"250\"/>\n",
    "\n",
    "<b>Noise regularization</b>: Se añade ruido a las entradas o a las activaciones dentro de la red durante el entrenamiento, lo que puede ayudar a evitar el sobreajuste al hacer que la red sea más robusta a pequeñas perturbaciones en sus entradas.\n",
    "\n",
    "<b>Data augmentation</b>: Aumentar tus datos significa crear versiones modificadas de los datos de entrada para ampliar el conjunto de entrenamiento. En el caso de imágenes puede ser rotarlas, recortarlas o voltearlas para crear más ejemplos de entrenamiento. \n",
    "\n",
    "Cada una de estas estrategias de regularización se puede usar individualmente, así como combinar dos o más de estas. Como siempre ocurre en nuestra disciplina, habrá que probar y buscar entre combinaciones y parámetros, lo cual suele ser un proceso largo, que nos lleve a identificar qué funciona mejor con el problema que estemos atacando. \n",
    "\n",
    "<b>Implementemos regularización $L2$ sobre nuestra red neuronal</b> \n",
    "<br> <img src=https://i.pinimg.com/736x/a9/a2/45/a9a245e8b6eea005944fc4e5dec7ee79.jpg\n",
    "          width=\"300\"/>\n",
    "\n",
    "Basta con sumar un coeficiente $\\lambda*\\|\\mathbf{W}\\|_2$ a la función de costos. Sin embargo, necesitaríamos acceder a cada peso de la red al momento de computar el costo, lo cual haría bastante menos eficiente la implementación. Podemos usar un truco para incorporar la regularización $L2$ directamente en el paso de backpropagation, añadiendo la derivada del término de penalización ($L2$) con respecto a los pesos. La derivada de $ \\lambda \\sum_i w_i^2 $ con respecto a $ w_i $ es simplemente $ 2\\lambda w_i $. Es decir, la regularización $L2$ es proporcional a $ \\lambda $ en la transmisión del error en la fase de backpropagation, así que eso es lo que implementaremos, evitando así cálculos adicionales con los pesos $w$ al momento de computar la pérdida. \n",
    " \n",
    "Note que esta solución no afecta el cálculo que ya tenemos de la función de costos, sólo produce el efecto de si hubieramos incluido $\\lambda*\\|\\mathbf{W}\\|_2$ al momento de hacer el cálculo del gradiente para efecto del ajuste de pesos. Por otro lado, eso es lo único que necesitamos realmente, no nos importa que la función de pérdida (costo) se siga calculando sin la regularización para efectos de reporte. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2661e6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCLayer(Layer):\n",
    "    def __init__(self, input_size, output_size, lambda_reg=0):\n",
    "        #np.random.seed(1234)\n",
    "        self.weights = np.random.rand(input_size, output_size) - 0.5\n",
    "        self.bias = np.random.rand(1, output_size) - 0.5\n",
    "        self.lambda_reg = lambda_reg  # Se agrega parámetro coeficiente lambda de regularización L2\n",
    "\n",
    "    def forward_propagation(self, input_data):\n",
    "        self.input = input_data\n",
    "        self.output = np.dot(self.input, self.weights) + self.bias\n",
    "        return self.output\n",
    "\n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        input_error = np.dot(output_error, self.weights.T)\n",
    "        weights_error = np.dot(self.input.T, output_error)\n",
    "\n",
    "        # Adicionamos acá el término de regularización L2 que castiga error en los pesos\n",
    "        weights_error += self.lambda_reg * self.weights\n",
    "\n",
    "        # Actualizar los parámetros\n",
    "        self.weights -= learning_rate * weights_error\n",
    "        self.bias -= learning_rate * output_error\n",
    "        return input_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a027228f",
   "metadata": {},
   "source": [
    "Con este cambio el modelo castigará soluciones en las que los pesos tienen valores grandes, lo que la lleva a buscar que los pesos sean del menor tamaño posible. Usar $L2$ implica que el castigo es proporcional al cuadrado de los pesos, lo cual inhibe a que el modelo reduzca la suma total de pesos por la vía de desactivar neuronas (dejar pesos en cero) para compensar pesos grandes. Esto permite que todos los pesos de la red sigan siendo distintos a cero, pero lo más cercanos posible. \n",
    "\n",
    "Volvemos a definir y entrenar la red, pero esta vez usando un parámetro de regularización $L2$ en la forma de un coeficiente $\\lambda$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e00095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necesitamos identificar cuantos nodos tiene nuestra entrada, y eso depende del tamaño de X.\n",
    "entrada_dim = len(X_train[0])\n",
    "\n",
    "# Crear instancia de Network\n",
    "model = Network()\n",
    "\n",
    "# Definimos el término de regularización lambda. \n",
    "lambda_reg=0.001\n",
    "\n",
    "# REPETIMOS EL MISMO EXPERIMENTO, PERO CON REGULARIZACIÓN\n",
    "model.add(FCLayer(entrada_dim, 256, lambda_reg=lambda_reg))  # AGREGAREMOS PARÁMETRO L2 A CADA CAPA DENSA            \n",
    "model.add(ActivationLayer(sigmoid, sigmoid_prime))\n",
    "model.add(FCLayer(256, 256, lambda_reg=lambda_reg))\n",
    "model.add(ActivationLayer(sigmoid, sigmoid_prime))\n",
    "model.add(FCLayer(256, 10, lambda_reg=lambda_reg))  \n",
    "model.add(ActivationLayer(sigmoid, sigmoid_prime))\n",
    "\n",
    "# Usar el modelo creado\n",
    "model.use(mse, mse_prime)\n",
    "model.fit(X_train, y_train, epochs=20, learning_rate=0.1) # DUPLIQUEMOS EL ENTRENAMIENTO\n",
    "\n",
    "# Usamos el modelo para predecir sobre el conjunto de prueba\n",
    "y_hat = model.predict(X_test)\n",
    "\n",
    "# Transformamos la salida en un vector one-hot encoded, es decir 0s y un 1. \n",
    "for i in range(len(y_hat)):\n",
    "    y_hat[i] = np.argmax(y_hat[i][0])\n",
    "\n",
    "# Reportamos los resultados del modelo\n",
    "matriz_conf = confusion_matrix(y_test, y_hat)\n",
    "\n",
    "print('\\nMATRIZ DE CONFUSIÓN para modelo ANN')\n",
    "print(matriz_conf,'\\n')\n",
    "print('La exactitud de testeo del modelo ANN es: {:.3f}'.format(accuracy_score(y_test,y_hat)))\n",
    "\n",
    "# Usamos el modelo para predecir sobre el conjunto de ENTRENAMIENTO\n",
    "y_hat = model.predict(X_train)\n",
    "\n",
    "# Transformamos la salida en un vector one-hot encoded, es decir 0s y un 1. \n",
    "for i in range(len(y_hat)):\n",
    "    y_hat[i] = np.argmax(y_hat[i][0])\n",
    "\n",
    "# Reportamos los resultados del modelo\n",
    "matriz_conf = confusion_matrix(y_train_value, y_hat)\n",
    "\n",
    "print('\\nMATRIZ DE CONFUSIÓN para modelo ANN')\n",
    "print(matriz_conf,'\\n')\n",
    "print('La exactitud de ENTRENAMIENTO del modelo ANN es: {:.3f}'.format(accuracy_score(y_train_value,y_hat)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf61856f",
   "metadata": {},
   "source": [
    "Usando Regularización L2 una Red con alta capacidad de representación, permite regular su capacidad de aprender con demasiada precisión los datos de entrenamiento, lo que en teoría nos debiera permitir aprovechar las capacidad de la red para obtener mejor generalización. \n",
    "\n",
    "El coeficiente $\\lambda$ pasa a ser un nuevo hiperparámetro del modelo, así que se debiera realizar una búsqueda exhaustiva sobre todos los hiperparámetros, mediante un GridSearch y Validación Cruzada, para encontrar un mejor modelo. Seguramente el resultado que está a la vista todavía se puede mejorar con los recursos que hemos visto hasta acá, pero el proceso de búsqueda será largo.\n",
    "\n",
    "<b>¿Cómo podemos acelerar el entrenamiento?</b>\n",
    "\n",
    "Nuestra implementación básica de red neuronal realiza el ajuste de sus parámetros internos de a una observación cada vez. Una forma de acelerar el proceso sería implementar <b><i>mini batch</i></b>. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7420b6",
   "metadata": {},
   "source": [
    "<b>MINI-BATCH</b>\n",
    "\n",
    "Cuando usando <i>Gradient Descent</i> ajustamos los pesos con muestra individual en el conjunto de datos de entrenamiento, se le conoce como <i>Stochastic Gradient Descent</i>. Si en lugar de con cada observación en la muestra usamos todo el lote del conjunto de entrenamiento, al final de cada <i>epoch</i>, se denomina <i>Bath Gradient Descent</i>. Esta última técnica reduce el tiempo de ejecución del algoritmo, a costa de hacer más lenta la convergencia del algoritmo. \n",
    "\n",
    "<i>Mini-Batch</i> equilibria ambos enfoques. En lugar de calcular el gradiente utilizando todo el conjunto de datos (como en BGD) o solo una muestra (como en SGD), se calcula utilizando un pequeño subconjunto de datos llamado mini-batch. Este subconjunto de datos puede tener un tamaño arbitrario, pero se recomienda usar potencias de dos: 16, 32, 64, 128, etc. \n",
    "\n",
    "Si implementamos mini-batch, podremos hacer búsquedas más rápidas de hiperparámetros adecuados a cada problema. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd33698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TAREA PARA LA CASA\n",
    "# Haga acá o en su propia implementación en GitHub una versión que use MINI BATCH.\n",
    "\n",
    "# Debe permitir configurar como hiperparámetro el tamaño del mini batch al hacer fit.\n",
    "# El último mini lote tendrá que tener un tamaño de ajuste, para calzar con el lote completo.\n",
    "# Su implementación debe enfocarse en usar el mini batch en la etapa de backward propagation\n",
    "# Obtenga el gradiente promedio del mini lote, y realice el ajuste de pesos una vez por mini lote.\n",
    "# Se sugiere realizar barajado aleatorio del mini lote (sin perder de vista orden de etiquetas).\n",
    "\n",
    "#... próxima clase vengan preparados para comentar cómo les fue. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa1223a",
   "metadata": {},
   "source": [
    "<b> CONCLUSIÓN </b>\n",
    "\n",
    "Hemos probado que nuestra implementación de Red Neuronal puede crecer, incorporando nuevas funciones de activación y de pérdida. \n",
    "\n",
    "Hemos ilustrado el problema del desvanecimiento del gradiente en redes con muchas capas, que impide el aprendizaje de la red. También hemos podido apreciar el efecto del sobreajuste en la capacidad de generalizar de nuestras redes. \n",
    "\n",
    "Hemos incorporando Regularización L2 (Ridge) para comenzar a combatir el sobreajuste. Queda pendiente implementar otras estrategias de regularización. Disponer de herramientas para hacer regularización será importante, en la medida que adoptemos modelos más potentes para resolver problemas específicos, cosa que esperamos hacer desde la próxima clase. Así que no es mala idea ir pensando en como incluiría otras técnicas de regularización, sean las mencionadas en este apunte como otras que pueda investigar. \n",
    "\n",
    "Si bien estamos ya reconociendo imágenes con esta implementación, aún simple, de red neuronal, necesitamos ir más allá. En la próxima clase nos enfocaremos en realizar cambios a la topología de la red, a fin de hacerla más propicia para tareas que involucran imágenes.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
