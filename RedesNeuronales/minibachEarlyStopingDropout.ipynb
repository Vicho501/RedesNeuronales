{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "5d85b848-4af9-4c32-8eed-dd7936e76f38",
      "metadata": {
        "id": "5d85b848-4af9-4c32-8eed-dd7936e76f38"
      },
      "source": [
        "<h1> Dropout aplicado en la capa full connected"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8abc7e48-9fa4-4dea-a3de-5674ecbe6051",
      "metadata": {
        "id": "8abc7e48-9fa4-4dea-a3de-5674ecbe6051"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "8bf1e09e-0c73-41ed-a510-41bd734e9fd9",
      "metadata": {
        "id": "8bf1e09e-0c73-41ed-a510-41bd734e9fd9"
      },
      "outputs": [],
      "source": [
        "from keras.datasets import mnist\n",
        "import random\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import numpy as np\n",
        "\n",
        "# No necesitamos tantos datos.\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "#random.seed(123) # Vamos a controlar la aleatoriedad en adelante.\n",
        "X, y = zip(*random.sample(list(zip(X_train, y_train)), 2000))\n",
        "\n",
        "# Sí necesitamos que la forma de X sea la de un vector, en lugar de una matriz.\n",
        "X, y = np.array(X, dtype='float64'), np.array(y, dtype='float64')\n",
        "X = np.reshape(X, (X.shape[0], -1))\n",
        "\n",
        "# Normalizamos Min-Max\n",
        "X= MinMaxScaler().fit_transform(X)\n",
        "\n",
        "# Dividimos la muestra en dos, una para entrenar y otra para testing, como tenemos\n",
        "# muestra de sobra nos damos el lujo de testear con la misma cantidad que entrenamos.\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=123)\n",
        "\n",
        "# Necesitamos que y_train sea un valor categórico, en lugar de un dígito entero.\n",
        "y_train_value = y_train # Guardaremos y_train como valor para un observación más abajo.\n",
        "from keras.utils import to_categorical\n",
        "y_train = to_categorical(y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "2dff8ac5-7f57-4187-9538-ff6d381f059c",
      "metadata": {
        "id": "2dff8ac5-7f57-4187-9538-ff6d381f059c"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Clase base para Capa\n",
        "class Layer:\n",
        "    def __init__(self):\n",
        "        self.input = None\n",
        "        self.output = None\n",
        "    # computes the output Y of a layer for a given input X\n",
        "    def forward_propagation(self, input, training=True):\n",
        "        raise NotImplementedError\n",
        "    # computes dE/dX for a given dE/dY (and update parameters if any)\n",
        "    def backward_propagation(self, output_error, learning_rate):\n",
        "        raise NotImplementedError\n",
        "\n",
        "# Clase para capas densas (fully connected) con Dropout\n",
        "class FCLayer(Layer):\n",
        "    def __init__(self, input_size, output_size, dropout_rate=0.5):\n",
        "        # np.random.seed(1234)\n",
        "        self.weights = np.random.rand(input_size, output_size) - 0.5\n",
        "        self.bias = np.random.rand(1, output_size) - 0.5\n",
        "        self.dropout_rate = dropout_rate  # Probabilidad de Dropout (para apagar neuronas)\n",
        "\n",
        "    def forward_propagation(self, input_data, training=True):\n",
        "        self.input = input_data\n",
        "        self.output = np.dot(self.input, self.weights) + self.bias\n",
        "\n",
        "        if training:\n",
        "            # Aplicamos Dropout durante el entrenamiento\n",
        "            self.dropout_mask = (np.random.rand(*self.output.shape) > self.dropout_rate).astype(int)\n",
        "            self.output *= self.dropout_mask  # Apagamos neuronas aleatoriamente\n",
        "        else:\n",
        "            # Durante la inferencia, escalamos el output por la tasa de retención (1 - dropout_rate)\n",
        "            self.output *= (1 - self.dropout_rate)\n",
        "\n",
        "        return self.output\n",
        "\n",
        "    def backward_propagation(self, output_error, learning_rate):\n",
        "        # Aplicamos el dropout mask también en la propagación hacia atrás\n",
        "        output_error *= self.dropout_mask  # Solo propagamos el error en las neuronas activas\n",
        "\n",
        "        input_error = np.dot(output_error, self.weights.T)\n",
        "        weights_error = np.dot(self.input.T, output_error)\n",
        "\n",
        "        # Actualizar los parámetros\n",
        "        self.weights -= learning_rate * weights_error\n",
        "        self.bias -= learning_rate * output_error\n",
        "        return input_error\n",
        "\n",
        "# Clase para Capa de Activación. Junto con la capa densa forman perceptrones.\n",
        "class ActivationLayer(Layer):\n",
        "    def __init__(self, activation, activation_prime):\n",
        "        self.activation = activation\n",
        "        self.activation_prime = activation_prime\n",
        "\n",
        "    # returns the activated input\n",
        "    def forward_propagation(self, input_data, training=True):\n",
        "        self.input = input_data\n",
        "        self.output = self.activation(self.input)\n",
        "        return self.output\n",
        "\n",
        "    # Returns input_error=dE/dX for a given output_error=dE/dY.\n",
        "    # learning_rate is not used because there is no \"learnable\" parameters.\n",
        "    def backward_propagation(self, output_error, learning_rate):\n",
        "        return self.activation_prime(self.input) * output_error\n",
        "\n",
        "# Clase Red, conecta múltiples capas.\n",
        "class Network:\n",
        "    def __init__(self):\n",
        "        self.layers = []\n",
        "        self.loss = None\n",
        "        self.loss_prime = None\n",
        "\n",
        "    # add layer to network\n",
        "    def add(self, layer):\n",
        "        self.layers.append(layer)\n",
        "\n",
        "    # set loss to use\n",
        "    def use(self, loss, loss_prime):\n",
        "        self.loss = loss\n",
        "        self.loss_prime = loss_prime\n",
        "\n",
        "    # predict output for given input\n",
        "    def predict(self, input_data):\n",
        "        if input_data.ndim == 1:  # YA NO SIEMPRE VAMOS A RECIBIR ARREGLOS UNIDIMENSIONALES\n",
        "            input_data = np.array([[x] for x in input_data])\n",
        "        samples = len(input_data)\n",
        "        result = []\n",
        "\n",
        "        # run network over all samples\n",
        "        for i in range(samples):\n",
        "            # forward propagation (sin dropout durante la predicción)\n",
        "            output = input_data[i]\n",
        "            for layer in self.layers:\n",
        "                output = layer.forward_propagation(output, training=False)\n",
        "            result.append(output)\n",
        "        return result\n",
        "\n",
        "    # train the network\n",
        "    def fit(self, x_train, y_train, epochs, learning_rate):\n",
        "        if x_train[0].ndim == 1:  # YA NO SIEMPRE VAMOS A RECIBIR ARREGLOS UNIDIMENSIONALES\n",
        "            x_train = np.array([[x] for x in x_train])\n",
        "        samples = len(x_train)\n",
        "        # training loop\n",
        "        for i in range(epochs):\n",
        "            err = 0\n",
        "            for j in range(samples):\n",
        "                # forward propagation (con dropout durante el entrenamiento)\n",
        "                output = x_train[j]\n",
        "                for layer in self.layers:\n",
        "                    output = layer.forward_propagation(output, training=True)\n",
        "\n",
        "                # compute loss (for display purpose only)\n",
        "                err += self.loss(y_train[j], output)\n",
        "\n",
        "                # backward propagation\n",
        "                error = self.loss_prime(y_train[j], output)\n",
        "                for layer in reversed(self.layers):\n",
        "                    error = layer.backward_propagation(error, learning_rate)\n",
        "\n",
        "            # calculate average error on all samples\n",
        "            err /= samples\n",
        "\n",
        "            # Para usar en clasificación (con más de dos clases)\n",
        "            # calculamos el error promedio entre nodos de salida.\n",
        "            err = np.mean(err)\n",
        "\n",
        "            # Imprimimos el error promedio de cada época, más que nada\n",
        "            # para seguimiento del aprendizaje.\n",
        "            print('epoch %d/%d   error=%f' % (i+1, epochs, err))\n",
        "\n",
        "# Funciones de Activación y sus correspondientes derivadas\n",
        "def tanh(x):\n",
        "    return np.tanh(x)\n",
        "\n",
        "def tanh_prime(x):\n",
        "    return 1 - np.tanh(x)**2\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_prime(x):\n",
        "    sig = sigmoid(x)  # Calculamos sigmoid(x) para cada elemento del vector\n",
        "    return sig * (1 - sig)\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def relu_prime(x):\n",
        "    return np.where(x > 0, 1, 0)\n",
        "\n",
        "# Funciones de pérdida y sus derivadas.\n",
        "def mse(y_true, y_hat):\n",
        "    return (y_true - y_hat) ** 2\n",
        "\n",
        "def mse_prime(y_true, y_hat):\n",
        "    return 2 * (y_hat - y_true)\n",
        "\n",
        "def bce(y_true, y_hat):\n",
        "    return np.mean(-y_true * np.log(y_hat) - (1 - y_true) * np.log(1 - y_hat))\n",
        "\n",
        "def bce_prime(y_true, y_hat):\n",
        "    return ((1 - y_true) / (1 - y_hat) - y_true / y_hat) / np.size(y_true)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "3a98780a-660d-4496-ac6b-0ebec968d1f0",
      "metadata": {
        "id": "3a98780a-660d-4496-ac6b-0ebec968d1f0"
      },
      "outputs": [],
      "source": [
        "class FCLayer(Layer):\n",
        "    def __init__(self, input_size, output_size, dropout_rate=0.5):\n",
        "        #np.random.seed(1234)\n",
        "        self.weights = np.random.rand(input_size, output_size) - 0.5\n",
        "        self.bias = np.random.rand(1, output_size) - 0.5\n",
        "        self.dropout_rate = dropout_rate  # Probabilidad de Dropout (para apagar neuronas)\n",
        "\n",
        "    def forward_propagation(self, input_data, training=True):\n",
        "        self.input = input_data\n",
        "        self.output = np.dot(self.input, self.weights) + self.bias\n",
        "\n",
        "        if training:\n",
        "            # Aplicamos Dropout durante el entrenamiento\n",
        "            self.dropout_mask = (np.random.rand(*self.output.shape) > self.dropout_rate).astype(int)\n",
        "            self.output *= self.dropout_mask  # Apagamos neuronas aleatoriamente\n",
        "        else:\n",
        "            # Durante la inferencia, escalamos el output por la tasa de retención (1 - dropout_rate)\n",
        "            self.output *= (1 - self.dropout_rate)\n",
        "\n",
        "        return self.output\n",
        "\n",
        "    def backward_propagation(self, output_error, learning_rate):\n",
        "        # Aplicamos el dropout mask también en la propagación hacia atrás\n",
        "        output_error *= self.dropout_mask  # Solo propagamos el error en las neuronas activas\n",
        "\n",
        "        input_error = np.dot(output_error, self.weights.T)\n",
        "        weights_error = np.dot(self.input.T, output_error)\n",
        "\n",
        "        # Actualizar los parámetros\n",
        "        self.weights -= learning_rate * weights_error\n",
        "        self.bias -= learning_rate * output_error\n",
        "        return input_error\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def early_stopping(model, X_test, y_test, patience=5, min_delta=0.001):\n",
        "    best_accuracy = 0\n",
        "    patience_counter = 0\n",
        "    for epoch in range(80):  # Ajusta el número de épocas según tus necesidades\n",
        "        model.fit(X_train, y_train, epochs=1, learning_rate=0.1)  # Entrena el modelo durante una época\n",
        "        y_hat = model.predict(X_test)  # Predice sobre los datos de prueba\n",
        "        for i in range(len(y_hat)):\n",
        "            y_hat[i] = np.argmax(y_hat[i][0])  # Transforma la salida en un vector one-hot encoded\n",
        "        accuracy = accuracy_score(y_test, y_hat)  # Calcula la exactitud del modelo\n",
        "        print(f'Epoch {epoch+1}, Accuracy: {accuracy:.4f}')\n",
        "        if accuracy > best_accuracy:\n",
        "            best_accuracy = accuracy\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(f\"Early stopping: la exactitud en el conjunto de validación ha dejado de mejorar después de {patience} épocas\")\n",
        "                break\n",
        "    return model\n",
        "\n",
        "# ...\n",
        "\n",
        "# Entrenamos el modelo con datos de entrenamiento y early stopping\n",
        "model = early_stopping(model, X_test, y_test, patience=5, min_delta=0.001)\n",
        "\n",
        "# Usamos el modelo para predecir sobre los datos de prueba (validación)\n",
        "y_hat = model.predict(X_test)\n",
        "\n",
        "# Transformamos la salida en un vector one-hot encoded, es decir 0s y un 1.\n",
        "for i in range(len(y_hat)):\n",
        "    y_hat[i] = np.argmax(y_hat[i][0])\n",
        "\n",
        "# Reportamos los resultados del modelo\n",
        "matriz_conf = confusion_matrix(y_test, y_hat)\n",
        "\n",
        "print('MATRIZ DE CONFUSIÓN para modelo ANN')\n",
        "print(matriz_conf,'\\n')\n",
        "print('La exactitud de testeo del modelo ANN es: {:.3f}'.format(accuracy_score(y_test,y_hat)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3OVaFt4ANRly",
        "outputId": "907e36a2-1061-47ea-c6f6-dd31e1d9efc2"
      },
      "id": "3OVaFt4ANRly",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1/1   error=0.468659\n",
            "Epoch 1, Accuracy: 0.7070\n",
            "epoch 1/1   error=0.463874\n",
            "Epoch 2, Accuracy: 0.7110\n",
            "epoch 1/1   error=0.458067\n",
            "Epoch 3, Accuracy: 0.7470\n",
            "epoch 1/1   error=0.454090\n",
            "Epoch 4, Accuracy: 0.7530\n",
            "epoch 1/1   error=0.456410\n",
            "Epoch 5, Accuracy: 0.7620\n",
            "epoch 1/1   error=0.454562\n",
            "Epoch 6, Accuracy: 0.7590\n",
            "epoch 1/1   error=0.446390\n",
            "Epoch 7, Accuracy: 0.7670\n",
            "epoch 1/1   error=0.438992\n",
            "Epoch 8, Accuracy: 0.7730\n",
            "epoch 1/1   error=0.441547\n",
            "Epoch 9, Accuracy: 0.7780\n",
            "epoch 1/1   error=0.440488\n",
            "Epoch 10, Accuracy: 0.7820\n",
            "epoch 1/1   error=0.433677\n",
            "Epoch 11, Accuracy: 0.7900\n",
            "epoch 1/1   error=0.434556\n",
            "Epoch 12, Accuracy: 0.8060\n",
            "epoch 1/1   error=0.430211\n",
            "Epoch 13, Accuracy: 0.8060\n",
            "epoch 1/1   error=0.428439\n",
            "Epoch 14, Accuracy: 0.7960\n",
            "epoch 1/1   error=0.425751\n",
            "Epoch 15, Accuracy: 0.8160\n",
            "epoch 1/1   error=0.422787\n",
            "Epoch 16, Accuracy: 0.8210\n",
            "epoch 1/1   error=0.424841\n",
            "Epoch 17, Accuracy: 0.8280\n",
            "epoch 1/1   error=0.424537\n",
            "Epoch 18, Accuracy: 0.8250\n",
            "epoch 1/1   error=0.422978\n",
            "Epoch 19, Accuracy: 0.8220\n",
            "epoch 1/1   error=0.424225\n",
            "Epoch 20, Accuracy: 0.8220\n",
            "epoch 1/1   error=0.420273\n",
            "Epoch 21, Accuracy: 0.8280\n",
            "epoch 1/1   error=0.421073\n",
            "Epoch 22, Accuracy: 0.8330\n",
            "epoch 1/1   error=0.410473\n",
            "Epoch 23, Accuracy: 0.8320\n",
            "epoch 1/1   error=0.413628\n",
            "Epoch 24, Accuracy: 0.8370\n",
            "epoch 1/1   error=0.408695\n",
            "Epoch 25, Accuracy: 0.8380\n",
            "epoch 1/1   error=0.411943\n",
            "Epoch 26, Accuracy: 0.8460\n",
            "epoch 1/1   error=0.412572\n",
            "Epoch 27, Accuracy: 0.8450\n",
            "epoch 1/1   error=0.407843\n",
            "Epoch 28, Accuracy: 0.8490\n",
            "epoch 1/1   error=0.406042\n",
            "Epoch 29, Accuracy: 0.8450\n",
            "epoch 1/1   error=0.409288\n",
            "Epoch 30, Accuracy: 0.8450\n",
            "epoch 1/1   error=0.400525\n",
            "Epoch 31, Accuracy: 0.8580\n",
            "epoch 1/1   error=0.411197\n",
            "Epoch 32, Accuracy: 0.8580\n",
            "epoch 1/1   error=0.405211\n",
            "Epoch 33, Accuracy: 0.8540\n",
            "epoch 1/1   error=0.399103\n",
            "Epoch 34, Accuracy: 0.8470\n",
            "epoch 1/1   error=0.400273\n",
            "Epoch 35, Accuracy: 0.8600\n",
            "epoch 1/1   error=0.401491\n",
            "Epoch 36, Accuracy: 0.8560\n",
            "epoch 1/1   error=0.401875\n",
            "Epoch 37, Accuracy: 0.8630\n",
            "epoch 1/1   error=0.398704\n",
            "Epoch 38, Accuracy: 0.8660\n",
            "epoch 1/1   error=0.399071\n",
            "Epoch 39, Accuracy: 0.8670\n",
            "epoch 1/1   error=0.403952\n",
            "Epoch 40, Accuracy: 0.8650\n",
            "epoch 1/1   error=0.395966\n",
            "Epoch 41, Accuracy: 0.8670\n",
            "epoch 1/1   error=0.399856\n",
            "Epoch 42, Accuracy: 0.8630\n",
            "epoch 1/1   error=0.396665\n",
            "Epoch 43, Accuracy: 0.8660\n",
            "epoch 1/1   error=0.399339\n",
            "Epoch 44, Accuracy: 0.8650\n",
            "Early stopping: la exactitud en el conjunto de validación ha dejado de mejorar después de 5 épocas\n",
            "MATRIZ DE CONFUSIÓN para modelo ANN\n",
            "[[ 89   0   1   2   0   1   0   0   0   0]\n",
            " [  0 113   2   0   0   0   0   0   1   0]\n",
            " [  0   3  69   4   0   0   0   3   2   2]\n",
            " [  1   0   3  98   1   1   0   2   1   0]\n",
            " [  0   1   0   0  76   0   1   1   0  12]\n",
            " [  6   3   1  15   1  63   4   3   3   1]\n",
            " [  0   0   1   0   0   2 102   0   0   0]\n",
            " [  2   5   0   0   2   1   0 107   0   1]\n",
            " [  2   3   2   8   1   2   2   3  63   0]\n",
            " [  3   1   1   1   7   1   0   2   0  85]] \n",
            "\n",
            "La exactitud de testeo del modelo ANN es: 0.865\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "65fcf0af-87c8-46cd-a433-62cc594cb2ea",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "65fcf0af-87c8-46cd-a433-62cc594cb2ea",
        "outputId": "7130859f-4266-4fdd-992e-9464c01ede9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1/80   error=0.518734\n",
            "epoch 2/80   error=0.509886\n",
            "epoch 3/80   error=0.491296\n",
            "epoch 4/80   error=0.502741\n",
            "epoch 5/80   error=0.487247\n",
            "epoch 6/80   error=0.484225\n",
            "epoch 7/80   error=0.482278\n",
            "epoch 8/80   error=0.461884\n",
            "epoch 9/80   error=0.463521\n",
            "epoch 10/80   error=0.469897\n",
            "epoch 11/80   error=0.461972\n",
            "epoch 12/80   error=0.456526\n",
            "epoch 13/80   error=0.447594\n",
            "epoch 14/80   error=0.448014\n",
            "epoch 15/80   error=0.451160\n",
            "epoch 16/80   error=0.451539\n",
            "epoch 17/80   error=0.438532\n",
            "epoch 18/80   error=0.439232\n",
            "epoch 19/80   error=0.439925\n",
            "epoch 20/80   error=0.436278\n",
            "epoch 21/80   error=0.432609\n",
            "epoch 22/80   error=0.435254\n",
            "epoch 23/80   error=0.429870\n",
            "epoch 24/80   error=0.427978\n",
            "epoch 25/80   error=0.422648\n",
            "epoch 26/80   error=0.430734\n",
            "epoch 27/80   error=0.425504\n",
            "epoch 28/80   error=0.424259\n",
            "epoch 29/80   error=0.416432\n",
            "epoch 30/80   error=0.418383\n",
            "epoch 31/80   error=0.417529\n",
            "epoch 32/80   error=0.422138\n",
            "epoch 33/80   error=0.414185\n",
            "epoch 34/80   error=0.413822\n",
            "epoch 35/80   error=0.422180\n",
            "epoch 36/80   error=0.405721\n",
            "epoch 37/80   error=0.418537\n",
            "epoch 38/80   error=0.406372\n",
            "epoch 39/80   error=0.408473\n",
            "epoch 40/80   error=0.405349\n",
            "epoch 41/80   error=0.401679\n",
            "epoch 42/80   error=0.408400\n",
            "epoch 43/80   error=0.404228\n",
            "epoch 44/80   error=0.400488\n",
            "epoch 45/80   error=0.400959\n",
            "epoch 46/80   error=0.406193\n",
            "epoch 47/80   error=0.401271\n",
            "epoch 48/80   error=0.403656\n",
            "epoch 49/80   error=0.403199\n",
            "epoch 50/80   error=0.397391\n",
            "epoch 51/80   error=0.397144\n",
            "epoch 52/80   error=0.394291\n",
            "epoch 53/80   error=0.398422\n",
            "epoch 54/80   error=0.393118\n",
            "epoch 55/80   error=0.391905\n",
            "epoch 56/80   error=0.397629\n",
            "epoch 57/80   error=0.392342\n",
            "epoch 58/80   error=0.392644\n",
            "epoch 59/80   error=0.381068\n",
            "epoch 60/80   error=0.394226\n",
            "epoch 61/80   error=0.386587\n",
            "epoch 62/80   error=0.392169\n",
            "epoch 63/80   error=0.389639\n",
            "epoch 64/80   error=0.384080\n",
            "epoch 65/80   error=0.388434\n",
            "epoch 66/80   error=0.387001\n",
            "epoch 67/80   error=0.386086\n",
            "epoch 68/80   error=0.387496\n",
            "epoch 69/80   error=0.384185\n",
            "epoch 70/80   error=0.388742\n",
            "epoch 71/80   error=0.382137\n",
            "epoch 72/80   error=0.382883\n",
            "epoch 73/80   error=0.384770\n",
            "epoch 74/80   error=0.382717\n",
            "epoch 75/80   error=0.381954\n",
            "epoch 76/80   error=0.371414\n",
            "epoch 77/80   error=0.385947\n",
            "epoch 78/80   error=0.385856\n",
            "epoch 79/80   error=0.380850\n",
            "epoch 80/80   error=0.375487\n",
            "MATRIZ DE CONFUSIÓN para modelo ANN\n",
            "[[ 82   0   0   0   0   1   2   0   2   0]\n",
            " [  0 116   1   0   0   1   0   0   0   0]\n",
            " [  1   2 102   4   2   0   5   3   2   0]\n",
            " [  1   0   2 103   0   4   2   2   2   0]\n",
            " [  0   0   0   0 110   0   2   0   0   7]\n",
            " [  1   1   0   5   2  52   3   2   5   0]\n",
            " [  1   0   2   0   1   1  89   0   1   0]\n",
            " [  1   2   2   0   0   0   0  69   1   3]\n",
            " [  0   3   1   3   0   4   2   0  88   1]\n",
            " [  0   0   0   3   4   1   1   7   1  76]] \n",
            "\n",
            "La exactitud de testeo del modelo ANN es: 0.887\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "\n",
        "# Necesitamos identificar cuantos nodos tiene nuestra entrada, y eso depende del tamaño de X.\n",
        "entrada_dim = len(X_train[0])\n",
        "\n",
        "# Crear instancia de Network\n",
        "model = Network()\n",
        "\n",
        "# Agregamos capas al modelo\n",
        "model.add(FCLayer(entrada_dim, 128))\n",
        "model.add(ActivationLayer(relu, relu_prime))\n",
        "model.add(FCLayer(128, 64))\n",
        "model.add(ActivationLayer(sigmoid, sigmoid_prime))\n",
        "model.add(FCLayer(64, 10))\n",
        "model.add(ActivationLayer(sigmoid, sigmoid_prime))\n",
        "\n",
        "# Asignamos función de pérdida\n",
        "model.use(bce, bce_prime)\n",
        "\n",
        "# Entrenamos el modelo con datos de entrenamiento\n",
        "model.fit(X_train, y_train, epochs=80, learning_rate=0.1)\n",
        "\n",
        "# Usamos el modelo para predecir sobre los datos de prueba (validación)\n",
        "y_hat = model.predict(X_test)\n",
        "\n",
        "# Transformamos la salida en un vector one-hot encoded, es decir 0s y un 1.\n",
        "for i in range(len(y_hat)):\n",
        "    y_hat[i] = np.argmax(y_hat[i][0])\n",
        "\n",
        "# Reportamos los resultados del modelo\n",
        "matriz_conf = confusion_matrix(y_test, y_hat)\n",
        "\n",
        "print('MATRIZ DE CONFUSIÓN para modelo ANN')\n",
        "print(matriz_conf,'\\n')\n",
        "print('La exactitud de testeo del modelo ANN es: {:.3f}'.format(accuracy_score(y_test,y_hat)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "c8f238fe-0b16-42ed-b641-7261a53e8e9a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c8f238fe-0b16-42ed-b641-7261a53e8e9a",
        "outputId": "530a8b0c-b3ed-4304-c4c1-08cc49f0e65c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1/10   error=0.524829\n",
            "epoch 2/10   error=0.511097\n",
            "epoch 3/10   error=0.503990\n",
            "epoch 4/10   error=0.491889\n",
            "epoch 5/10   error=0.494010\n",
            "epoch 6/10   error=0.479365\n",
            "epoch 7/10   error=0.480733\n",
            "epoch 8/10   error=0.475314\n",
            "epoch 9/10   error=0.465396\n",
            "epoch 10/10   error=0.463378\n",
            "MATRIZ DE CONFUSIÓN para modelo ANN\n",
            "[[ 81   0   0   1   0   0   3   0   2   0]\n",
            " [  0 116   0   1   0   1   0   0   0   0]\n",
            " [ 13   8  38   6   1   3  43   5   3   1]\n",
            " [  4   1   1  89   1   2   2   2   6   8]\n",
            " [  0   0   0   0 101   0   3   0   1  14]\n",
            " [  7   8   0  17   4  28   7   0   0   0]\n",
            " [  3   0   1   0   1   1  88   1   0   0]\n",
            " [  1   3   0   0   5   0   0  61   3   5]\n",
            " [  8   6   0  15   3   3   3   1  60   3]\n",
            " [  0   2   0   2  25   0   2   4   0  58]] \n",
            "\n",
            "La exactitud de testeo del modelo ANN es: 0.720\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "\n",
        "# Necesitamos identificar cuántos nodos tiene nuestra entrada, y eso depende del tamaño de X.\n",
        "entrada_dim = len(X_train[0])\n",
        "\n",
        "# Crear instancia de Network\n",
        "model = Network()\n",
        "\n",
        "# Parámetros para Dropout (probabilidad de apagar neuronas)\n",
        "dropout_rate = 0.5  # Puedes ajustar este valor entre 0.2 y 0.5\n",
        "\n",
        "# Agregamos capas al modelo con Dropout\n",
        "model.add(FCLayer(entrada_dim, 128, dropout_rate=dropout_rate))\n",
        "model.add(ActivationLayer(relu, relu_prime))\n",
        "model.add(FCLayer(128, 64, dropout_rate=dropout_rate))\n",
        "model.add(ActivationLayer(sigmoid, sigmoid_prime))\n",
        "model.add(FCLayer(64, 10, dropout_rate=dropout_rate))\n",
        "model.add(ActivationLayer(sigmoid, sigmoid_prime))\n",
        "\n",
        "# Asignamos función de pérdida\n",
        "model.use(bce, bce_prime)\n",
        "\n",
        "# Entrenamos el modelo con datos de entrenamiento\n",
        "model.fit(X_train, y_train, epochs=10, learning_rate=0.1)\n",
        "\n",
        "# Usamos el modelo para predecir sobre los datos de prueba (validación)\n",
        "y_hat = model.predict(X_test)\n",
        "\n",
        "# Transformamos la salida en un vector one-hot encoded, es decir 0s y un 1.\n",
        "for i in range(len(y_hat)):\n",
        "    y_hat[i] = np.argmax(y_hat[i][0])\n",
        "\n",
        "# Reportamos los resultados del modelo\n",
        "matriz_conf = confusion_matrix(y_test, y_hat)\n",
        "\n",
        "print('MATRIZ DE CONFUSIÓN para modelo ANN')\n",
        "print(matriz_conf, '\\n')\n",
        "print('La exactitud de testeo del modelo ANN es: {:.3f}'.format(accuracy_score(y_test, y_hat)))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Función para entrenar el modelo usando mini-batches\n",
        "def train_with_minibatch(model, X_train, y_train, X_test, y_test, batch_size=32, epochs=10, patience=5):\n",
        "    best_accuracy = 0\n",
        "    patience_counter = 0\n",
        "    for epoch in range(epochs):\n",
        "        print(f'Epoch {epoch + 1}/{epochs}')\n",
        "        # Iterar a través de los datos en mini-batches\n",
        "        for i in range(0, len(X_train), batch_size):\n",
        "            X_batch = X_train[i:i + batch_size]\n",
        "            y_batch = y_train[i:i + batch_size]\n",
        "            model.train_on_batch(X_batch, y_batch)\n",
        "        # Evaluar el modelo al final de cada época\n",
        "        loss, acc = model.evaluate(X_test, y_test, verbose=0)\n",
        "        print(f'Loss: {loss:.4f}, Accuracy: {acc:.4f}')\n",
        "\n",
        "        # Early stopping\n",
        "        if acc > best_accuracy:\n",
        "            best_accuracy = acc\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(f\"Early stopping: la precisión en el conjunto de validación ha dejado de mejorar después de {patience} épocas\")\n",
        "                break\n",
        "\n",
        "# Función principal para manejar ambos tipos de redes (fully connected y cnn)\n",
        "def main(data_type='fully_connected'):\n",
        "    # Generar datos de acuerdo al tipo de red\n",
        "    X, y = generate_data(data_type=data_type)\n",
        "\n",
        "    # Dividir los datos en entrenamiento y prueba\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Construir el modelo de acuerdo al tipo de datos\n",
        "    if data_type == 'fully_connected':\n",
        "        model = build_fully_connected_model()\n",
        "    elif data_type == 'cnn':\n",
        "        model = build_cnn_model_with_pooling()\n",
        "\n",
        "    # Compilar el modelo\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "    # Entrenar el modelo usando mini-batches\n",
        "    train_with_minibatch(model, X_train, y_train, X_test, y_test, batch_size=32, epochs=10, patience=5)\n",
        "\n",
        "# Ejecutar el código para la red Fully Connected\n",
        "print(\"Entrenando red Fully Connected con mini-batches:\")\n",
        "main(data_type='fully_connected')\n",
        "\n",
        "# Ejecutar el código para la red CNN con capas de Pooling\n",
        "print(\"\\nEntrenando red CNN con capas de Pooling y mini-batches:\")\n",
        "main(data_type='cnn')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "id": "Swk5fR7TQLlp",
        "outputId": "bf6c4ce7-15e0-4392-a11f-57517e5548a0"
      },
      "id": "Swk5fR7TQLlp",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entrenando red Fully Connected con mini-batches:\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'generate_data' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-535563048348>\u001b[0m in \u001b[0;36m<cell line: 48>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;31m# Ejecutar el código para la red Fully Connected\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Entrenando red Fully Connected con mini-batches:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'fully_connected'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;31m# Ejecutar el código para la red CNN con capas de Pooling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-535563048348>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(data_type)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'fully_connected'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;31m# Generar datos de acuerdo al tipo de red\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;31m# Dividir los datos en entrenamiento y prueba\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'generate_data' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0ea0a6f-0e47-477e-9f0e-316ba590ac0f",
      "metadata": {
        "id": "b0ea0a6f-0e47-477e-9f0e-316ba590ac0f"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}